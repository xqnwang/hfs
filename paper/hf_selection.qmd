---
title: "Optimal forecast reconciliation with time series selection"
author:
- name: Xiaoqian Wang
  acknowledgements: |
      Corresponding author.
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
- name: Rob J Hyndman
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
- name: Shanika L Wickramasuriya
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
abstract: Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can enhance overall accuracy in hierarchical or grouped structures, the most substantial improvements occur in series with initially poor-performing base forecasts. Nevertheless, certain series may experience deteriorations in reconciled forecasts. In practical settings, series in a structure often exhibit poor base forecasts due to model misspecification or low forecastability. To prevent their negative impact, we propose two categories of forecast reconciliation methods that incorporate time series selection based on out-of-sample and in-sample information, respectively. These methods keep "poor" base forecasts unused in forming reconciled forecasts, while adjusting weights allocated to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods ameliorate disparities stemming from varied estimates of the base forecast error covariance matrix, alleviating challenges associated with estimator selection. Empirical evaluations through two simulation studies and applications using Australian labour force and domestic tourism data demonstrate improved forecast accuracy, particularly evident in higher aggregation levels, longer forecast horizons, and cases involving model misspecification.
keywords: "Coherent, Hierarchical time series, Grouped time series, Linear forecast reconciliation, Optimization problem"
bibliography: references.bib
format:
  asa-pdf:
    keep-tex: true
    pdf-engine: pdflatex
    classoption: 12pt
    journal:
      blinded: false
include-in-header: preamble.tex

execute:
  echo: false
  warning: false
  message: false
  cache: false
date: last-modified
---

```{r}
#| label: load
#| cache: false
# Load all required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(fabletools)
library(patchwork)
library(knitr)
library(kableExtra)
library(latex2exp)

source("../R/nemenyi.R") # MCB test
source("../R/analysis.R") # Other functions used for analysis

#theme_set(theme_get() + theme(text = element_text(family = "Source Sans Pro")))
```

\spacing{1.8}

## Introduction {#sec-introduction}

Forecast reconciliation is a post-processing method that ensures forecasts of multivariate time series adhere to known linear constraints [@Hyndman2011-sd]. For example, the sum of regional unemployment forecasts should be equal to the national unemployment forecast.

@Hyndman2011-sd introduced optimal forecast reconciliation, whereby "base" forecasts of all series are generated independently, and then adjusted to satisfy the constraints, leading to a set of coherent reconciled forecasts. Subsequent research has extended and developed the idea in the context of cross-sectional data [@Hyndman2016-cz; @Wickramasuriya2019-fc; @Panagiotelis2021-mf], temporal data [@Athanasopoulos2017-jj], and cross-temporal data [@Di_Fonzo2023-vo]. @Athanasopoulos2023-sm provided a comprehensive introduction to the forecast reconciliation literature.

Reconciliation is known to improve overall forecast accuracy in collections of time series with aggregation constraints. On average, when the base forecasts are unbiased, the mean squared reconciled forecast error from the minimum trace reconciliation method [@Wickramasuriya2019-fc] is lower than that from the base forecasts [@Wickramasuriya2021-am]. Most of the improvements attributed to reconciliation are observed in series with initially poor-performing base forecasts [@Athanasopoulos2017-jj]. In practice, it is not uncommon for some series to have poor base forecasts due to challenges such as model misspecification or low signal-to-noise ratio (SNR). In such cases, it may be advantageous to exclude the worst base forecasts when performing reconciliation. This is the motivation for our proposed methods.

First, we propose forecast reconciliation methods that incorporate time series selection based on out-of-sample information, assuming unbiased base forecasts. We formulate this as an optimization problem, using diverse penalty functions to control the number of nonzero column entries in the weighting matrix for linear forecast reconciliation. We show that the number of selected time series is at least equal to the number of series at the bottom level, and we can reconstruct the entire structure by aggregating/disaggregating the selected series. Second, we relax the unbiasedness assumption and introduce an additional reconciliation method with selection, utilizing in-sample observations and their fitted values. This enables us to use the in-sample reconciliation performance for selection purposes. In this case, it may happen that fewer than the number of series at the bottom level are used for reconciliation. Through simulation experiments and two empirical applications, we demonstrate that our proposed methods guarantee coherent forecasts that outperform or match their respective benchmark methods. The improvements are particularly pronounced when focusing on higher aggregation levels, longer forecast horizons, and cases of model misspecification. A remarkable feature of the proposed methods is their ability to diminish disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating challenges associated with estimator selection, which is a prominent concern in the field of forecast reconciliation research.

The remainder of the paper is structured as follows. @sec-preliminaries presents the notations and a review of linear forecast reconciliation methods. @sec-methodology introduces our proposed methods to achieve time series selection in reconciliation, and provides some theoretical insights. @sec-simulations and @sec-applications show the results from simulations and two real-world datasets, respectively, followed by concluding remarks in @sec-conclusion. The R code for reproducing the results is available at [https://github.com/xqnwang/hfs](https://github.com/xqnwang/hfs).

## Preliminaries {#sec-preliminaries}

### Notation

A *hierarchical time series* is an $n$-dimensional multivariate time series that adheres to known linear constraints. Let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations from all time series in the hierarchy at time $t$, and $\bm{b}_t \in \mathbb{R}^{n_b}$ be a vector comprising observations of only the most disaggregated ("bottom-level") time series at time $t$. The full hierarchy at time $t$ can be written as
$$
\bm{y}_t = \bm{S}\bm{b}_t,
$$
for $t=1,2,\ldots,T$, where $T$ is the length of the time series, and $\bm{S}$ is an $n \times n_b$ *summing matrix* that defines the aggregation constraints. We can write the summing matrix as $\bm{S} = \left[\begin{array}{c}\bm{A} \\ \bm{I}_{n_b}\end{array}\right]$, where $\bm{A}$ is an $n_a \times n_b$ *aggregation matrix* with $n = n_a + n_b$, and $\bm{I}_{n_b}$ is an $n_b$-dimensional identity matrix.

![An example of a two-level hierarchical time series.](figs/hts_example.pdf){#fig-hts fig-align="center" width="35%" fig-pos="!t"}

For example, @fig-hts shows a simple hierarchy with $n = 7$, $n_b = 4$, $n_a = 3$, $\bm{y}_t = [y_{\text{Total},t}, y_{\text{A},t}, y_{\text{B},t}, y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, $\bm{b}_t = [y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, and
$$
\bm{S} = \left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\bm{I}_4}
\end{array}\right].
$$
The notation is general enough to include aggregation constraints that are non-hierarchical. Please refer to @Hyndman2021-fo for further details.

### Linear forecast reconciliation

Let $\hat{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ be a vector of $h$-step-ahead *base forecasts* for all time series in the structure, given observations up to and including time $T$, and stacked in the same order as $\bm{y}_t$. We can use any method to generate these forecasts, but in general they will not be coherent (i.e., they won't satisfy the constraints).
Let $\tilde{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ denote a vector of $h$-step-ahead *reconciled forecasts* given by
$$
\tilde{\bm{y}}_{T+h \mid T} = \bm{S}\bm{G}_h\hat{\bm{y}}_{T+h \mid T},
$$ {#eq-lr}
where $\bm{G}_h$ is an $n_b \times n$ weighting matrix and $\bm{S}$ is an $n \times n_b$ summing matrix.

#### Minimum trace reconciliation {-}

Let $\hat{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \hat{\bm{y}}_{t+h \mid t}$ denote the $h$-step-ahead in-sample *base forecast errors*, and $\tilde{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h \mid t}$ denote the $h$-step-ahead *reconciled forecast errors*, for $t = 1,2,\ldots,T-h$. @Wickramasuriya2019-fc formulated the reconciliation problem as minimizing the trace (MinT) of the $h$-step-ahead covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\bm{e}}_{t+h \mid t})$. Assuming the base forecasts are unbiased, and that we want the reconciled forecasts to be unbiased, the unique solution to the minimization problem is
$$
\bm{G}_h=\left(\bm{S}^{\prime} \bm{W}_h^{-1} \bm{S}\right)^{-1} \bm{S}^{\prime} \bm{W}_h^{-1},
$$ {#eq-mint}
where $\bm{W}_h$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors.

The MinT problem can be reformulated as a least squares problem with linear constraints:
$$
\min_{\tilde{\bm{y}}_{T+h \mid T}} \quad \frac{1}{2}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})^{\prime} \bm{W}_{h}^{-1}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})
 \qquad \text{s.t.} \quad \tilde{\bm{y}}_{T+h \mid T}=\bm{S}\tilde{\bm{b}}_{T+h \mid T},
$$ {#eq-mint_op}
where $\tilde{\bm{b}}_{T+h \mid T} \in \mathbb{R}^{n_b}$ comprises the $h$-step-ahead bottom-level reconciled forecasts, made at time $T$. The intuition behind MinT reconciliation is that **the larger the estimated variance of the base forecast errors, the larger the range of adjustments permitted for forecast reconciliation**.

It is challenging to estimate $\bm{W}_h$, especially for $h > 1$. It is common to assume $\bm{W}_h = k_h\bm{W}_1$, $\forall h$, where $k_h > 0$; then the MinT solution of $\bm{G}$ does not change with the forecast horizon, $h$. Hence, we will drop the subscript $h$ for ease of exposition. Table \ref{tbl-bench} lists the most popularly used candidate estimators for $\bm{W}_h$.

```{=tex}
\begin{table}[!h]
\caption{Forecast reconciliation methods for which different estimators of $\bm{W}$ are used.}\label{tbl-bench}
\centering
\begin{threeparttable}
\begin{tabular}{p{0.7\linewidth}r}
\toprule
Reconciliation method & $\bm{W}_h \propto$\\
\midrule
\textbf{OLS} \citep{Hyndman2011-sd} & $\bm{I}$\\
\textbf{WLSs} \citep{Athanasopoulos2017-jj} & $\operatorname{diag}(\bm{S} \bm{1})$\\
\textbf{WLSv} \citep{Hyndman2016-cz} & $\operatorname{Diag}(\hat{\bm{W}}_1)$\\
\textbf{MinT} \citep{Wickramasuriya2019-fc} & $\hat{\bm{W}}_1$\\
\textbf{MinTs} \citep{Wickramasuriya2019-fc} & $\lambda\operatorname{Diag}(\hat{\bm{W}}_1) + (1-\lambda)\hat{\bm{W}}_1$\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\item Note: $\bm{1}$ is a vector of 1s of size $n_b$, $\operatorname{diag}(\cdot)$ constructs a diagonal matrix using a given vector, $\hat{\bm{W}}_1$ denotes the unbiased covariance estimator based on the in-sample one-step-ahead base forecast errors (i.e., residuals), and $\operatorname{Diag}(\cdot)$ forms a diagonal matrix using the diagonal elements of the input matrix.
\end{tablenotes}
\end{threeparttable}
\end{table}
```

<!-- @Pritularga2021-lz demonstrated that the performance of forecast reconciliation is affected by two sources of uncertainties: the base forecast uncertainty and the reconciliation weight uncertainty. Recall that the uncertainty in the MinT solution in @eq-mint is introduced by the uncertainty in the weighting matrix as the summing matrix is fixed for a given structure. This indicates that OLS and WLSs estimators for $\bm{W}$ may lead to less volatile reconciliation performance compared to WLSv, MinT, and MinTs estimators. @Panagiotelis2021-mf provided a geometric intuition for reconciliation and showed that, when considering the Euclidean distance loss function, OLS reconciliation yields results that are at least as favorable as the base forecasts, whereas MinT reconciliation performs poorly relative to the base forecasts. However, when considering the expected Euclidean distance of the reconciled forecast error, @Wickramasuriya2021-am indicated that MinT reconciliation is better than OLS reconciliation. Therefore, which estimator for $\bm{W}$ to use hinges on the specific time series structure of interest, the targeted level or series, and the selected loss function. -->

#### Relaxation of the unbiasedness assumptions {-}

Both @Hyndman2011-sd and @Wickramasuriya2019-fc imposed unbiasedness conditions on the base forecasts and the reconciled forecasts. @Ben_Taieb2019-be proposed a reconciliation method relaxing the assumption of unbiasedness. Specifically, by expanding the training window incrementally, one observation at a time, they formulated the reconciliation problem as a regularized empirical risk minimization (RERM) problem given by
$$
\min_{\bm{G}_h} \frac{1}{(T-T_1-h+1)n}\left\|\bm{Y}_{h}^{*}-\hat{\bm{Y}}_{h}^{*} \bm{G}_{h}^{\prime} \bm{S}^{\prime}\right\|_F^2+\lambda\|\operatorname{vec}( \bm{G}_h)\|_1,
$$
where $T_1$ denotes the minimum number of observations used for model training, $\left\| \cdot \right\|_F$ is the Frobenius norm, $\|\cdot\|_1$ is the $L_1$ norm, $\operatorname{vec}(\cdot)$ denotes the vectorization of a matrix (stacking the columns of the matrix), $\bm{Y}_{h}^{*}=\left[\bm{y}_{T_1+h}, \ldots, \bm{y}_T\right]^{\prime}$, $\hat{\bm{Y}}_{h}^{*}=\left[\hat{\bm{y}}_{T_1+h \mid T_1}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime}$, and $\lambda \geq 0$ is a regularization parameter.

When $\lambda = 0$, the problem reduces to an empirical risk minimization (ERM) problem without regularization. Assuming that the series in the structure are jointly weakly stationary and $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is invertible, it has a closed-form solution given by
$$
\hat{\bm{G}}_h = \bm{B}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\left(\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\right)^{-1},
$$
where $\bm{B}_{h}^{*}=\left[\bm{b}_{T_1+h}, \ldots, \bm{b}_T\right]^{\prime}$. If $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is not invertible, @Ben_Taieb2019-be suggested using a generalized inverse. When $\lambda > 0$, imposing the $L_1$ penalty on $\bm{G}_h$ will introduce sparsity and reduce estimation variance, albeit at the cost of introducing some bias.

@Wickramasuriya2021-am proposed an empirical MinT (**EMinT**) solution without the unbiasedness constraint by minimizing the trace of the covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\bm{e}}_{T+h \mid T})$. Assuming the series are jointly weakly stationary, she derived the solution
$$
\hat{\bm{G}}_{h} = \bm{B}_{h}^{\prime}\hat{\bm{Y}}_{h}\left(\hat{\bm{Y}}_{h}^{\prime}\hat{\bm{Y}}_{h}\right)^{-1},
$$
where $\bm{B}_{h}=\left[\bm{b}_{h}, \ldots, \bm{b}_T\right]^{\prime}$, and $\hat{\bm{Y}}_{h}=\left[\hat{\bm{y}}_{h \mid 0}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime}$.

The difference between EMinT and ERM lies in the data sources, as EMinT uses in-sample observations and base forecasts, while ERM relies on observations and base forecasts from a holdout validation set. Both ERM and EMinT consider an estimate of $\bm{G}$ that changes over the forecast horizon, which is why we keep the subscript $h$ here.

A challenge in forecast reconciliation arises when some base forecasts perform poorly, as the role of the weighting matrix $\bm{G}$ is to assimilate *all* base forecasts and map them into bottom-level disaggregated forecasts, which are subsequently summed by $\bm{S}$. While the RERM method proposed by @Ben_Taieb2019-be introduces sparsity by shrinking some elements of $\bm{G}$ towards zero, it remains incapable of mitigating the adverse impact of underperforming base forecasts. Moreover, the method is time-consuming because it uses expanding windows to recursively generate out-of-sample base forecasts.

We therefore propose two new forecast reconciliation methods involving time series selection: constrained out-of-sample (under the unbiasedness assumption) and unconstrained in-sample (without the unbiasedness assumption). These methods aim to address the negative effect of some poor base forecasts on the overall performance of the reconciled forecasts. Additionally, through the incorporation of regularization in the objective function, our method improves reconciliation outcomes produced with a "poor" choice of $\bm{W}$.

## Forecast reconciliation with time series selection {#sec-methodology}

In this section, we introduce our methods for forecast reconciliation while automatically avoiding the use of poor base forecasts. @sec-constrained introduces constrained reconciliation methods in a formulation of the problem based on out-of-sample base forecasts, while @sec-unconstrained presents an unconstrained reconciliation method, where we formulate the problem based on in-sample observations and base forecasts.

### Series selection with unbiasedness constraint {#sec-constrained}

As $\bm{S}$ is fixed and $\hat{\bm{y}}_{T+h \mid T}$ is given, $\bm{G}_h$ determines the linear reconciliation performance, as shown in @eq-lr. We drop the subscript $h$ here as we assume $\bm{W}$ and $\bm{G}$ do not vary with the forecast horizon. A natural way to remove forecasts of some series is by controlling the number of nonzero column entries in $\bm{G}$. This leads to a generalization of the MinT optimization problem with an additional penalty term:
$$
\min_{\bm{G}} \quad \frac{1}{2}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)
+ \lambda\mathfrak{g}(\bm{G}) \qquad \text{s.t.} \quad \bm{G}\bm{S}=\bm{I},
$$ {#eq-op_u}
where $\hat{\bm{y}}:=\hat{\bm{y}}_{T+1 \mid T}$, $\mathfrak{g}(\cdot)$ penalizes the columns of $\bm{G}$ towards zero, and $\lambda$ is a penalty parameter. This can be considered *a grouped variable selection problem*, with each group corresponding to a column of $\bm{G}$. The constraint, $\bm{G}\bm{S}=\bm{I}$, ensures that the reconciled forecasts are unbiased if the base forecasts are unbiased [@Wickramasuriya2019-fc]. When $\lambda = 0$, the problem reduces to the MinT optimization problem in @eq-mint_op with a closed-form solution given by @eq-mint.

**Proposition 1.** *Under the assumption of unbiasedness, the count of nonzero column entries of* $\bm{G}$ (*i.e., the number of time series selected for reconciliation*), *derived through solving @eq-op_u, is at least equal to the number of time series at the bottom level. In addition, we can restore the full hierarchical structure by aggregating/disaggregating the selected time series.*

*Proof*. Under unbiasedness, $\bm{G}\bm{S}=\bm{I}$, and
$$
\min \left(\operatorname{rank}(\bm{G}), \operatorname{rank}(\bm{S})\right) \geq \operatorname{rank}(\bm{I}_{n_b})=n_b,
$$
which indicates that the count of nonzero column entries of $\bm{G}$ is at least equal to $n_b$.

Let $\bm{X}_{\cdot \mathbb{S}} \in \mathbb{R}^{r \times |\mathbb{S}|}$ denote the submatrix of the $r \times c$ matrix $\bm{X}$ with column indices forming a set $\mathbb{S}$ (and when $\mathbb{S} = \{j\}$, we simply use $\bm{X}_{\cdot j}$), where $|\mathbb{S}|$ denotes the size of the set $\mathbb{S}$. Similarly, let $\bm{X}_{\mathbb{S}\cdot} \in \mathbb{R}^{|\mathbb{S}| \times c}$ denote the submatrix of $\bm{X}$ whose rows are indexed by a set $\mathbb{S}$ (and when $\mathbb{S} = \{i\}$, we simply use $\bm{X}_{i\cdot}$). Assuming that the set $\mathbb{S}$ consists of the indices of nonzero columns in the solution to @eq-op_u, $\hat{\bm{G}}$, the following equations hold:
$$
\bm{G}\bm{S} = \hat{\bm{G}}_{\cdot \mathbb{S}}\bm{S}_{\mathbb{S}\cdot} = \bm{I},
\qquad\text{and}\qquad
\min \left(\operatorname{rank}(\hat{\bm{G}}_{\cdot \mathbb{S}}), \operatorname{rank}(\bm{S}_{\mathbb{S}\cdot})\right) \geq \operatorname{rank}(\bm{I}_{n_b})=n_b.
$$
Additionally, we have $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) \leq n_b$ as $\bm{S}$ has $n_b$ columns. Therefore, we can conclude that $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) = n_b$. Moreover, we have
$$
\bm{y}_t = \bm{S}\bm{b}_t = \bm{S}\bm{G}\bm{S}\bm{b}_t=\bm{S}\hat{\bm{G}}_{\cdot\mathbb{S}}\bm{S}_{\mathbb{S}\cdot}\bm{b}_t=\bm{S}\hat{\bm{G}}_{\cdot \mathbb{S}}(\bm{y}_t)_{\mathbb{S}},
$$
which implies that the hierarchical structure can be fully restored by aggregating/disaggregating the selected time series denoted by $(\bm{y}_{t})_{\mathbb{S}}$.

For example, consider the simple hierarchy shown in @fig-hts, it is not possible for our constrained reconciliation methods with selection to simultaneously zero out columns of $\bm{G}$ associated with series AA and AB. However, it is possible to zero out columns related to series AA and BA simultaneously.

**Proposition 2.** *The optimization problem in @eq-op_u can be reformulated as a least squares problem with regularization and linear equality constraint as follows:*
$$
\begin{aligned}
& \min_{\operatorname{vec}(\bm{G})} \quad \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda\mathfrak{g}\left(\operatorname{vec}(\bm{G})\right) \\
& \text{s.t.} \quad \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}),
\end{aligned}
$$ {#eq-op_u_reg}
*which is characterized as a high-dimensional problem in which the number of features, denoted as* $p = n_b \times n$*, is much larger than the number of observations,* $n$*.*

*Proof.* We have\vspace*{-0.4cm}\enlargethispage{0.4cm}
$$
\begin{aligned}
& \operatorname{vec}\left(\hat{\bm{y}}\right) = \hat{\bm{y}}, \\
& \operatorname{vec}\left(\bm{SG}\hat{\bm{y}}\right) = \left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}), \\
& \operatorname{vec}\left(\bm{G}\bm{S}\right) = \operatorname{vec}\left(\bm{I}_{n_b}\bm{G}\bm{S}\right) = \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G}).
\end{aligned}
$$
Substituting these into @eq-op_u, the previous problem now takes the form of a regression problem with an additional regularization term and an equality constraint on the coefficients, as shown in @eq-op_u_reg.

Next, we present three classes of regularizations that allow forecast reconciliation with series selection, resulting in three optimization problems: (i) group best-subset selection with ridge regularization, (ii) an intuitive method with $L_0$ regularization, and (iii) a group lasso method.

#### Group best-subset selection with ridge regularization {-}

In a high-dimensional context with $p \gg n$, it is common to assume that the true regression coefficient (i.e., $\operatorname{vec}(\bm{G})$ in our problem) is sparse. We apply a combination of $L_0$ and $L_2$ regularization to control the nonzero column entries in $\bm{G}$:
\begin{align}
\min_{\operatorname{vec}(\bm{G})} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda_0 \sum_{j=1}^n 1\left(\bm{G}_{\cdot j} \neq \bm{0}\right) + \lambda_2 \left\|\operatorname{vec}\left(\bm{G}\right)\right\|_2^2 \nonumber\\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}), \label{eq-subset}
\end{align}
where $1(\cdot)$ is the indicator function, $\lambda_0 \geq 0$ controls the number of nonzero columns of $\bm{G}$, $\lambda_2 \geq 0$ controls the strength of the ridge regularization, and $\|\cdot\|_2$ is the $L_2$ norm. In a hierarchical or grouped time series context, $\operatorname{vec}(\bm{G})$ has an inherent non-overlapping grouping structure, wherein each group corresponds to a single column of $\bm{G}$, each of size $n_b$. Hence, we call this reconciliation method *group best-subset selection with ridge regularization*. In the results that follow, we label the **Subset** method differently based on various $\bm{W}$ estimators, referring to them as **OLS-subset**, **WLSs-subset**, **WLSv-subset**, **MinT-subset**, and **MinTs-subset**, respectively.

The inclusion of the ridge term in Equation \ref{eq-subset} is motivated by earlier work on best-subset selection [e.g., @Hazimeh2020-xd; @Mazumder2022-hx], which suggests that additional ridge regularization helps mitigate the poor predictive performance of best-subset selection method in low SNR regimes.

We present a Big-M based mixed integer programming (MIP) formulation for the problem in Equation \ref{eq-subset}:
\begin{align} \label{eq-subset_mip}
\min_{\operatorname{vec}(\bm{G}), \bm{z}, \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j + \lambda_2 \bm{g}^{+\prime}\bm{g}^{+} \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right)\nonumber \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bm{G}) = \check{\bm{e}} \nonumber\\
& \sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M} z_j, \quad j \in[n] \nonumber\\
& \bm{g}^{+} \geqslant \operatorname{vec}(\bm{G}) \nonumber\\
& \bm{g}^{+} \geqslant-\operatorname{vec}(\bm{G}) \nonumber\\
& z_j \in\{0,1\}, \quad j \in[n], \nonumber
\end{align}
where $\mathcal{M}$ is a Big-M parameter (specified a-priori) that is sufficiently large that the optimal solution to Equation \ref{eq-subset_mip}, $\bm{g}^{+*}$, satisfies $\max_{j \in [n]}\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. The binary variable $z_j=0$ implies that $\bm{G}_{\cdot j}=\bm{0}$, and $z_j=1$ implies that $\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. Such Big-M formulations are commonly used in MIP problems to model relations between discrete and continuous variables, and have been recently explored in regression with $L_0$ regularization [@Bertsimas2016-ig]. The problem is a mixed integer quadratic program (MIQP) that can be solved using commercial MIP solvers, e.g., Gurobi and CPLEX.

**Parameter tuning.** To avoid computationally expensive cross-validation, we tune the parameters to minimize the sum of squared reconciled forecast errors on the truncated training set, comprising only the $\max\{h, s\}$ observations closest to the forecast origin, where $s$ is the seasonal period for seasonal data and $s=T$ for non-seasonal data. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$, which captures the scale of the first term in the objective function, where $\tilde{\bm{y}}^{\text{bench}}$ is a vector of reconciled forecasts obtained using @eq-mint with the same estimator of $\bm{W}$, and define $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$. For the parameter $\lambda_0$, we consider a grid of $k+1$ values, $\{\lambda_{0}^{1},\dots,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. So $\lambda_{0}^{1},\dots,\lambda_{0}^{k}$ is a sequence decreasing on the log scale. We use a grid of six values for the parameter $\lambda_2$, $\{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$. Thus, we tune over a two-dimensional grid of $(k+1) \times 6$ values to find the optimal combination of $\lambda_0$ and $\lambda_2$.

**Computation details.** The MIQP problem in Equation \ref{eq-subset_mip} is NP-hard and computationally intensive. @Bertsimas2016-ig showed that commercial MIP solvers are capable of tackling problem instances for $p$ up to a thousand. To address larger instances, there has been impressive work on developing MIP-based approaches for solving $L_0$-regularized regression problem; e.g., @Bertsimas2016-ig, @Hazimeh2020-xd, and @Hazimeh2022-hc. However, it is challenging to extend these approaches to accommodate additional constraints in the optimization problem. Despite potential challenges in handling large instances with commercial MIP solvers, in our experiments, we use Gurobi to solve Equation \ref{eq-subset_mip} by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for cases with $p > 1000$. This allows to terminate the solver before reaching the global optimum and return a suboptimal solution instead. This strategy is motivated by our need to consider numerous parameter candidates, and the final solution will be validated against the training set, which prevents the use of a poor estimate of $\bm{G}$.

#### Intuitive method with $L_0$ regularization {-}

Instead of estimating the entire matrix $\bm{G}$ as above, we leverage the MinT solution in @eq-mint to streamline the optimization problem under consideration. Specifically, we define $\bar{\bm{S}} = \bm{A}\bm{S}$, where $\bm{A} = \operatorname{diag}(\bm{z})$ is an $n \times n$ diagonal matrix, and $\bm{z}$ is an $n$-dimensional vector with elements either equal to 0 or 1. Taking the MinT solution in @eq-mint, we have $\bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}$. Given fixed $\bm{S}$ and estimation of $\bm{W}$, $\bar{\bm{G}}$ is entirely determined by $\bm{A}$. Thus, when the $j$th diagonal element of $\bm{A}$ is zero, the $j$th column of $\bar{\bm{G}}$ becomes entirely composed of zeros. Therefore, the optimization problem can be reduced to an integer quadratic programming (IQP) problem where all of the variables are restricted to being integers:
\begin{align*}
\min_{\bm{A}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right) + \lambda_0 \sum_{j=1}^n \bm{A}_{jj} \\
\text{s.t.} \quad & \bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \qquad\text{and}\qquad \bar{\bm{G}}\bm{S} = \bm{I},
\end{align*}
where $\lambda_0 \geq 0$ controls the number of nonzero diagonal elements in $\bm{A}$, consequently affecting the number of nonzero columns (i.e., selected time series) in $\bm{G}$. We call this reconciliation method the *intuitive method with* $L_0$ *regularization*. In the results that follow, we label the **Intuitive** method differently based on various estimators for $\bm{W}$, referring to them as **OLS-intuitive**, **WLSs-intuitive**, **WLSv-intuitive**, **MinT-intuitive**, and **MinTs-intuitive**, respectively.

We should note that implementing grouped variable selection with this optimization problem can be challenging because it imposes restrictions $\bar{\bm{G}}$ to ensure it adheres rigorously to the analytical solution of MinT while making the selection. Therefore, the resulting solution tends to be dense and may not have zero columns.

To ensure the invertibility of $\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S}$, and make the problem compatible with Gurobi, we reformulate the problem as
$$
\begin{aligned}
\min_{\bm{A},\bar{\bm{G}},\bm{C},\check{\bm{e}},\bm{z}} \quad & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j \\
\text{s.t.} \quad & \bar{\bm{G}}\bm{S} = \bm{I} \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bar{\bm{G}}) = \check{\bm{e}} \\
& \bar{\bm{G}}\bm{A}\bm{S} = \bm{I} \\
& \bar{\bm{G}} = \bm{C}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \\
& z_j \in\{0,1\}, \quad j \in[n].
\end{aligned}
$$ {#eq-intuitive_mip}

**Parameter tuning.** Similar to the setup in the group best-subset selection, we select the tuning parameter, $\lambda_0$, by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $\max\{h, s\}$ observations that occurred prior to the forecast origin. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$, and $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$, the collection of candidate values for $\lambda_0$ we consider is $\{\lambda_{0}^{1},\dots,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$.

**Computation details.** Following a setup akin to that in the group best-subset selection, we employ Gurobi to solve @eq-intuitive_mip by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for problems with $p > 1000$.

#### Group lasso method {-}

Lasso is another popular method for the selection and estimation of parameters in the context of linear regression. @Yuan2006-mw introduced the group lasso method that can be used when there is a grouped structure among the variables. Here, we consider *a group lasso problem under the unbiasedness assumption* given by
$$
\begin{aligned}
\min_{\bm{G}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2 \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right),
\end{aligned}
$$ {#eq-lasso}
where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make the model more flexible, and the second term in the objective is the penalty function that is intermediate between the $L_1$-penalty that is used in the lasso and the $L_2$-penalty that is used in ridge regression. In the results that follow, we label the **Lasso** method based on various estimators for $\bm{W}$, referring to them as **OLS-lasso**, **WLSs-lasso**, **WLSv-lasso**, **MinT-lasso**, and **MinTs-lasso**, respectively.

Next, we present the second order cone programming (SOCP) formulation for the group lasso based estimators given by
\begin{align}
\min_{\operatorname{vec}(\bm{G}), \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}_h^{-1}\check{\bm{e}} + \lambda \sum_{j=1}^n w_j c_j \label{eq-lasso_socp}\\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right) \nonumber \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}) = \check{\bm{e}} \nonumber \\
& c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}, \quad j \in[n]. \nonumber
\end{align}
Equation \ref{eq-lasso_socp} includes additional auxiliary variables $c_j \in \mathbb{R}_{\geq 0}$, $j \in [n]$, and second order cone constraints, $c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}$ for $j \in[n]$.

Compared to the previous two methods above, the group lasso method is computationally friendlier. Nonetheless, @Hazimeh2023-ie demonstrated, both empirically and theoretically, that the group $L_0$-regularized method exhibits advantages over its group lasso counterpart across a range of regimes. Group lasso can either be highly dense or possess non-zero coefficients that are overly shrunk. This issue becomes more pronounced when the groups are correlated with each other, as group lasso tends to retain all correlated groups instead of seeking a more concise model.

**Penalty weights and parameter tuning.** In the context of group lasso, the default choice for the penalty weight, $w_j$, is $\sqrt{p_j}$, where $p_j$ is the size of each group (in our case, $p_j = n_b$). In our experiments, we allocate different penalty weights to each group using $w_j = 1/\|\bm{G}_{\cdot j}^{\text{bench}}\|_2$, which allows us to account for variations in scale across different time series in the structure.

We compute the group lasso over $k+1$ values of the tuning parameter $\lambda$, and select the parameter by optimizing the sum of squared reconciled forecast errors on a truncated training set, consisting only of $\max\{h, s\}$ observations occurred prior to the forecast origin. The collection of candidate values for $\lambda$ is $\{\lambda^{1},\dots,\lambda^{k}, 0\}$, where $\lambda^{1} = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}(\lambda^{k} / \lambda^{1})^{(j-1) / (k-1)}$ for $j \in [k]$.

**Proposition 3.** *Ignoring the unbiasedness constraint, we define* $\lambda^{1}$ *as the smallest* $\lambda$ *value such that all predictors in the group lasso problem have zero coefficients. Then we have*
$$
\lambda^{1} = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j,
$$
*where* $j^{*}$ *denotes the column index of* $\hat{\bm{y}}^{\prime} \otimes \bm{S}$ *that corresponds to the* $j$*th column of* $\bm{G}$*.*

*Proof.* Denote $\bm{\beta} = \operatorname{vec}(\bm{G})$, and the first term in the objective of @eq-lasso as $L\left(\bm{\beta} \mid \bm{D}\right)$, where $\bm{D}$ is the working data $\{\hat{\bm{y}} , \hat{\bm{y}}^{\prime} \otimes \bm{S}\}$. Ignoring the unbiasedness constraint, we define $\lambda^{1}$ as the smallest $\lambda$ value such that all predictors in the group lasso problem have zero coefficients, i.e., the solution at $\lambda^{1}$ is $\hat{\bm{\beta}}^{1}=\bm{0}$. (Note that there is no intercept in our problem.) Under the Karush-Kuhn-Tucker conditions, we have
$$
\lambda^{1}
 = \max_{j=1, \ldots, n}\big\|\big[\nabla L(\hat{\bm{\beta}}^{1} \mid \bm{D})\big]^{(j)}\big\|_2 / w_j
 = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j.
$$

**Computation details.** Due to the incorporation of the unbiasedness constraint, we can not directly use some open-source packages designed for group lasso. Consequently, we employ Gurobi to solve the SOCP problem, configuring it by setting OptimalityTol = $0.0001$.

### Series selection method without unbiasedness constraint {#sec-unconstrained}

In this section, we relax the unbiasedness constraint, $\bm{G}\bm{S} = \bm{I}$, and introduce a reconciliation method with selection that relies on in-sample observations and fitted values. Let $\bm{Y} \in \mathbb{R}^{T \times n}$ denote a matrix comprising observations from all time series on the training set in the structure, and $\hat{\bm{Y}} \in \mathbb{R}^{T \times n}$ denote a matrix of in-sample one-step-ahead forecasts (i.e., fitted values) for all time series. The proposed *empirical group lasso* method considers the optimization problem
$$
\min_{\bm{G}} \quad \frac{1}{2 T} \left\|\bm{Y}-\hat{\bm{Y}} \bm{G}^{\prime} \bm{S}^{\prime}\right\|_F^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$
where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make a more flexible model. We rewrite the problem as
$$
\min_{\operatorname{vec}(\bm{G})} \quad \frac{1}{2 T} \left\|\operatorname{vec}(\bm{Y})-(\bm{S} \otimes \hat{\bm{Y}}) \operatorname{vec}\left(\bm{G}^{\prime}\right)\right\|_2^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$
which becomes a standard group lasso problem, with $\operatorname{vec}(\bm{Y})$ serving as the dependent variable and $\bm{S} \otimes \hat{\bm{Y}}$ as the covariate matrix. We denote this as **Elasso** in the results that follow.

Relaxing the unbiasedness constraint may result in fewer non-zero column entries in the $\bm{G}$ solution than the number of series at the bottom level. This differs from constrained reconciliation methods detailed in @sec-constrained. In an extreme scenario, the solution may take the form of a top-down $\bm{G}_{TD}=[\bm{p} \mid \bm{O}_{n_b \times (n-1)}]$, where only the column corresponding to the top level (most aggregated level) retains non-zero values, and $\bm{p} = (p_1, p_2, \ldots, p_{n_b})$ is a proportionality vector obtained based on in-sample reconciled forecast errors.

We also explored the empirical version of group best-subset selection with ridge regularization and the intuitive method with $L_0$ regularization in which we omit the unbiasedness constraint. It is worth mentioning that @Hazimeh2023-ie presented an algorithmic framework for formulating the group $L_0$ problem with ridge regularization and provided the **L0Group** Python package for implementation. However, our experiments showed that this algorithm can not terminate within five hours for typical instances with $p \sim 10^4$. Therefore, in this paper, we only present the empirical group lasso method for series selection without the unbiasedness constraint.

**Penalty weights and parameter tuning.** Similar to the setup in the group lasso method, we assign different penalty weights to each group by setting $w_j = 1/\|\bm{G}_{\cdot j}^{\text{OLS}}\|_2$, where $\bm{G}^{\text{OLS}}$ is the solution obtained by the OLS estimator of $\bm{W}$. Given a fixed tuning parameter, we solve the target optimization problem by considering the initial $T-T_v$ observations, where $T_v = \max\{h, s\}$ for seasonal time series and $T_v = \lfloor \frac{1}{10}T \rfloor$ for non-seasonal time series. Then the tuning parameter, $\lambda$, is selected by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $T_v$ observations closest to the forecast origin. Specifically, for $\lambda$ values, we consider $\{\lambda^{1},\dots,\lambda^{k}, 0\}$, where $\lambda^{1} = \max_{j=1, \ldots, n}\left\|-\frac{1}{N}\left(\left(\bm{S} \otimes \hat{\bm{Y}}\right)_{\cdot j*}\right)^{\prime} \operatorname{vec}(\bm{Y})\right\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}\left(\lambda^{k} / \lambda^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. Following the derivation in the proof of **Proposition 3**, $\lambda^{1}$ is the smallest $\lambda$ value such that all predictors in the empirical group lasso problem have zero coefficients, i.e., $\bm{G} = \bm{O}$. Note that we need to resolve the optimization problem based on the whole training set by using the optimal tuning parameter to obtain the final solution.

**Computation details.** While there are open-source packages available for solving group lasso problems, they are still relatively slow when handling large instances. For example, given a specific value for the parameter, $\lambda$, our experiments observed that, using the **gglasso** R package, we can not obtain a solution within five hours for typical instances with $p \sim 10^4$. Instead, we use Gurobi to solve the problem using the SOCP formulation for the empirical group lasso which aligns with Equation \ref{eq-lasso_socp} but omits the unbiasedness constraint.

## Monte Carlo simulations {#sec-simulations}

To assess the proposed reconciliation methods with time series selection outlined in @sec-methodology, we carry out two simulations with different designs. Both simulations consider a hierarchy comprising two levels of aggregation, as shown in @fig-hts. Specifically, the structure has four series at the bottom level, and seven series in total; i.e., $n_b = 4$, and $n = 7$. The bottom-level series are first generated and then summed to obtain aggregated series at higher levels.

@sec-sim1 considers a setup where the bottom-level series are generated using a structural time series model, but model misspecification exists for some series within the structure. @sec-sim2 explores the impact of the correlation between series on the performance of reconciled forecasts.

### Setup 1: Exploring the effect of model misspecification {#sec-sim1}

We follow a simulation setup similar to @Wickramasuriya2019-fc, assuming that the bottom-level time series are generated using the basic structural time series model
$$
\bm{b}_t=\bm{\mu}_t+\bm{\gamma}_t+\bm{\eta}_t,
$$
where $\bm{\mu}_t$ and $\bm{\gamma}_t$ are trend and seasonal components defined by
\begin{align*}
\bm{\mu}_t & =\bm{\mu}_{t-1}+\bm{v}_t+\bm{\varrho}_t, &&& \bm{\varrho}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_{\varrho}^2 \bm{I}_4\right), \\
\bm{v}_t & =\bm{v}_{t-1}+\bm{\zeta}_t, &&& \bm{\zeta}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\zeta^2 \bm{I}_4\right), \\
\bm{\gamma}_t & =-\sum_{i=1}^{s-1} \bm{\gamma}_{t-i}+\bm{\omega}_t, &&& \bm{\omega}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\omega^2 \bm{I}_4\right),
\end{align*}
$\bm{\varrho}_t$, $\bm{\zeta}_t$, and $\bm{\omega}_t$ are error terms independent of each other and over time, and $\bm{\eta}_t$ is generated independently from an $\text{ARIMA}(p,0,q)$ process, where $p$ and $q$ take values of $0$ or $1$ with equal probability. Coefficients in the ARIMA process are randomly sampled from a uniform distribution within the range $[0.5, 0.7]$, and the contemporaneous error covariance matrix is given by
$$
\left[\begin{array}{llll}
5 & 3 & 2 & 1 \\
3 & 4 & 2 & 1 \\
2 & 2 & 5 & 3 \\
1 & 1 & 3 & 4
\end{array}\right],
$$
which enables correlations among time series in a hierarchical structure.

We set $s = 4$ for quarterly data with error variances $\sigma_{\varrho}^2=2$, $\sigma_\zeta^2=0.007$, and $\sigma_\omega^2=7$. Initial values for $\bm{\mu}_0$, $\bm{v}_0$, $\bm{\gamma}_0$, $\bm{\gamma}_1$, and $\bm{\gamma}_2$ are generated independently from a multivariate normal distribution with zero mean and identity covariance matrix. For each bottom-level series, we generate a total of $T+h = 180$ observations, with the last $h = 16$ observations forming the test set. The bottom-level series are aggregated for data at higher levels. This process is repeated $500$ times.

We use ETS models to generate base forecasts in the hierarchy with the default settings from the **forecast** R package [@Hyndman2023-fc]. To introduce model misspecification, we deliberately undermine in-sample and out-of-sample forecasts (i.e., fitted values and base forecasts) for specific time series in three scenarios. In each scenario, a 1.5 multiplier is applied to in-sample and out-of-sample forecasts for a single series; i.e., series AA at the bottom level, series A at the middle level, and series Total at the top level, resulting in Scenarios A--C, respectively.

@tbl-s1-rmse, @tbl-s2-rmse, and @tbl-s3-rmse summarize the results, each reporting the average root mean squared error (RMSE) for each level as well as the whole structure (denoted as *Average*). The *Base* row shows average RMSE of base forecasts, while entries below report the percentage decrease (negative) or increase (positive) in average RMSE of reconciled forecasts compared to base forecasts. The *BU* row uses a "bottom-up" approach, aggregating bottom-level base forecasts to form higher level forecasts. Notably, in each scenario, the largest improvements occur at the respective hierarchical level with model misspecification, while slightly deteriorating the performance at other levels.

```{r}
#| label: tbl-s1-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario A, Setup 1.

rmse_s1 <- readRDS("results/sim_rmse_s1.rds")
latex_table(rmse_s1)
```

Focusing on the results of benchmark reconciliation methods, we find that the BU approach performs the best in Scenarios B and C but ranks as the worst overall in Scenario A. This is not surprising, as bottom-level base forecasts are deteriorated in Scenario A, while higher-level base forecasts are deteriorated in Scenarios B and C. Moreover, WLSv, MinT, and MinTs perform especially well in Setup 1, benefiting from their ability to consider in-sample covariance of base forecast errors, allowing for a larger range of adjustments in reconciliation for base forecasts with higher estimated error variance. EMinT also provides accurate reconciled forecasts in our setup, where the in-sample forecasts for specific series are intentionally undermined, a situation that can be detected by the in-sample information based EMinT method. However, OLS and WLSs significantly underperform other benchmark methods in this simulation design.

In all three scenarios, our proposed methods consistently produce either improved or comparable reconciled forecasts compared to their respective benchmarks. The improvements are particularly pronounced when using OLS and WLSs estimators of $\bm{W}$ in the benchmark methods, which do not take into account the in-sample covariance of base forecast errors. One advantage of using our proposed forecast reconciliation methods with selection is their ability to reduce the difference introduced by using different estimates of $\bm{W}$, thereby mitigating the risk of estimator selection. In some cases, such as Scenarios B and C, we can align the forecast accuracy achieved using different estimators, and make them approach the best results we can obtain. Dropping the unbiasedness assumption, Elasso performs similarly to EMinT overall while achieving improvements at the top level, which is typically the aspect of greatest concern to practitioners.

```{r}
#| label: tbl-s1-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario A, Setup 1.

selection_sim <- readRDS("results/sim_selection.rds")
latex_sim_nos_table(selection_sim$out_s1$z, selection_sim$out_s1$n, "s1")
```

In addition, we report the proportion of time series being selected from our proposed methods in 500 simulation instances, as shown in @tbl-s1-selection, @tbl-s2-selection, and @tbl-s3-selection. Clearly, our methods select fewer time series, while enhancing forecast accuracy compared to benchmarks. Subset methods, in particular, tend to return fewer time series compared to the Intuitive and Lasso methods, which aligns with our expectations that the Intuitive and Lasso methods tend to produce dense estimates. Most importantly, depending on the scenario considered, the time series with model misspecification has been selected less often than others. For example, in Scenario A, series AA is expected to be removed while retaining AB. This allows obtaining series AA via operations such as A$-$AB, Total$-$B$-$AB, or Total$-$AB$-$BA$-$BB. The results in @tbl-s1-selection align with our expectations, showing frequent exclusion of series AA and consistent selection of AB.

### Setup 2: Exploring the effect of correlation {#sec-sim2}

We now simulate a hierarchical structure with correlated series, using a similar simulation to @Wickramasuriya2021-am, and the same hierarchical structure as shown in @fig-hts. We use a stationary VAR(1) data generating process for the time series at the bottom level:
$$
\bm{b}_t= \bm{c} + \left[\begin{array}{cc}
\bm{A}_1 & \bm{0} \\
\bm{0} & \bm{A}_2
\end{array}\right] \bm{b}_{t-1} + \bm{\varepsilon}_t,
$$
where $\bm{c}$ is a constant vector with all entries set to $1$, $\bm{A}_1$ and $\bm{A}_2$ are $2 \times 2$ matrices with eigenvalues $z_{1,2}=0.6[\cos (\pi / 3) \pm i \sin (\pi / 3)]$ and $z_{3,4}=0.9[\cos (\pi / 6) \pm i \sin (\pi / 6)]$, respectively, $\bm{\varepsilon}_t \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$, where
$$
\bm{\Sigma}=\left[\begin{array}{cc}
\bm{\Sigma}_1 & 0 \\0 & \bm{\Sigma}_2
\end{array}\right], \quad\text{and}\quad \bm{\Sigma}_1=\bm{\Sigma}_2=\left[\begin{array}{cc}2 & \sqrt{6} \rho \\\sqrt{6} \rho & 3\end{array}\right],
$$
and $\rho \in \{0, \pm 0.2, \pm 0.4, \pm 0.6, \pm 0.8\}$ controls the error correlation in the simulated hierarchy.

For each time series at the bottom level, we generate a total of $101$ observations, with the last observation serving as the test set, i.e., $T=100$ and $h=1$. Once again, the data at the higher levels are obtained by aggregating the bottom-level series. The process is repeated $500$ times for each candidate correlation, $\rho$.

```{r}
#| label: fig-corr-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 9
#| fig-height: 6.5
#| fig-pos: "!b"
#| fig-cap: An example hierarchical time series and its in-sample residuals in Setup 2.

data <- readRDS("results/corr_data_neg.rds")
resid <- readRDS("results/corr_resid_neg.rds")

theme_plot <- theme_bw() +
  theme(legend.position="bottom",
        legend.margin=margin(0,0,0,0),
        legend.box.spacing = unit(0, "pt"),
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())
p11 <- data |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: observations") +
  theme_plot

p12 <- data |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: observations") +
  theme_plot

p13 <- data |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: observations") +
  theme_plot

p21 <- resid |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: residuals") +
  theme_plot

p22 <- resid |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: residuals") +
  theme_plot

p23 <- resid |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: residuals") +
  theme_plot

(p11 + p21) / (p12 + p22) / (p13 + p23)
```

For each series, base forecasts are generated from ARMA models. We identify the best ARMA model using the automated algorithm implemented in the **forecast** R package [@HK08]. Additionally, when fitting ARMA models for time series Total, A, and BA, we introduce a slight bias by omitting the constant term. @fig-corr-data presents an illustrative example of a simulated hierarchical time series. The left panels depict time plots for each series at different levels of the structure, while the right panels show the residuals obtained from forecasting each series using the fitted ARMA model. Notably, despite our omission of the constant term when fitting ARMA models to series Total, A, and BA, the residuals derived from the identified optimal models still exhibit fluctuations around zero and do not display significant deviations in comparison to the residuals from other series. This is because the influence of the constant term is minimal, i.e., it is much smaller compared to the data variability. Thus, it may be challenging to identify the "poor" base forecasts and exclude them from reconciliation in this setup.

```{r}
#| label: tbl-corr-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results across various error correlations for simulation in Setup 2.

rmse_corr <- readRDS("results/corr_rmse.rds")
latex_corr_table(rmse_corr)
```

@tbl-corr-rmse summarizes the average RMSE of the base forecasts across various error correlations and the percentage relative improvements in RMSE achieved by reconciliation methods relative to the base forecasts. The results show that, for OLS, WLSs, and WLSv estimators, our proposed methods consistently dominate or are equivalent to their respective benchmark methods at all levels. We should highlight the challenge of identifying the "poor" base forecasts in this simulation design, given that the omission of the constant term has minimal impact relative to the data variability. In addition, we observe that the MinT and MinTs methods perform especially well and our methods provide results similar to these benchmark methods. This is attributed to the use of in-sample covariance by MinT and MinTs, which allows for large adjustments in reconciliation for base forecasts with high estimated error variance. Elasso forecasts are slightly worse than EMinT, possibly due to the difficulty of identifying underperforming base forecasts in this simulation setup.
We have also considered alternative error correlation values, $\rho = -0.6, -0.2, 0.2, 0.4$, for this simulation setting, but to save space, we do not present all results. The omitted results follow a similar pattern and are available upon request.

In @tbl-corr-selection-neg and @tbl-corr-selection-pos, we present the proportion of time series being selected using our proposed methods. @tbl-corr-selection-neg shows that, for OLS, WLSs, and WLSv estimators, Subset and Intuitive methods are able to exclude the series Total, A, and BA in some instances, in which small biases are introduced in model fitting, while essentially retaining the remaining series. Subset methods outperform Intuitive methods in selection. Lasso methods typically select all bottom-level series, given their tendency to yield dense estimates, as discussed in @sec-constrained. Elasso also selects all bottom-level series. When dealing with a high positive error correlation, @tbl-corr-selection-pos shows that our methods still show potential for selection but it becomes somewhat challenging to identify and exclude the series that should be omitted in reconciliation. Hence, our methods are preferred, especially when the error correlation within the structure is negative.

```{r}
#| label: tbl-corr-selection-neg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Setup 2, with the error correlation being -0.8.

selection_corr_neg <- readRDS("results/corr_selection_neg.rds")
latex_sim_nos_table(selection_corr_neg$out_s0$z,
                    selection_corr_neg$out_s0$n,
                    "corr_neg")
```

## Applications {#sec-applications}

In this section we describe two empirical applications: @sec-labour focuses on a grouped hierarchy built using the Australian labour force survey data released by the Australian Bureau of Statistics, while @sec-tourism considers Australian domestic tourism flows with a natural geographic hierarchy.

### Forecasting Australian labour force {#sec-labour}

The dataset from the Labour Force Survey was released by the Australian Bureau of Statistics, and comprises monthly data on the number of unemployed persons in Australia from January 2010 to July 2023[^1]. To deal with the few missing observations, we use linear interpolation. Analyzing unemployment data by labour market region and duration of job search offers valuable insights into regional disparities, and the structural nuances underlying unemployment. Forecast reconciliation is crucial in such a case to ensure aligned decision making.

[^1]: The Labour Force Survey data is publicly available at <https://www.abs.gov.au/statistics/labour/employment-and-unemployment/labour-force-australia-detailed/aug-2023>.

We construct a grouped hierarchy by disaggregating the number of unemployed persons over two independent attributes, duration of job search (referred to as *Duration*), and State and Territory (referred to as *STT* ). At the bottom level, the data are disaggregated by both attributes. We refer to the bottom level as the *Duration* $\times$ *STT* level. Specifically, there are six different groups of job search duration, under 1 month, 1--3 months, 3--6 months, 6--12 months, 1--2 years, and 2 years and over. Additionally, the number of unemployed persons in Australia can be disaggregated by eight states and territories, i.e., NSW (New South Wales), VIC (Victoria), QLD (Queensland), SA (South Australia), WA (Western Australia), TAS (Tasmania), NT (Northern Territory), and ACT (Australian Capital Territory). So the final grouped hierarchy consists of the top series, six series at the Duration level, eight series at the STT level, and $48$ series at the Duration $\times$ STT level, giving $63$ time series in total, each of length $163$ observations.

```{r}
#| label: fig-labour-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 10
#| fig-height: 6
#| fig-pos: "!t"
#| fig-cap: Australia unemployed persons, disaggregated by state and territory, and by duration of job search.

labour_ts <- readRDS("results/labour_gts.rds")

p1 <- labour_ts |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Total unemployed persons") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p2 <- labour_ts |>
  filter(Series %in% c("NSW", "VIC", "QLD", "SA", "WA", "TAS", "NT", "ACT")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "State and territory") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p3 <- labour_ts |>
  filter(Series %in% c("Under 1 month", "1-3 months", "3-6 months", "6-12 months",
                       "1-2 years", "2 years and over")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Duration of job search") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p1 / (p2 + p3)
```

The top panel in @fig-labour-data shows the total number of unemployed persons in Australia from January 2010 to July 2023, representing the top-level series in the hierarchical structure. The monthly series shows strong seasonality within each year, marked by prominent peaks occurring every January, attributable to school-leavers. Lower peaks occur in July, perhaps impacted by the start of the financial year. Amidst the backdrop of COVID-19's non-essential service shutdowns and trading restrictions, March and April 2020 saw a notable surge in unemployment. However, as coronavirus cases dwindled significantly and restrictions eased in the aftermath, employment made a remarkable recovery, leading to a subsequent decline in unemployment. The bottom-left panel displays the breakdown of unemployed individuals by state and territory, while the bottom-right panel presents the breakdown by the duration of job search. The plots display diverse and rich dynamics both within and between different levels of the hierarchy. For example, there was noticeable growth observed during 2020 for some states such as NSW, VIC, and QLD, whereas other states did not experience such significant growth. Additionally, there is a resemblance in the seasonal patterns between NSW and QLD, while the seasonal pattern in VIC differs. When comparing the series at the STT level and Duration level, the seasonal patterns in the Duration-level series are more consistent and potentially easier to forecast.

We assess the forecast accuracy of base forecasts and various reconciliation methods through a rolling forecast origin approach. Our aim is to generate $1$- to $12$-step-ahead forecasts for each of the $63$ series while ensuring coherence. Given the limited data compared to the forecast horizon, we initiate the process with a training set of $139$ observations for each series. The training set is used to select the optimal ETS model with the automatic algorithm implemented in the **forecast** package for R [@HK08]. Using these fitted ETS models, we generate base forecasts, and perform diverse forecast reconciliation methods. Then we roll the forecast origin forward by one month and repeat the process, until July 2022. We note that it may be challenging to identify the series with "poor" forecasts due to structural changes in the data caused by the COVID-19 pandemic, which affect the accuracy of forecasts across all time series.

```{r}
#| label: tbl-labour-rmse-avg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Average out-of-sample forecast results for Australian labour force data.

rmse_labour_avg <- readRDS("results/labour_rmse.rds")
latex_table(rmse_labour_avg)
```

The average results are presented in @tbl-labour-rmse-avg. The poor performance of the MinT method and associated methods can be attributed to the poor sample covariance estimator when the sample size is only slightly larger than the number of series in the structure. Subset methods using different estimators of $\bm{G}$ generally improve forecast accuracy over their benchmark methods, particularly when focusing on aggregation levels, which are typically of paramount concern to practitioners. The only exception is the WLSs-subset method, which returns reduced accuracy for longer horizons. However, it still demonstrates improvements in top-level forecasts. Moreover, the Intuitive and Lasso methods almost always yield results identical to the corresponding benchmark methods, because they tend to provide dense estimates, and ETS models typically do not result in extremely poor forecasts. The only exception is OLS-intuitive, which shows improved forecast accuracy at the top level but deterioration at other levels. When we drop the unbiasedness assumption, EMinT is the worst-performing method across all levels because it relies on the assumption that the series in the hierarchy are jointly weakly stationary, which is evidently not the case in the application. Elasso significantly improves the quality of forecasts over EMinT, with the most accurate coherent forecasts observed at the top level and STT level. Overall, Elasso performs well for longer forecast horizons, but it is less effective for one-step-ahead forecasts.

We provide results based on the final test set spanning from August 2022 to July 2023 in @tbl-labour-rmse. This is the latest available data, enabling us to use more data for model training and explore the post-COVID pattern. All Subset methods (using various estimators of $\bm{W}$) produce improved or comparable reconciled forecasts compared to their benchmarks. The accuracy improvements become more noticeable for longer forecast horizons. Similar to the average results in @tbl-labour-rmse-avg, the Intuitive and Lasso methods yield results identical to the benchmark methods due to their tendency to offer dense estimates. Surprisingly, when relaxing the unbiasedness constraint, Elasso ranks the best and demonstrates significant improvement over EMinT, and outperforms other methods across almost all levels except for the top level.

@tbl-labour-info presents the number of series selected at each level and the optimal tuning parameter values obtained using proposed methods. We only show results from Subset and Elasso methods, as they return the best RMSE results. The scale variation in optimal parameters for different methods is due to objective function scales. @tbl-labour-info shows that all Subset methods exclude some series. Remarkably, the Elasso method consistently outperforms the others overall, even though it uses only $11$ series for forecast reconciliation. Most of the series at the STT level are removed, while the majority of series at the Duration level are retained. This aligns with our data description, highlighting that the seasonal patterns in the Duration level series are more consistent and potentially easier to forecast compared to those at the STT level.

### Forecasting Australian domestic tourism {#sec-tourism}

Australian domestic tourism flows are measured as the number of overnight trips Australians spend away from home. The data are sourced from the National Visitor Survey and collected through computer-assisted telephone interviews with approximately $120,000$ residents aged $15$ years and older. The data follow a geographic structure, with national total tourism flows at the top level, then disaggregated into seven states and territories (referred to as *State* level hereafter), further dividing into $27$ zones, and finally into $76$ regions. Thus, $n_b=76$ and $n=111$. Each series spans January 1998 to December 2017, with a total of $240$ observations.

```{r}
#| label: fig-tourism-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 9
#| fig-height: 6
#| fig-cap: Domestic tourism flows from January 1998 to December 2017 for the whole of Australia as well as the states.
tourism_ts <- readRDS("results/tourism_hts.rds")
tourism_ts |>
  autoplot(Value) +
  facet_wrap(vars(Series), scales = "free_y", ncol = 2) +
  xlab("Time") +
  ylab("Australian domestic tourism flows ('000)") +
  theme_bw() +
  theme(legend.position = "none",
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold", size = 14),
        axis.title.x = element_text(face = "bold", size = 12),
        axis.text = element_text(face = "bold", size = 10),
        axis.ticks.x.top = element_blank())
```

@fig-tourism-data shows aggregate tourism flows for Australia and individual states, revealing pronounced seasonal patterns across the national total and states, albeit with varying seasonal patterns among series. Notably, significant growth began around 2010 for the national total flow and some states such as NSW, VIC, QLD, and WA. While flows are relatively flat for SA, TAS, and NT. Moreover, the time plot displays a large decrease in tourism flows for WA in 2016.

Our objective is to forecast tourism flows for each series in the geographic hierarchy while ensuring coherence across all levels. We use a rolling forecast origin to evaluate the forecast accuracy of different methods. We start with a training set of $216$ months for each series, and compute base forecasts from optimal ETS models. We then roll the forecast origin forward, month by month, until December 2016. The base forecasts are reconciled using our proposed methods and some existing reconciliation methods.

@tbl-tourism-rmse-avg reports average RMSE values for base forecasts generated by ETS models, along with the percentage relative improvements obtained by each reconciliation method. Similar to @sec-labour, MinT and the respective proposed methods are not considered due to their poor performance. The results show that the OLS method outperforms other benchmarks like WLSs, WLSv and MinTs, despite the fact that WLSv and MinTs account for the in-sample covariance of base forecast errors. This highlights the effectiveness of the OLS method despite its simplicity.

Overall, the Subset methods outperform their respective benchmark methods, especially for aggregation levels and longer horizons. The only exception is the OLS-subset method, which slightly reduces overall accuracy while improving top-level forecasts. Intuitive and Lasso methods produce results almost identical to the corresponding benchmarks, which is not surprising as ETS models rarely yield extremely poor forecasts, making them challenging to be selected out using methods that tend to return dense estimates. When we relax the unbiasedness constraint, EMinT consistently performs the worst across all levels due to the evident lack of joint weak stationarity among the series in the hierarchy. The Elasso method shows significant improvement compared to EMinT, and outperforms other methods across almost all levels except the bottom level.

```{r}
#| label: tbl-tourism-rmse-avg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Average out-of-sample forecast results for Australian domestic tourism data.

rmse_tourism_avg <- readRDS("results/tourism_rmse.rds")
latex_table(rmse_tourism_avg)
```

@tbl-tourism-rmse present the results based on the last one training set spanning from January 2017 to December 2017. The reconciliation errors across $111$ series and across four hierarchy levels are displayed in @fig-tourism-rmse. The results show a similar performance to the average results described above, indicating relatively high-quality forecasts from Subset and Elasso methods.

```{r}
#| label: fig-tourism-rmse
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 10
#| fig-height: 3
#| fig-pos: "!t"
#| fig-cap: Average out-of-sample forecasting performance, measured in terms of RMSE (from 1- to 12-step-ahead), for each series across different reconciliation methods. Time series are arranged along the horizontal axis.

tourism_heatmap <- readRDS("results/tourism_heatmap.rds")
ggplot(tourism_heatmap, aes(x = Series, y = Method, fill = RMSE)) +
  geom_tile() +
  geom_vline(xintercept = c(1.5, 8.5, 35.5), linetype = "dashed", linewidth = 0.5) +
  scale_fill_gradientn(colors = c("#f7d9a6",
                                  rev(hcl.colors(100, "Purples"))[c(seq(1, 50, 10), seq(51, 100, 1))])) +
  labs(x = "Time series", y = "") +
  scale_x_continuous(expand = c(0, 0),
                     breaks =  seq(20, 100, 20),
                     sec.axis = dup_axis(name = "",
                                         breaks = c(4.5, 11.5, 39),
                                         labels = c("States", "Zones", "Regions"))) +
  scale_y_discrete(expand = c(0, 0),
                   limits = rev(c("Base", "BU", "OLS", "OLS-subset",
                                  "WLSs", "WLSs-subset", "WLSv", "WLSv-subset",
                                  "MinTs", "MinTs-subset", "EMinT", "Elasso"))) +
  theme(
    plot.background = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, linewidth=1),
    axis.title.x = element_text(face = "bold", size = 12),
    legend.title = element_text(face = "bold", size = 10),
    legend.text = element_text(face = "bold", size = 10),
    legend.position = "right",
    axis.text = element_text(face = "bold", size = 10),
    axis.ticks.x.top = element_blank()
  ) +
  guides(fill = guide_colourbar(barwidth = 1.5,
                                barheight = 10))
```

Additionally, @tbl-tourism-info summarizes the number of series selected using proposed methods as well as the optimal tuning parameter values identified. Here we focus on the Subset and Elasso methods since they are useful in the tourism application. Note that the scale variation in optimal parameters for different methods comes from to objective function scales. We observe that the OLS-subset and WLSs-subset methods exclude some series at the State and Zone levels for reconciliation. In contrast, the WLSv and MinTs methods retain all series, benefiting from the consideration of in-sample covariance, which enables larger adjustments made to series with large in-sample forecast error variances during reconciliation. Nonetheless, the WLSv and MinTs methods can still enhance the quality of reconciled forecasts due to the inclusion of shrinkage through ridge regularization. It is surprising that Elasso performs exceptionally well despite using only $13$ series for reconciliation.

## Conclusion {#sec-conclusion}

In existing forecast reconciliation literature, we map all base forecasts into bottom-level disaggregated forecasts, which are then summed to yield coherent forecasts for the entire structure. The mapping step can be conceptually regarded as a forecast combination. It is common that the base forecasts for some time series perform poorly. This may reduce overall reconciliation effectiveness. In this paper, we have addressed this issue by introducing a selection mechanism to forecast reconciliation; i.e., incorporating time series selection when reconciling forecasts, while ensuring coherent forecasts for all series.

Under the unbiasedness constraint, we developed three reconciliation methods with selection mechanisms to automatically remove some base forecasts when forming reconciled forecasts. These methods include group best-subset selection with ridge regularization (Subset), an intuitive method with $L_0$ regularization (Intuitive), and a group lasso method (Lasso). These methods use different penalty functions designed to penalize the columns of the weighting matrix, $\bm{G}$, towards zero. Additionally, we relaxed the unbiasedness constraint and proposed the empirical group lasso method (Elasso) which selects series based on in-sample observations and fitted values.

Simulation experiments and two empirical applications demonstrated the superiority of the proposed methods over existing reconciliation methods without series selection. When model misspecification was introduced for some series in the hierarchy, our proposed methods ensured coherent forecasts that outperformed or, at least, matched their respective benchmarks in the minimum trace reconciliation framework. In both empirical applications, where no apparent model misspecification was present, Subset and Elasso methods were always preferred, especially for aggregation levels and longer forecast horizons, while Intuitive and Lasso methods yielded results identical to corresponding benchmarks, as they tend to provide dense estimates.

A feature of the proposed methods is their ability to reduce the disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating the challenges associated with estimator selection, which is a prominent issue within the field of forecast reconciliation research.

As the number of series grows, solving these problems efficiently becomes challenging, and the exact computation of these estimators remains a hurdle. In our study, we have used Gurobi, one of the most widely used commercial solvers, to address NP-hard MIP problems. Despite various efforts to develop MIP-based approaches for solving $L_0$-regularized regression problems, extending these methods to incorporate additional constraints remains a challenge. We leave this aspect to be addressed in future research.

## Supplementary materials {.unnumbered}

**Appendix:** Additional results obtained in @sec-simulations and @sec-applications. (.pdf file)

## References {.unnumbered}

::: {#refs}
:::

```{=tex}
\newpage
\appendix
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
```

## Appendix {.unnumbered}

The section provides additional results obtained in @sec-simulations and @sec-applications.

```{r}
#| label: tbl-s2-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario B, Setup 1.

rmse_s2 <- readRDS("results/sim_rmse_s2.rds")
latex_table(rmse_s2)
```

```{r}
#| label: tbl-s3-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario C, Setup 1.

rmse_s3 <- readRDS("results/sim_rmse_s3.rds")
latex_table(rmse_s3)
```

```{r}
#| label: tbl-s2-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario B, Setup 1.

latex_sim_nos_table(selection_sim$out_s2$z, selection_sim$out_s2$n, "s2")
```

```{r}
#| label: tbl-s3-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario C, Setup 1.

latex_sim_nos_table(selection_sim$out_s3$z, selection_sim$out_s3$n, "s3")
```

```{r}
#| label: tbl-corr-selection-pos
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Setup 2, with the error correlation being 0.8.

selection_corr_pos <- readRDS("results/corr_selection_pos.rds")
latex_sim_nos_table(selection_corr_pos$out_s0$z,
                    selection_corr_pos$out_s0$n,
                    "corr_pos")
```

```{r}
#| label: tbl-labour-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results on a single test set (from August 2022 to July 2023) for Australian labour force data.

rmse_labour <- readRDS("results/labour_1_rmse.rds")
latex_table(rmse_labour)
```

```{r}
#| label: tbl-labour-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Number of time series selected using different proposed methods and the optimal parameter values identified in the labour application, considering a single test set (from August 2022 to July 2023). The None row shows the original number of series in the structure.

labour_info <- readRDS("results/labour_info.rds")

options(knitr.kable.NA = '-')
labour_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```

```{r}
#| label: tbl-tourism-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results on a single test set (from January 2017 to December 2017) for Australian domestic tourism data.

rmse_tourism <- readRDS("results/tourism_1_rmse.rds")
latex_table(rmse_tourism)
```

```{r}
#| label: tbl-tourism-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Number of time series selected using different proposed methods and the optimal parameter values identified in the tourism application, considering a single test set (from  January 2017 to December 2017). The None row shows the original number of series in the structure.

tourism_info <- readRDS("results/tourism_info.rds")

options(knitr.kable.NA = '-')
tourism_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```
