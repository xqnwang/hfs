---
title: "Optimal forecast reconciliation with time series selection"
author:
- name: Xiaoqian Wang
  acknowledgements: |
      Corresponding author.
- name: Rob J Hyndman
- name: Shanika L Wickramasuriya
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
abstract: Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can improve overall forecast accuracy in hierarchical or grouped structures, the most substantial improvements occur in series with initially poor base forecasts, and some series may still experience a deterioration in reconciled forecasts. However, in practice, some series in a structure often have poor base forecasts due to model misspecification or low forecastability. To address this, we propose two categories of forecast reconciliation methods that incorporate time series selection based on out-of-sample and in-sample information, respectively. Our methods keep "poor" base forecasts of some series unused in forming reconciled forecasts, preventing their negative influence on the reconciled forecasts. This process adjusts the weights allocated to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods can reduce disparities arising from using different estimates of the base forecast error covariance matrix, thus alleviating the challenge of estimator selection. We evaluate the proposed methods through two simulation studies and two empirical applications using Australian labour force data and domestic tourism data, showing improved accuracy compared to alternative methods, especially for aggregation levels, longer forecast horizons, and under model misspecification.
keywords: "Coherent, Hierarchical time series, Grouped time series, Linear forecast reconciliation, Optimization problem"
bibliography: references.bib
format:
  asa-pdf:
    keep-tex: true
    pdf-engine: pdflatex
    classoption: 11pt
    journal:
      blinded: false
include-in-header: preamble.tex

execute:
  echo: false
  warning: false
  message: false
  cache: false
date: last-modified
---

```{r}
#| label: load
#| cache: false
# Load all required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(fabletools)
library(patchwork)
library(knitr)
library(kableExtra)
library(latex2exp)

source("../R/nemenyi.R") # MCB test
source("../R/analysis.R") # Other functions used for analysis

#theme_set(theme_get() + theme(text = element_text(family = "Source Sans Pro")))
```

\spacingset{1.65}

## Introduction {#sec-introduction}

Hierarchical time series are characterized as a set of time series organized in a unique hierarchical aggregation structure, while grouped time series arise when attributes of interest are crossed rather than nested in the aggregation structure [@Hyndman2016-cz]. For example, unemployment data, a crucial social and economic indicator, is a natural example of a grouped time series. The analysis of the number of unemployment persons in a country is crucial for informed policymaking and economic research. It is also valuable to examine attributes such as labour market region and the length of time that unemployed people have been looking for work. Such a disaggregation allows us to identify regional disparities, and comprehend the structural nuances underlying unemployment. Forecasts in such a hierarchical or grouped structure should adhere to some known aggregation constraints to maintain coherence, which is a vital aspect for aligned decision making.

Earlier studies perform forecast reconciliation by focusing only on a single level of the structure, subsequently aggregating or disaggregating their forecasts to produce coherent forecasts for other levels of the structure. These single-level methods typically fall into three categories, namely bottom-up [@Dunn1976-op], top-down [@Gross1990-lg], and middle-out [@Athanasopoulos2009-ps]. However, these methods only use information from a single level while overlooking valuable insights available at other levels and the intricate relationships in the structure.

To overcome these limitations, @Hyndman2011-sd introduced a reconciliation approach using a linear regression model based on forecasts from all series within a given structure, resulting in a generalized least squares (GLS) solution. This method initially generates independent base forecasts for all series in the structure and subsequently adjusts these forecasts to make them coherent. Following this work, further research has led to the modifications of the least squares-based reconciliation method in various frameworks, including cross-sectional data [@Hyndman2016-cz; @Wickramasuriya2019-fc; @Panagiotelis2021-mf], temporal data [@Athanasopoulos2017-jj; @Nystrup2020-di], and cross-temporal data [@Di_Fonzo2023-vo]. In particular, assuming unbiased base forecasts, @Wickramasuriya2019-fc proposed the minimum trace method, which formulates the reconciliation problem as minimizing the trace of the covariance matrix of reconciled forecast errors. These reconciliation approaches have been demonstrated to produce coherent and potentially more accurate forecasts compared to traditional single-level methods in various empirical applications [see, for example, @Taieb2021-tc; @Panagiotelis2021-mf; @Wickramasuriya2023-hn]. Additionally, @Van_Erven2015-ir, @Wickramasuriya2019-fc, @Panagiotelis2021-mf, and @Wickramasuriya2021-am provided theoretical insights into the performance of forecast reconciliation methods. Please refer to @Athanasopoulos2023-sm for a comprehensive introduction of various forecast reconciliation methods.

Reconciliation is well recognized for its ability to improve overall forecast accuracy in structures with aggregation constraints. Theoretically, the mean squared reconciled forecast error from the minimum trace reconciliation method for each series in the structure is lower than that of OLS and base forecasts [@Wickramasuriya2021-am]. However, it is important to acknowledge that reconciled forecasts for some series in the structure may experience deterioration in its forecasting performance. As shown by @Athanasopoulos2017-jj, most of the improvements attributed to reconciliation are observed in series with initially poor-performing base forecasts. Thus, they suggested that the ideal solution would be to combine the most accurate aspects of base forecasts from each level, aiming to avoid a myopic view from a single level. In practice, it is not uncommon for some series in a structure to perform poorly in their base forecasts due to inherent challenges that are difficult to completely mitigate in real-world situations. These challenges may include model misspecification or low forecastability resulting from the absence of discernible patterns (low signal-to-noise ratio). In such cases, it becomes crucial to exclude "poor" base forecasts of some series in a structure when performing reconciliation, thereby preventing their negative influence on reconciled forecasts. This forms the primary objective of this paper.

This paper addresses a few gaps in the field of forecast reconciliation. First, we propose forecast reconciliation methods that incorporate time series selection based on out-of-sample information, assuming unbiased base forecasts. We formulate this as an optimization problem, using different penalty functions designed to control the number of nonzero column entries in the weighting matrix for linear forecast reconciliation. We theoretically show that the number of selected time series is at least equal to the number of series at the bottom level, and we can reconstruct the full hierarchical structure by aggregating/disaggregating the selected series. Second, we relax the unbiasedness assumption and introduce another reconciliation method with selection, utilizing in-sample observations and their fitted values. This allows us to use the in-sample reconciliation performance for selection purposes. In this case, it may happen that less than the number of series at the bottom level are used for reconciliation. We carry out simulation experiments and two empirical applications, showing that our proposed methods guarantee coherent forecasts that outperform or, at the very least, match their respective benchmark methods. The improvements are particularly pronounced when focusing on aggregation levels, longer forecast horizons, and model misspecification. A remarkable feature of the proposed methods is their ability to reduce the disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating the challenges associated with estimator selection, which is a prominent issue in the field of forecast reconciliation research.

The remainder of the paper is structured as follows. @sec-preliminaries presents the notations and a review of linear forecast reconciliation methods. @sec-methodology introduces our proposed methods to achieve time series selection in reconciliation, and provides some theoretical insights. @sec-simulations and @sec-applications show the results from simulations and two real-world datasets, respectively, followed by concluding remarks in @sec-conclusion. The R code for reproducing the results is available at LINK.

## Preliminaries {#sec-preliminaries}

### Notation

We denote the set $\{1,\ldots,k\}$ by $[k]$ for any positive integer $k$. A *hierarchical time series* can be considered as an $n$-dimensional multivariate time series that adheres to known linear constraints. Let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations of all time series in the hierarchy at time $t$, and $\bm{b}_t \in \mathbb{R}^{n_b}$ be a vector comprising observations of all bottom-level time series at time $t$. The full hierarchy at time $t$ can be written as
$$
\bm{y}_t = \bm{S}\bm{b}_t,
$$
for $t=1,2,\ldots,T$, where $T$ is the length of the time series, and $\bm{S}$ is an $n \times n_b$ *summing matrix* that shows aggregation constraints present in the structure. We can write the summing matrix as $\bm{S} = \left[\begin{array}{c}\bm{A} \\ \bm{I}_{n_b}\end{array}\right]$, where $\bm{A}$ is an $n_a \times n_b$ *aggregation matrix* with $n = n_a + n_b$, and $\bm{I}_{n_b}$ is an $n_b$-dimensional identity matrix.

![An example of a two-level hierarchical time series.](figs/hts_example.pdf){#fig-hts fig-align="center" width="40%"}

To clarify these notations, consider the example of a simple hierarchy in @fig-hts. For this two-level hierarchy, we have $n = 7$, $n_b = 4$, $n_a = 3$, $\bm{y}_t = [y_{\text{Total},t}, y_{\text{A},t}, y_{\text{B},t}, y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, $\bm{b}_t = [y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, and
$$
\bm{S} = \left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\bm{I}_4}
\end{array}\right].
$$

When data structure does not naturally disaggregate in a unique hierarchical manner, we can combine these hierarchical structures to form a *grouped time series*. Thus, grouped time series can also be considered as hierarchical time series with more than one grouping structure. Please refer to @Hyndman2021-fo on further details.

### Linear forecast reconciliation

Let $\hat{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ be a vector of $h$-step-ahead *base forecasts* for all time series in the structure, given observations up to and including time $T$, and stacked in the same order as $\bm{y}_t$. We can use any method to generate these forecasts, but in general they will not add up especially when we forecast each series independently.

When forecasting these time series structure, we expect the forecasts to be *coherent* (i.e., aggregation constraints of the data are satisfied). Let $\tilde{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ denote a vector of $h$-step-ahead *reconciled forecasts* which are coherent by construction, $\psi$ a *mapping* that reconciles base forecasts, $\hat{\bm{y}}_{T+h \mid T}$. Then we have *forecast reconciliation* $\tilde{\bm{y}}_{T+h \mid T}=\psi(\hat{\bm{y}}_{T+h \mid T})$, which is essentially a post-processing method. In this paper, we focus on linear forecast reconciliation given by
$$
\tilde{\bm{y}}_{T+h \mid T} = \bm{S}\bm{G}_h\hat{\bm{y}}_{T+h \mid T},
$$ {#eq-lr}
where

-   $\bm{G}_h$ is an $n_b \times n$ weighting matrix that maps base forecasts into the bottom level. It combines all base forecasts to form reconciled forecasts for bottom-level series.
-   $\bm{S}$ is an $n \times n_b$ summing matrix that sums up bottom-level reconciled forecasts to produce coherent forecasts of all levels. It identifies the linear aggregation constraints in a given structure.

#### Minimum trace reconciliation

Let the $h$-step-ahead in-sample *base forecast errors* be defined as $\hat{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \hat{\bm{y}}_{t+h \mid t}$, and the $h$-step-ahead *reconciled forecast errors* be defined as $\tilde{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h \mid t}$ for $t = 1,2,\ldots,T-h$. @Wickramasuriya2019-fc formulated a linear reconciliation problem as minimizing the trace (MinT) of the $h$-step-ahead covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\bm{e}}_{t+h \mid t})$. Under the assumption of unbiasedness of base forecasts and reconciled forecasts, the unique solution of the minimization problem is given by
$$
\bm{G}_h=\left(\bm{S}^{\prime} \bm{W}_h^{-1} \bm{S}\right)^{-1} \bm{S}^{\prime} \bm{W}_h^{-1},
$$ {#eq-mint}
where $\bm{W}_h$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors.

The trace minimization problem can be reformulated as a least squares problem with linear constraints given by
$$
 \min _{\tilde{\bm{y}}_{T+h \mid T}} \quad \frac{1}{2}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})^{\prime} \bm{W}_{h}^{-1}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})
 \qquad \text { s.t. } \quad \tilde{\bm{y}}_{T+h \mid T}=\bm{S}\tilde{\bm{b}}_{T+h \mid T},
$$ {#eq-mint_op}
where $\tilde{\bm{b}}_{T+h \mid T} \in \mathbb{R}^{n_b}$ is the vector comprising $h$-step-ahead bottom-level reconciled forecasts, made at time $T$. Focusing on $\bm{W}_h$, the intuition behind the MinT reconciliation is that **the larger the estimated variance of the base forecast errors, the larger the range of adjustments permitted for forecast reconciliation**.

It is challenging to estimate $\bm{W}_h$, especially for $h > 1$. Assuming that $\bm{W}_h = k_h\bm{W}_1$, $\forall h$, where $k_h > 0$, the MinT solution of $\bm{G}$ does not change with the forecast horizon, $h$. Hence, we will drop the subscript $h$ for the ease of exposition. The most popularly used candidate estimators for $\bm{W}$ in the forecast reconciliation literature are listed in Table \ref{tbl-bench}.

```{=tex}
\begin{table}[!h]
\caption{Forecast reconciliation methods for which different estimators of $\bm{W}$ are used.}\label{tbl-bench}
\centering
\begin{threeparttable}
\begin{tabular}{p{0.7\linewidth}r}
\toprule
Reconciliation method & $\bm{W}_h \propto$\\
\midrule
\textbf{OLS} \citep{Hyndman2011-sd} & $\bm{I}$\\
\textbf{WLSs} \citep{Athanasopoulos2017-jj} & $\operatorname{diag}(\bm{S} \bm{1})$\\
\textbf{WLSv} \citep{Hyndman2016-cz} & $\operatorname{Diag}(\hat{\bm{W}}_1)$\\
\textbf{MinT} \citep{Wickramasuriya2019-fc} & $\hat{\bm{W}}_1$\\
\textbf{MinTs} \citep{Wickramasuriya2019-fc} & $\lambda\operatorname{Diag}(\hat{\bm{W}}_1) + (1-\lambda)\hat{\bm{W}}_1$\\
\bottomrule
\end{tabular}
\begin{tablenotes}[para]
\linespread{1}\small
\footnotesize
\underline{\textit{NOTE:}}
$\bm{1}$ is a vector of 1s of size $n_b$, $\operatorname{diag}(\cdot)$ constructs a diagonal matrix using a given vector, $\hat{\bm{W}}_1$ denotes the unbiased covariance estimator based on the in-sample one-step-ahead base forecast errors (i.e., residuals), and $\operatorname{Diag}(\cdot)$ forms a diagonal matrix using the diagonal elements of the input matrix.
\end{tablenotes}
\end{threeparttable}
\end{table}
```

In practice, it is hard to say which estimator for $\bm{W}$ works better. @Pritularga2021-lz demonstrated that the performance of forecast reconciliation is affected by two sources of uncertainties, i.e., the base forecast uncertainty and the reconciliation weight uncertainty. Recall that the uncertainty in the MinT solution in @eq-mint is introduced by the uncertainty in the weighting matrix as the summing matrix is fixed for a given structure. This indicates that OLS and WLSs estimators for $\bm{W}$ may lead to less volatile reconciliation performance compared to WLSv, MinT, and MinTs estimators. @Panagiotelis2021-mf provided a geometric intuition for reconciliation and showed that, when considering the Euclidean distance loss function, OLS reconciliation yields results that are at least as favorable as the base forecasts, whereas MinT reconciliation performs poorly relative to the base forecasts. However, when considering the expected Euclidean distance of the reconciled forecast error, @Wickramasuriya2021-am indicated that MinT reconciliation is better than OLS reconciliation. Therefore, which estimator for $\bm{W}$ to use hinges on the specific time series structure of interest, the targeted level or series, and the selected loss function.

#### Relaxation of the unbiasedness assumptions

Both @Hyndman2011-sd and @Wickramasuriya2019-fc impose two unbiasedness conditions, i.e., the base forecasts and the reconciled forecasts are unbiased. @Ben_Taieb2019-be proposed a reconciliation method relaxing the assumption of unbiasedness. Specifically, by expanding the training window forward by one observation until $T-h$, they formulated the reconciliation problem as a regularized empirical risk minimization (RERM) problem given by
$$
\min _{\bm{G}_h} \frac{1}{(T-T_1-h+1)n}\left\|\bm{Y}_{h}^{*}-\hat{\bm{Y}}_{h}^{*} \bm{G}_{h}^{\prime} \bm{S}^{\prime}\right\|_F^2+\lambda\|\operatorname{vec}( \bm{G}_h)\|_1,
$$
where $T_1$ denotes the minimum number of observations used for model training, $\left\| \cdot \right\|_F$ is the Frobenius norm, $\|\cdot\|_1$ is the $L_1$ norm, $\operatorname{vec}(\cdot)$ denotes the vectorization of a matrix, which stacks the columns of the matrix on top of one another, $\bm{Y}_{h}^{*}=\left[\bm{y}_{T_1+h}, \ldots, \bm{y}_T\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$, $\hat{\bm{Y}}_{h}^{*}=\left[\hat{\bm{y}}_{T_1+h \mid T_1}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$, and $\lambda \geq 0$ is a regularization parameter.

When $\lambda = 0$, the problem reduces to an empirical risk minimization (ERM) problem without regularization. Assuming that the series in the structure are jointly weakly stationary and $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is invertible, it has a closed-form solution given by
$$
\hat{\bm{G}}_h = \bm{B}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\left(\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\right)^{-1},
$$
where $\bm{B}_{h}^{*}=\left[\bm{b}_{T_1+h}, \ldots, \bm{b}_T\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$. If $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is not invertible, they suggested using a generalized inverse.

When $\lambda > 0$, imposing such a $L_1$ penalty on $\bm{G}_h$ will introduce sparsity and reduce estimation variance, albeit at the cost of introducing some bias. In addition, they also proposed another strategy that penalizes the matrix $\bm{G}_h$ towards the solution obtained by bottom-up (**BU**) method, i.e., $\bm{G}_{\text{BU}} = \left[\bm{0}_{n_b \times n_a} \mid \bm{I}_{n_b}\right]$.

Following the work, @Wickramasuriya2021-am proposed an empirical MinT (**EMinT**) without the unbiasedness constraint by minimizing the trace of the covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\bm{e}}_{T+h \mid T})$. Assuming that the series are jointly weakly stationary, she derived the solution given by
$$
\hat{\bm{G}}_{h} = \bm{B}_{h}^{\prime}\hat{\bm{Y}}_{h}\left(\hat{\bm{Y}}_{h}^{\prime}\hat{\bm{Y}}_{h}\right)^{-1},
$$
where $\bm{B}_{h}=\left[\bm{b}_{h}, \ldots, \bm{b}_T\right]^{\prime} \in \mathbb{R}^{\left(T-h+1\right) \times n}$, and $\hat{\bm{Y}}_{h}=\left[\hat{\bm{y}}_{h \mid 0}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime} \in \mathbb{R}^{\left(T-h+1\right) \times n}$. The difference between EMinT and ERM lies in the data sources, as EMinT uses in-sample observations and base forecasts, while ERM relies on observations and base forecasts from a holdout validation set. We note that both ERM and EMinT consider an estimate of $\bm{G}$ that changes over the forecast horizon, which is why we keep the subscript $h$ here. When reporting the EMinT results that follow, we assume the weighting matrix $\bm{G}$ for $h=1$ holds for $h>1$.

In practice, a prevalent challenge in forecast reconciliation arises when the base forecasts of some time series within the structure may perform poorly, especially for large hierarchies. This can be attributed to either the inherent complexity of forecasting these series or potential model misspecification. In such cases, the effectiveness of forecast reconciliation may diminish, as the role of the weighting matrix $\bm{G}$ is to assimilate *all* base forecasts and map them into bottom-level disaggregated forecasts which are subsequently summed by $\bm{S}$. While the RERM method proposed by @Ben_Taieb2019-be introduces sparsity by shrinking some elements of $\bm{G}$ towards zero, it remains incapable of mitigating the adverse impact of underperforming base forecasts on the quality of the reconciled forecasts. Moreover, the method is time-consuming and might be problematic when there are limited number of observations because it uses expanding windows to recursively generate out-of-sample base forecasts, which are then used in the minimization problem.

We therefore propose two categories of innovative methods, constrained out-of-sample (under the unbiasedness assumption) and unconstrained in-sample (without unbiasedness assumption) forecast reconciliation with time series selection. These methods aim to identify and address the negative effect of some base forecasts of poor performance in a structure on the overall performance of the reconciled forecasts. Additionally, through the incorporation of regularization in our objective function, our method has the potential to enhance reconciliation outcomes produced by using a "poor" choice of $\bm{W}$, thus reducing the risk of choosing estimator of $\bm{W}$.

## Forecast reconciliation with time series selection {#sec-methodology}

In this section, we introduce our methods for keeping forecasts of an automatically selected set of series, identified as harmful to reconciliation, unused in forming reconciled forecasts, i.e., forecast reconciliation with time series selection. @sec-constrained introduces constrained reconciliation methods with selection that formulate the problem based on out-of-sample base forecasts, while @sec-unconstrained presents an unconstrained reconciliation method with selection, where we formulate the problem based on in-sample observations and base forecasts.

### Series selection with unbiasedness constraint {#sec-constrained}

As $\bm{S}$ is fixed and $\hat{\bm{y}}_{T+h \mid T}$ is given, once we get the estimation of $\bm{G}$, the linear reconciliation performance is determined, as shown in @eq-lr. In this section, the subscript $h$ is dropped as we assume $\bm{W}$ and $\bm{G}$ do not vary with the forecast horizon. A natural way to keep forecasts of some series unused in reconciliation is through controlling the number of nonzero column entries in $\bm{G}$. This leads to a generalization of the MinT optimization problem by applying an additional penalty to the objective function. More precisely, let $\hat{\bm{y}}:=\hat{\bm{y}}_{T+1 \mid T}$, we consider the optimization problem given by
$$
\min _{\bm{G}} \quad \frac{1}{2}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)
+ \lambda\mathfrak{g}(\bm{G}) \qquad
\text { s.t. } \quad \bm{GS}=\bm{I},
$$ {#eq-op_u}
where $\mathfrak{g}(\cdot)$ is defined as an exterior penalty function designed to penalize the columns of $\bm{G}$ towards zero, with $\lambda$ is the corresponding penalty coefficient. Thus, this can be considered as *a grouped variable selection problem*, with each group corresponding to a column of $\bm{G}$. Obviously, these groups are not overlapped. The constraint, $\bm{GS}=\bm{I}$, reflects the assumption that base forecasts and reconciled forecasts are unbiased. When $\lambda = 0$, $\forall h$, the problem reduces to the MinT optimization problem in @eq-mint_op with a closed-form solution given by @eq-mint.

**Proposition 1.** *Under the assumption of unbiasedness, the count of nonzero column entries of* $\bm{G}$ (*i.e., the number of time series selected for reconciliation*), *derived through solving @eq-op_u, is at least equal to the number of time series at the bottom level. In addition, we can restore the full hierarchical structure by aggregating/disaggregating the selected time series.*

*Proof*. According to the unbiasedness constraint $\bm{GS}=\bm{I}$, we have
$$
\min \left(\operatorname{rank}(\bm{G}), \operatorname{rank}(\bm{S})\right) \geq \operatorname{rank}(\bm{I}_{n_b})=n_b,
$$
which indicates that the count of nonzero column entries of $\bm{G}$ is at least equal to $n_b$.

Let $\bm{X}_{\cdot \mathbb{S}} \in \mathbb{R}^{r \times |\mathbb{S}|}$ denote the submatrix of the $r \times c$ matrix $\bm{X}$ with column indices forming a set $\mathbb{S}$ (and when $\mathbb{S} = \{j\}$, we simply use $\bm{X}_{\cdot j}$), where $|\mathbb{S}|$ denotes the size of the set $\mathbb{S}$. Similarly, let $\bm{X}_{\mathbb{S}\cdot} \in \mathbb{R}^{|\mathbb{S}| \times c}$ denote the submatrix of $\bm{X}$ whose rows are indexed by a set $\mathbb{S}$ (and when $\mathbb{S} = \{i\}$, we simply use $\bm{X}_{i\cdot}$). Assuming that the set $\mathbb{S}$ consists of the indices of nonzero columns in the solution of @eq-op_u, $\hat{\bm{G}}$, the following equations hold:
$$
\bm{G}\bm{S} = \hat{\bm{G}}_{\cdot \mathbb{S}}\bm{S}_{\mathbb{S}\cdot} = \bm{I},
\qquad\text{and}\qquad
\min \left(\operatorname{rank}(\hat{\bm{G}}_{\cdot \mathbb{S}}), \operatorname{rank}(\bm{S}_{\mathbb{S}\cdot})\right) \geq \operatorname{rank}(\bm{I}_{n_b})=n_b.
$$
Additionally, we have $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) \leq n_b$ as $\bm{S}$ has $n_b$ columns. Therefore, we can conclude that $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) = n_b$. Moreover, we have
$$
\bm{y}_t = \bm{S}\bm{b}_t = \bm{S}\bm{G}\bm{S}\bm{b}_t=\bm{S}\hat{\bm{G}}_{\cdot\mathbb{S}}\bm{S}_{\mathbb{S}\cdot}\bm{b}_t=\bm{S}\hat{\bm{G}}_{\cdot \mathbb{S}}(\bm{y}_t)_{\mathbb{S}},
$$
which implies that the hierarchical structure can be fully restored by aggregating/disaggregating the selected time series denoted by $(\bm{y}_{t})_{\mathbb{S}}$.

For example, consider the simple hierarchy shown in @fig-hts, it is not possible for our constrained reconciliation methods with selection to simultaneously zero out columns of $\bm{G}$ associated with series AA and AB. However, it is possible to zero out columns related to series AA and BA simultaneously.

**Proposition 2.** *The optimization problem in @eq-op_u can be reformulated as a least squares problem with regularization and linear equality constraint as follows:*
$$
\begin{aligned}
& \min _{\operatorname{vec}(\bm{G})} \quad \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda\mathfrak{g}\left(\operatorname{vec}(\bm{G})\right) \\
& \text { s.t. } \quad \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}),
\end{aligned}
$$ {#eq-op_u_reg}
*which is characterized as a high-dimensional problem in which the number of features, denoted as* $p = n_b \times n$*, is much larger than the number of observations,* $n$*.*

*Proof.* We have
$$
\begin{aligned}
& \operatorname{vec}\left(\hat{\bm{y}}\right) = \hat{\bm{y}}, \\
& \operatorname{vec}\left(\bm{SG}\hat{\bm{y}}\right) = \left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}), \\
& \operatorname{vec}\left(\bm{GS}\right) = \operatorname{vec}\left(\bm{I}_{n_b}\bm{GS}\right) = \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G}).
\end{aligned}
$$
Substituting the terms in @eq-op_u with these expressions, the previous problem now takes the form of a regression problem with an additional regularization term and an equality constraint on the coefficients, as shown in @eq-op_u_reg.

Moving forward, we present three classes of regularizations we use to establish forecast reconciliation with series selection, resulting in the consideration of three optimization problems: (i) group best-subset selection with ridge regularization, (ii) intuitive method with $L_0$ regularization, and (iii) group lasso method.

#### Group best-subset selection with ridge regularization {#sec-subset}

In high-dimensional regime with $p \gg n$, a common desiderata is to assume that the true regression coefficient (i.e., $\operatorname{vec}(\bm{G})$ in our problem) is sparse. We propose to apply a combination of $L_0$ and $L_2$ regularization as the exterior penalty function to control the nonzero column entries in $\bm{G}$:
\begin{align}
\min _{\operatorname{vec}(\bm{G})} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda_0 \sum_{j=1}^n 1\left(\bm{G}_{\cdot j} \neq \bm{0}\right) + \lambda_2 \left\|\operatorname{vec}\left(\bm{G}\right)\right\|_2^2 \nonumber\\
\text { s.t. } \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}), \label{eq-subset}
\end{align}
where $1(\cdot)$ is the indicator function, $\lambda_0 \geq 0$ controls the number of nonzero columns of $\bm{G}$ selected, $\lambda_2 \geq 0$ controls the strength of the ridge regularization, and $\|\cdot\|_2$ is the $L_2$ norm. In a hierarchical or grouped time series context, the parameter of interest in Equation \ref{eq-subset}, $\operatorname{vec}(\bm{G})$, has an inherent non-overlapping grouping structure, wherein each group corresponds to a single column of $\bm{G}$, each with a size of $n_b$. Therefore, we refer to this reconciliation method as *group best-subset selection with ridge regularization*. In the results that follow, we label the **Subset** method differently based on various estimators for $\bm{W}$, referring to them as **OLS-subset**, **WLSs-subset**, **WLSv-subset**, **MinT-subset**, and **MinTs-subset**, respectively.

The inclusion of the ridge term in Equation \ref{eq-subset} is motivated by earlier work on best-subset selection [e.g., @Hazimeh2020-xd; @Mazumder2022-hx], which suggests that additional ridge regularization can mitigate the poor predictive performance of best-subset selection method in the low signal-to-noise ratio (SNR) regimes.

We present a Big-M based mixed integer programming (MIP) formulation for problem in Equation \ref{eq-subset} given by
\begin{align} \label{eq-subset_mip}
\min _{\operatorname{vec}(\bm{G}), \bm{z}, \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j + \lambda_2 \bm{g}^{+\prime}\bm{g}^{+} \\
\text { s.t. } \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right)\nonumber \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bm{G}) = \check{\bm{e}} \nonumber\\
& \sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M} z_j, \quad j \in[n] \nonumber\\
& \bm{g}^{+} \geqslant \operatorname{vec}(\bm{G}) \nonumber\\
& \bm{g}^{+} \geqslant-\operatorname{vec}(\bm{G}) \nonumber\\
& z_j \in\{0,1\}, \quad j \in[n],
\end{align}
where $\mathcal{M}$ is a Big-M parameter (a-priori specified) that is sufficiently large such that some optimal solution, say $\bm{g}^{+*}$, to Equation \ref{eq-subset_mip} satisfies $\max _{j \in [n]}\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$, the binary variable $z_j$ controls whether all the regression coefficients, $\operatorname{vec}(\bm{G})$, in group $j$ are zero or not, i.e., $z_j=0$ implies that $\bm{G}_{\cdot j}=\bm{0}$, and $z_j=1$ implies that $\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. Such Big-M formulations are commonly used in MIP problems to model relations between discrete and continuous variables, and have been recently explored in regression with $L_0$ regularization [@Bertsimas2016-ig]. The problem is a mixed integer quadratic program (MIQP) that can be solved using commercial MIP solvers, e.g., Gurobi and CPLEX.

**Parameter tuning.** To avoid computationally-expensive cross-validation, we tune the parameters to minimize the sum of squared reconciled forecast errors on the truncated training set, comprising only the $\max\{h, s\}$ observations closest to the forecast origin, where $s$ is the seasonal period for seasonal data and $s=T$ for non-seasonal data. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$ that captures the scale of first term in the objective function, where $\tilde{\bm{y}}^{\text{bench}}$ is a vector of reconciled forecasts obtained using @eq-mint with same estimator of $\bm{W}$, and define $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$. For the parameter $\lambda_0$, we consider a grid of $k+1$ values, $\{\lambda_{0}^{1},...,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. So $\lambda_{0}^{1},...,\lambda_{0}^{k}$ is a sequence decreasing on the log scale. We use a grid of six values for the parameter $\lambda_2$, $\{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$. Therefore, we tune over a two-dimensional grid of $(k+1) \times 6$ values to find the optimal combination of $\lambda_0$ and $\lambda_2$.

**Computation details.** The MIQP problem in Equation \ref{eq-subset_mip} is NP-Hard and computationally intensive. @Bertsimas2016-ig showed that commercial MIP solvers are capable of tackling problem instances for $p$ up to a thousand. To address larger instances, there has been impressive work on developing MIP-based approches for solving $L_0$-regularized regression problem, e.g., @Bertsimas2016-ig, @Hazimeh2020-xd, and @Hazimeh2022-hc. However, it is challenging to extend their approaches to accommodate additional constraints within the optimization problem. Despite the potential sluggishness of handling large instances with commercial MIP solvers, in our experiments, we use Gurobi to solve our problem in Equation \ref{eq-subset_mip} by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for cases with $p > 1000$. This enables us to terminate the solver before reaching the global optimum and return a suboptimal solution instead. This strategy is motivated by our need to consider numerous parameter candidates, and the final solution will be validated against the training set, which prevents the utilization of a very poor estimate of $\bm{G}$.

#### Intuitive method with $L_0$ regularization {#sec-intuitive}

Instead of estimating the entire matrix $\bm{G}$ in @sec-subset, we leverage the MinT solution in @eq-mint to streamline the optimization problem under consideration. Specifically, we define $\bar{\bm{S}} = \bm{A}\bm{S}$, where $\bm{A} = \operatorname{diag}(\bm{z})$ is an $n \times n$ diagonal matrix, and $\bm{z}$ is an $n$-dimensional vector with elements either equal to 0 or 1. Taking the MinT solution in @eq-mint, we have $\bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}$. Given fixed $\bm{S}$ and estimation of $\bm{W}$, $\bar{\bm{G}}$ is entirely determined by $\bm{A}$. By this way, when the $j$th diagonal element of $\bm{A}$ equals zero, the $j$th column of $\bar{\bm{G}}$ becomes entirely composed of zeros. Therefore, the optimization problem can be reduced to an integer quadratic programming (IQP) problem in which all of the variables are restricted to be integers:
\begin{align*}
\min _{\bm{A}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right) + \lambda_0 \sum_{j=1}^n \bm{A}_{jj} \\
\text { s.t. } \quad & \bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \qquad\text{and}\qquad \bar{\bm{G}}\bm{S} = \bm{I},
\end{align*}
where $\lambda_0 \geq 0$ controls the number of nonzero diagonal elements in $\bm{A}$, consequently affecting the number of nonzero columns (i.e., selected time series) in $\bm{G}$. We refer to this reconciliation method as *intuitive method with* $L_0$ *regularization*. In the results that follow, we label the **Intuitive** method differently based on various estimators for $\bm{W}$, referring to them as **OLS-intuitive**, **WLSs-intuitive**, **WLSv-intuitive**, **MinT-intuitive**, and **MinTs-intuitive**, respectively.

We should note that implementing grouped variable selection with this optimization problem can be challenging because it imposes restrictions on the parameter of interest ($\bar{\bm{G}}$) to ensure it adheres rigorously to the analytical solution of MinT while making the selection. Therefore, the resulting solution tends to be dense and may not have zero columns.

To ensure the invertibility of $\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S}$ and make the problem compatible with Gurobi, we reformulate the problem as
$$
\begin{aligned}
\min _{\bm{A},\bar{\bm{G}},\bm{C},\check{\bm{e}},\bm{z}} \quad & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j \\
\text { s.t. } \quad & \bar{\bm{G}}\bm{S} = \bm{I} \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bar{\bm{G}}) = \check{\bm{e}} \\
& \bar{\bm{G}}\bm{A}\bm{S} = \bm{I} \\
& \bar{\bm{G}} = \bm{C}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \\
& z_j \in\{0,1\}, \quad j \in[n].
\end{aligned}
$$ {#eq-intuitive_mip}

**Parameter tuning.** Similarly to the setup in @sec-subset, we select the tuning parameter, $\lambda_0$, by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $\max\{h, s\}$ observations occurred prior to the forecast origin. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$, and $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$, the collection of candidate values for $\lambda_0$ we consider is $\{\lambda_{0}^{1},...,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$.

**Computation details.** Following a setup akin to that in @sec-subset, we employ Gurobi to solve @eq-intuitive_mip by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for problems with $p > 1000$.

#### Group lasso method {#sec-lasso}

Lasso is another popular method for selection and estimation of parameters in the context of linear regression. @Yuan2006-mw introduced the group lasso method that can be used when there is a grouped structure among the variables. Here, we consider *a group lasso problem under the unbiasedness assumption* given by
$$
\begin{aligned}
\min _{\bm{G}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2 \\
\text { s.t. } \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right),
\end{aligned}
$$ {#eq-lasso}
where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make model more flexible, and the second term in the objective is the penalty function that is intermediate between the $L_1$-penalty that is used in the lasso and the $L_2$-penalty that is used in ridge regression. In the results that follow, we label the **Lasso** method based on various estimators for $\bm{W}$, referring to them as **OLS-lasso**, **WLSs-lasso**, **WLSv-lasso**, **MinT-lasso**, and **MinTs-lasso**, respectively.

Next, we present the second order cone programming (SOCP) formulation for the group lasso based estimators given by
$$
\begin{aligned}
\min _{\operatorname{vec}(\bm{G}), \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}_h^{-1}\check{\bm{e}} + \lambda \sum_{j=1}^n w_j c_j \\
\text { s.t. } \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right) \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}) = \check{\bm{e}} \\
& c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}, \quad j \in[n].
\end{aligned}
$$ {#eq-lasso_socp}
@eq-lasso_socp includes additional auxiliary variables $c_j \in \mathbb{R}_{\geq 0}$, $j \in [n]$, and second order cone constraints, $c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}$ for $j \in[n]$.

Compared to the previous two methods we proposed, the group lasso method is computationally friendlier. Nonetheless, @Hazimeh2023-ie demonstrated, both empirically and theoretically, that group $L_0$-regularized method exhibits advantages over its group lasso counterpart across a range of regimes. Group lasso can either be highly dense or possess non-zero coefficients that are overly shrunk. This issue becomes more pronounced when the groups are correlated with each other as group lasso tends to retain all correlated groups instead of seeking a more concise model.

**Penalty weights and parameter tuning.** In the context of group lasso, the default choice for the penalty weight, $w_j$, is $\sqrt{p_j}$, where $p_j$ is the size of each group (in our case, $p_j = n_b$). In our experiments, we allocate different penalty weights to each group using $w_j = 1/\left\|\bm{G}_{\cdot j}^{\text{bench}}\right\|_2$, which allows us to account for variations in scale across different time series in the structure.

We compute the group lasso over $k+1$ values of the tuning parameter $\lambda$, and select the tuning parameter by optimizing the sum of squared reconciled forecast errors on a truncated training set, consisting only of $\max\{h, s\}$ observations occurred prior to the forecast origin. The collection of candidate values for $\lambda$ under consideration is $\{\lambda^{1},...,\lambda^{k}, 0\}$, where $\lambda^{1} = \max _{j=1, \ldots, n}\left\|-\left(\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)_{\cdot j^{*}}\right)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\right\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}\left(\lambda^{k} / \lambda^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$.

**Proposition 3.** *Ignoring the unbiasedness constraint, we define* $\lambda^{1}$ *as the smallest* $\lambda$ *value such that all predictors in the group lasso problem have zero coefficients. Then we have*
$$
\lambda^{1} = \max _{j=1, \ldots, n}\left\|-\left(\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)_{\cdot j^{*}}\right)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\right\|_2 / w_j,
$$
*where* $j^{*}$ *denotes the column index of* $\hat{\bm{y}}^{\prime} \otimes \bm{S}$ *that corresponds to the* $j$*th column of* $\bm{G}$*.*

*Proof.* Denote $\bm{\beta} = \operatorname{vec}(\bm{G})$, and the first term in the objective of @eq-lasso as $L\left(\bm{\beta} \mid \bm{D}\right)$, where $\bm{D}$ is the working data $\{\hat{\bm{y}} , \hat{\bm{y}}^{\prime} \otimes \bm{S}\}$. Ignoring the unbiasedness constraint, we define $\lambda^{1}$ as the smallest $\lambda$ value such that all predictors in the group lasso problem have zero coefficients, i.e., the solution at $\lambda^{1}$ is $\hat{\bm{\beta}}^{1}=\bm{0}$. (Note that there is no intercept in our problem.) Under the Karush-Kuhn-Tucker conditions, we have
$$
\lambda^{1}
 = \max _{j=1, \ldots, n}\left\|\left[\nabla L\left(\hat{\bm{\beta}}^{1} \mid \bm{D}\right)\right]^{(j)}\right\|_2 / w_j
 = \max _{j=1, \ldots, n}\left\|-\left(\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)_{\cdot j^{*}}\right)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\right\|_2 / w_j.
$$

**Computation details.** Due to the incorporation of the unbiasedness constraint, we can not directly use some open-source packages designed for group lasso. Consequently, we employ Gurobi to solve the SOCP problem, configuring it by setting OptimalityTol = $0.0001$.

### Series selection method without unbiasedness constraint {#sec-unconstrained}

In this section, we relax the unbiasedness constraint, $\bm{GS} = \bm{I}$, and introduce a reconciliation method with selection that relies on in-sample observations and fitted values. Let $\bm{Y} \in \mathbb{R}^{T \times n}$ denote a matrix comprising observations from all time series on the training set in the structure, and $\hat{\bm{Y}} \in \mathbb{R}^{T \times n}$ denote a matrix of in-sample one-step-ahead forecasts (i.e., fitted values) for all time series. The proposed *empirical group lasso* method considers the optimization problem
$$
\min _{\bm{G}} \quad \frac{1}{2 T} \left\|\bm{Y}-\hat{\bm{Y}} \bm{G}^{\prime} \bm{S}^{\prime}\right\|_F^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$
where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make a more flexible model. We rewrite the problem as
$$
\min _{\operatorname{vec}(\bm{G})} \quad \frac{1}{2 T} \left\|\operatorname{vec}(\bm{Y})-(\bm{S} \otimes \hat{\bm{Y}}) \operatorname{vec}\left(\bm{G}^{\prime}\right)\right\|_2^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$
which becomes a standard group lasso problem, with $\operatorname{vec}(\bm{Y})$ serving as the dependent variable and $\bm{S} \otimes \hat{\bm{Y}}$ as the covariate matrix. We denote this as **Elasso** in the results that follow.

Upon relaxing the unbiasedness constraint, the number of non-zero column entries in the solution for $\bm{G}$ may be less than the number of time series at the bottom level. This differs from the series selection methods with an unbiasedness constraint that we introduced in @sec-constrained. In an extreme scenario, it can happen that the solution takes the form of a top-down $\bm{G}_{TD}=[\bm{p} \mid \bm{O}_{n_b \times (n-1)}]$, where only the column corresponding to the top level (most aggregated level) retains non-zero values, and $\bm{p} = (p_1, p_2, \ldots, p_{n_b})$ is a proportionality vector obtained based on in-sample reconciled forecast errors.

We also explored the empirical version of group best-subset selection with ridge regularization and intuitive method with $L_0$ regularization in which we do not impose the unbiasedness constraint. It is worth mentioning that @Hazimeh2023-ie presented a new algorithmic framework for formulating the group $L_0$ problem with ridge regularization and provided the **L0Group** Python package for implementation. However, our experiments showed that this algorithm can not terminate within five hours for typical instances with $p \sim 10^4$. Therefore, in this paper, we only present the empirical group lasso method for series selection without unbiasedness constraint.

**Penalty weights and parameter tuning.** Similarly to the setup in @sec-lasso, we assign different penalty weights to each group by setting $w_j = 1/\left\|\bm{G}_{\cdot j}^{\text{OLS}}\right\|_2$, where $\bm{G}^{\text{OLS}}$ is the solution obtained by the OLS estimator of $\bm{W}$. Given a fixed tuning parameter value, we solve the target optimization problem by considering the initial $T-T_v$ observations, where $T_v = \max\{h, s\}$ for seasonal time series and $T_v = \lfloor \frac{1}{10}T \rfloor$ for non-seasonal time series. Then we select the tuning parameter, $\lambda$, by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $T_v$ observations closest to the forecast origin. Specifically, we form the set of candidate values for $\lambda$ as $\{\lambda^{1},...,\lambda^{k}, 0\}$, where $\lambda^{1} = \max _{j=1, \ldots, n}\left\|-\frac{1}{N}\left(\left(\bm{S} \otimes \hat{\bm{Y}}\right)_{\cdot j*}\right)^{\prime} \operatorname{vec}(\bm{Y})\right\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}\left(\lambda^{k} / \lambda^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. Following the same derivation as in the proof of **Proposition 3**, $\lambda^{1}$ is the smallest $\lambda$ value such that all predictors in the empirical group lasso problem have zero coefficients, i.e., $\bm{G} = \bm{O}$. Note that we need to resolve the optimization problem based the whole training set by using the optimal tuning parameter to obtain the final solution.

**Computation details.** While there are open-source packages available for solving a group lasso problem, they are still relatively slow when applied to large instance for practical usage. For example, given a specific value for the parameter, $\lambda$, our experiments observed that, using the **gglasso** R package, we can not obtain a solution within five hours for typical instances with $p \sim 10^4$. Instead, we use Gurobi to solve the problem based on the SOCP formulation for the empirical group lasso which aligns with @eq-lasso_socp but omits the unbiasedness constraint.

## Monte Carlo simulations {#sec-simulations}

To evaluate the performance of various reconciliation methods with time series selection outlined in @sec-methodology, we carry out two simulations with different designs. In both simulations, we consider a hierarchy comprising two levels of aggregation, as shown in @fig-hts. Specifically, the structure has four series at the bottom level, and seven series in total, i.e., $n_b = 4$, and $n = 7$. The bottom-level series are first generated and then summed appropriately to obtain aggregated series at higher levels.

@sec-sim1 considers a setup where the bottom-level series are generated using a structural time series model, but model misspecification exists for some series within the structure. @sec-sim2 explores the impact of correlation between series on the performance of reconciled forecasts.

### Setup 1: Exploring the effect of model misspecification {#sec-sim1}

In this simulation design, we follow a simulation setup similar to @Wickramasuriya2019-fc, assuming that the bottom-level time series are generated using the basic structural time series model
$$
\bm{b}_t=\bm{\mu}_t+\bm{\gamma}_t+\bm{\eta}_t,
$$
where $\bm{\mu}_t$, $\bm{\gamma}_t$, and $\bm{\eta}_t$ are trend, seasonality, and error components, respectively. The trend and seasonality components are defined by
\begin{align*}
\bm{\mu}_t & =\bm{\mu}_{t-1}+\bm{v}_t+\bm{\varrho}_t, &&& \bm{\varrho}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_{\varrho}^2 \bm{I}_4\right), \\
\bm{v}_t & =\bm{v}_{t-1}+\bm{\zeta}_t, &&& \bm{\zeta}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\zeta^2 \bm{I}_4\right), \\
\bm{\gamma}_t & =-\sum_{i=1}^{s-1} \bm{\gamma}_{t-i}+\bm{\omega}_t, &&& \bm{\omega}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\omega^2 \bm{I}_4\right),
\end{align*}
where $\bm{\varrho}_t$, $\bm{\zeta}_t$, and $\bm{\omega}_t$ are error terms independent of each other and over time. The error term $\bm{\eta}_t$ is generated independently from an $\text{ARIMA}(p,0,q)$ process, where $p$ and $q$ take values of $0$ or $1$ with equal probability. The coefficients for the AR and MA components in the ARIMA process are sampled randomly from a uniform distribution within the range $[0.5, 0.7]$, and the contemporaneous error covariance matrix is given by
$$
\left[\begin{array}{llll}
5 & 3 & 2 & 1 \\
3 & 4 & 2 & 1 \\
2 & 2 & 5 & 3 \\
1 & 1 & 3 & 4
\end{array}\right],
$$
which enables correlations among time series in a hierarchical structure.

We set $s = 4$ for quarterly data with error variances $\sigma_{\varrho}^2=2$, $\sigma_\zeta^2=0.007$, and $\sigma_\omega^2=7$, respectively. The initial values for $\bm{\mu}_0$, $\bm{v}_0$, $\bm{\gamma}_0$, $\bm{\gamma}_1$, and $\bm{\gamma}_2$ are generated independently from a multivariate normal distribution with zero mean and identity covariance matrix. For each series at the bottom level, we generate a total of $T+h = 180$ observations, with the last $h = 16$ observations serving as the test set. Recall that the bottom-level series are aggregated to obtain the data for the aggregated levels. This process is repeated $500$ times.

We use ETS models to generate base forecasts for all time series in the hierarchy, using the default settings as implemented in the **forecast** R package [@Hyndman2023-fc]. To introduce model misspecification into our experiment, we deliberately undermine the quality of in-sample and out-of-sample forecasts (i.e., fitted values and base forecasts) for some specific time series. Specifically, we investigate three scenarios characterized by artificial model misspecifications, where a 1.5 multiplier is applied to in-sample and out-of-sample forecasts for a single series in each scenario, i.e., series AA at the bottom level, series A at the middle level, and series Total at the top level, resulting in Scenario I, Scenario II, and Scenario III, respectively.

The results for Scenarios I, II, and III are presented in @tbl-s1-rmse, @tbl-s2-rmse, and @tbl-s3-rmse, respectively. Each table reports the average root mean squared error (RMSE) for each level as well as the whole structure (denoted as *Average*). The *Base* row shows the average RMSE of base forecasts, while entries below this row reporting the percentage decrease (negative) or increase (positive) in the average RMSE of reconciled forecasts compared to base forecasts. For each scenario considered, the largest improvements occur at the respective hierarchical level where model misspecification is introduced, while slightly deteriorating the performance of other levels.

```{r}
#| label: tbl-s1-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario I, Setup 1.

rmse_s1 <- readRDS("results/sim_rmse_s1.rds")
latex_table(rmse_s1)
```

```{r}
#| label: tbl-s1-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario I, Setup 1.

selection_sim <- readRDS("results/sim_selection.rds")
latex_sim_nos_table(selection_sim$out_s1$z, selection_sim$out_s1$n, "s1")
```

Focusing on the results of the benchmark reconciliation methods, we find that the BU approach performs the best in both Scenario II and III but ranks as the worst overall in Scenario I. This is not surprising, as bottom-level base forecasts are deteriorated in Scenario I, while higher-level base forecasts are deteriorated in Scenario II and III. Moreover, the WLSv, MinT, and MinTs approaches perform especially well in Setup 1, benefiting from their ability to consider the in-sample covariance of base forecast errors, allowing for larger range of adjustments in reconciliation for base forecasts with higher estimated error variance. EMinT also provides accurate reconciled forecasts in our setup, where the in-sample forecasts for specific series are intentionally undermined, a situation that can be detected by the in-sample information based EMinT method. However, OLS and WLSs perform much worse than other benchmark methods in this simulation design.

In all three scenarios, our proposed methods consistently produce either improved or comparable reconciled forecasts compared to their respective benchmark methods. The improvements are particularly pronounced when using OLS and WLSs estimators of $\bm{W}$ in the benchmark methods, which do not take into account the in-sample covariance of base forecast errors. One advantage of using the forecast reconciliation methods with selection proposed in this paper is that they can reduce the difference introduced by using different estimates of $\bm{W}$, thereby mitigating the risk of estimator selection. In some cases, such as Scenarios II and III, we can align the forecast accuracy achieved using different estimators, and make them close to the best results we can obtain. When we drop the unbiasedness assumption, Elasso delivers results on par with EMinT overall, while achieving improvements at the top level, which is typically the aspect of greatest concern to practitioners.

In addition, we report the proportion of time series being selected from the implementation of our proposed methods in 500 simulation instances, as shown in @tbl-s1-selection, @tbl-s2-selection, and @tbl-s3-selection for each respective scenario. Clearly, our proposed methods select fewer time series from the hierarchy for forecast reconciliation, and generally improve forecast accuracy over the benchmark methods. Furthermore, we observe that the Subset methods tend to return fewer time series compared to the Intuitive and Lasso methods, which aligns with our expectations that the Intuitive and Lasso methods tend to produce dense estimates. Most importantly, depending on the scenario considered, the time series with model misspecification has been selected less often than others. For example, considering Scenario I, series AA is expected to be removed, while AB is expected to be retained. This allows us to obtain series AA via operations such as A$-$AB, Total$-$B$-$AB, or Total$-$AB$-$BA$-$BB. The results in @tbl-s1-selection align with our expectations, and show that series AA is dropped often, whereas AB is selected all the time.

### Setup 2: Exploring the effect of correlation {#sec-sim2}

We now consider to simulate a hierarchical structure with correlated series. A similar simulation to @Wickramasuriya2021-am is implemented in this section. Using the same hierarchical structure as shown in @fig-hts, we assume the data generating process for the time series at the bottom level follows a stationary first-order vector autoregressive model, i.e., $\text{VAR}(1)$, given by
$$
\bm{b}_t= \bm{c} + \left[\begin{array}{cc}
\bm{A}_1 & \bm{0} \\
\bm{0} & \bm{A}_2
\end{array}\right] \bm{b}_{t-1} + \bm{\varepsilon}_t,
$$
where $\bm{c}$ is a constant vector with all entries set to $1$, $\bm{A}_1$ and $\bm{A}_2$ are $2 \times 2$ matrices with eigenvalues $z_{1,2}=0.6[\cos (\pi / 3) \pm i \sin (\pi / 3)]$ and $z_{3,4}=0.9[\cos (\pi / 6) \pm i \sin (\pi / 6)]$, respectively, and $\bm{\varepsilon}_t \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$, where
$$
\bm{\Sigma}=\left[\begin{array}{cc}
\bm{\Sigma}_1 & 0 \\0 & \bm{\Sigma}_2
\end{array}\right], \text { and } \bm{\Sigma}_1=\bm{\Sigma}_2=\left[\begin{array}{cc}2 & \sqrt{6} \rho \\\sqrt{6} \rho & 3\end{array}\right],
$$
and $\rho \in \{0, \pm 0.2, \pm 0.4, \pm 0.6, \pm 0.8\}$ controls the error correlation in the simulated hierarchy.

For each time series at the bottom level, we generate a total of $101$ observations, with the last one observation serving as the test set, i.e., $T=100$ and $h=1$. Once again, the data at the higher levels are obtained by aggreating the bottom-level series. The process is repeated $500$ times for each candidate correlation, $\rho$.

For each series in the hierarchy, base forecasts are generated from ARMA models based on a training data comprising $100$ observations. Specifically, we identify the best ARMA model with the minimum AICc (corrected Akaike information criterion) value for each series by using the automated algorithm implemented in the **forecast** R package. Additionally, when fitting ARMA models for time series Total, A, and BA, we introduce a slight bias by omitting the constant term, which is a common case when using some models to get base forecasts in practice. @fig-corr-data presents an illustrative example of a hierarchical time series simulated. The left panels depict time plots for each series at different levels of the structure, while right panels show the residuals obtained from forecasting each series using the fitted ARMA model. Notably, despite our omission of the constant term when fitting ARMA models to series Total, A, and BA, the residuals derived from the identified optimal models still exhibit fluctuations around zero and do not display significant deviations in comparison to the residuals from other series. This is because the influence of the constant term is minimal, i.e., it is much smaller compared to the data variability. Thus, it may be challenging to identify the "poor" base forecasts and exclude them from reconciliation in this setup.

```{r}
#| label: fig-corr-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 9
#| fig-height: 7
#| fig-pos: "!htb"
#| fig-cap: An example hierarchical time series and its in-sample residuals in Setup 2.

data <- readRDS("results/corr_data_neg.rds")
resid <- readRDS("results/corr_resid_neg.rds")

theme_plot <- theme_bw() +
  theme(legend.position="bottom",
        legend.margin=margin(0,0,0,0),
        legend.box.spacing = unit(0, "pt"),
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())
p11 <- data |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: observations") +
  theme_plot

p12 <- data |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: observations") +
  theme_plot

p13 <- data |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: observations") +
  theme_plot

p21 <- resid |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: residuals") +
  theme_plot

p22 <- resid |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: residuals") +
  theme_plot

p23 <- resid |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: residuals") +
  theme_plot

(p11 + p21) / (p12 + p22) / (p13 + p23)
```

@tbl-corr-rmse summarizes the average RMSE of the base forecasts across various error correlations and the percentage relative improvements in RMSE achieved by reconciliation methods relative to the base forecasts. The results show that, for OLS, WLSs, WLSv estimators, our proposed methods consistently dominate their respective benchmark methods at all levels when the error correlation is $-0.8$. In general, as the error correlation ranges from $-0.8$ to $0.8$, the overall improvements in our methods over the benchmark methods show comparable results. Similar pattern is observed in the overall improvements of all benchmark reconciliation methods compared to the base forecasts. We should highlight the challenge of identifying the "poor" base forecasts in this simulation design, given that the omission of the constant term has minimal impact relative to the data variability. In addition, we observe that the MinT and MinTs methods perform especially well and our methods provide results same with benchmark methods. This is attributed to the use of in-sample covariance by MinT and MinTs, which allows for large adjustments in reconciliation for base forecasts with high estimated error variance. Elasso forecasts are slightly worse than EMinT, possibly due to the difficulty of identifying underperforming base forecasts in this simulation setup.

We have also considered alternative error correlation values, $\rho = -0.6, -0.2, 0.2, 0.4$, for this simulation setting, but to save space, we do not present all results. The omitted results follow a similar pattern and are available upon request.

```{r}
#| label: tbl-corr-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results across various error correlations for simulation in Setup 2.

rmse_corr <- readRDS("results/corr_rmse.rds")
latex_corr_table(rmse_corr)
```

We present the proportion of time series being selected by applying our proposed methods in $500$ simulation instances for error correlation coefficients of $-0.8$ and $0.8$ in @tbl-corr-selection-neg and @tbl-corr-selection-pos, respectively. Once again, we note that it is difficult to exclude poor-performing base forecasts in this simulation design as the constant term omitted is very small compared to the data variability. As observed in @tbl-corr-selection-neg, for OLS, WLSs, and WLSv estimators, the Subset and Intuitive methods are still able to exclude the series Total, A, and BA in some instances, in which small biases are introduced in model fitting, while essentially retaining the rest series in the hierarchy. The Subset methods perform superior to the Intuitive method in selection. The Lasso methods typically select all bottom-level series since they tend to yield dense estimates as discussed in @sec-lasso. Elasso also select all bottom-level series. When dealing with a high positive error correlation, @tbl-corr-selection-pos shows that our methods still have the potential to do some selection but it becomes somewhat challenging to identify and exclude the series that should be omitted in reconciliation. Hence, our methods are preferred, particularly when the error correlation within the hierarchical structure is negative.

```{r}
#| label: tbl-corr-selection-neg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Setup 2, with the error correlation being -0.8.

selection_corr_neg <- readRDS("results/corr_selection_neg.rds")
latex_sim_nos_table(selection_corr_neg$out_s0$z,
                    selection_corr_neg$out_s0$n,
                    "corr_neg")
```

## Applications {#sec-applications}

In this section we perform two empirical applications to investigate the performance of our proposed methods and compare them with state-of-the-art reconciliation approaches. @sec-labour focuses on a grouped hierarchy built using the Australian labour force survey data released by the Australian Bureau of Statistics, while @sec-tourism considers Australian domestic tourism flows with a natural geographical hierarchy.

### Forecasting Australian labour force {#sec-labour}

This section evaluates the performance of the proposed methods using a grouped hierarchy built using the Australian labour force dataset. The dataset from the Labour Force Survey are released by the Australian Bureau of Statistics, consisting of monthly data on the number of unemployed persons in Australia for the period from January 2010 to July 2023[^1]. There are a few missing values in the dataset. To deal with the missing observations, we use a random walk to give linear interpolation between points. Analysis of unemployment data in a country by labor market region and duration of job search can provide valuable insights into regional disparities, and the structural nuances underlying unemployment. Forecast reconciliation is crucial in such a case to ensure aligned decision making.

[^1]: The Labour Force Survey data is publicly available at <https://www.abs.gov.au/statistics/labour/employment-and-unemployment/labour-force-australia-detailed/aug-2023>.

We construct a grouped hierarchy by disaggregating the number of unemployed persons over two independent attributes, duration of job search (referred to as *Duration*), and State and Territory (referred to as *STT* ). The two attributes are crossed, but none are nested within the others. At the bottom level, the data are disaggregated by both attributes. We refer to the bottom level as the *Duration* $\times$ *STT* level. Specifically, there are six different groups of job search duration, under 1 month, 1--3 months, 3--6 months, 6--12 months, 1--2 years, and 2 years and over. Additionally, the number of unemployed persons in Australia can be disaggregated by eight states and territories, i.e., NSW (New South Wales), VIC (Victoria), QLD (Queensland), SA (South Australia), WA (Western Australia), TAS (Tasmania), NT (Northern Territory), and ACT (Australian Capital Territory). So the final grouped hierarchy consists of the top series, six series at the Duration level, eight series at the STT level, and $48$ series at the Duration $\times$ STT level, giving $63$ time series in total, each of length $163$ observations.

```{r}
#| label: fig-labour-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 10
#| fig-height: 6.5
#| fig-pos: "!htb"
#| fig-cap: Australia unemployed persons, disaggregated by state and territory, and by duration of job search.

labour_ts <- readRDS("results/labour_gts.rds")

p1 <- labour_ts |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Total unemployed persons") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p2 <- labour_ts |>
  filter(Series %in% c("NSW", "VIC", "QLD", "SA", "WA", "TAS", "NT", "ACT")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "State and territory") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p3 <- labour_ts |>
  filter(Series %in% c("Under 1 month", "1-3 months", "3-6 months", "6-12 months",
                       "1-2 years", "2 years and over")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Duration of job search") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p1 / (p2 + p3)
```

The top panel in @fig-labour-data shows the total number of unemployed persons in Australia from January 2010 to July 2023, representing the top-level series in the grouped hierarchical structure. The monthly series shows strong seasonality within each year, marked by prominent peaks occurring every January, possibly attributable to people waiting to start new jobs. In addition, lower peaks occur in July, impacted by the timing of school holidays. Amidst the backdrop of COVID-19's non-essential service shutdowns and trading restrictions, March and April of 2020 saw a notable surge in unemployment. However, as coronavirus cases dwindled significantly and restrictions eased in the aftermath, employment made a remarkable recovery, leading to a subsequent decline in unemployment. The bottom-left panel displays the breakdown of unemployed individuals by state and territory, while the bottom-right panel presents the breakdown by the duration of job search. The plots display diverse and rich dynamics both within and between different levels of the hierarchy. For example, there was noticeable growth observed during 2020 for some states such as NSW, VIC, and QLD, whereas other states did not experience such significant growth. Additionally, there is a resemblance in the seasonal patterns between NSW and QLD, while the seasonal pattern in VIC appears relatively different. When comparing the series at the STT level and Duration level, we notice that the seasonal patterns in the Duration-level series is more consistent and potentially easier to forecast.

We assess the forecast accuracy of base forecasts and various reconciliation methods through a rolling forecast origin approach. Our aim is to generate $1$- to $12$-steps-ahead forecasts for each of the $63$ series while ensuring coherence. Given the limited data compared to the forecast horizon, we initiate the process with a training set of $139$ observations for each series. The training set is used to select the optimal ETS model with the automatic algorithm implemented in the **forecast** package for R. Using these fitted ETS models, we generate base forecasts, and then perform diverse forecast reconciliation methods. Following this, we roll the forecast origin forward by one month and repeat the process until July 2022. We note that it may be challenging to identify the series with "poor" forecasts due to structural changes in the data caused by the COVID-19 pandemic, which affect the accuracy of forecasts across all time series.

The average results are presented in @tbl-labour-rmse-avg. The MinT method and the respective proposed methods are not considered due to their poor performance, attributed to the poor sample covariance estimator when the sample size is slightly larger than the number of series in the structure. The Subset methods using different estimators of $\bm{G}$ generally improve forecast accuracy over their respective benchmark methods overall, particularly when focusing on aggregation levels, which are typically of paramount concern to practitioners. The only one exception is the WLSs-subset method, which returns reduced accuracy for longer horizons overall. However, it still demonstrates improvements in top-level forecasts, and other levels remains within a reasonable range. Moreover, the Intuitive and Lasso methods almost always yield results identical to the corresponding benchmark methods. This is because they tend to provide dense estimates, and ETS models typically do not result in extremely poor forecasts. The only exception is OLS-intuitive, which shows improved forecast accuracy at the top level but deterioration at other levels. When we drop the unbiasedness assumption, EMinT is the worst performing method across all levels because it relies on the assumption that the series in the hierarchy are jointly weakly stationary, which is evidently not the case in the application. Elasso significantly improves the quality of forecasts over EMinT, with the most accurate coherent forecasts observed at the top level and STT level. Overall, Elasso performs well for longer forecast horizons, but it is less effective for one-step-ahead forecasts.

```{r}
#| label: tbl-labour-rmse-avg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Average out-of-sample forecast results for Australian labour force data.

rmse_labour_avg <- readRDS("results/labour_rmse.rds")
latex_table(rmse_labour_avg)
```

We also provide the results based on the final test set spanning from August 2022 to July 2023 in @tbl-labour-rmse. We consider this period because it is the latest available data, enabling us to use more data for model training to get more knowledge about the post-COVID pattern. The results indicate that all Subset methods using different estimators of $\bm{W}$, i.e., OLS-subset, WLSs-subset, WLSv-subset, and MinTs-subset, produce improved or comparable reconciled forecasts compared to their respective benchmark methods. The improvements in forecast accuracy become more noticeable for longer forecast horizons. Similar to the average results in @tbl-labour-rmse-avg, the Intuitive and Lasso methods yield results identical to the benchmark methods due to their tendency to offer dense estimates. Surprisingly, when relaxing the unbiasedness constraint, the Elasso method ranks the best and demonstrates significant improvement compared to the EMinT method, and outperforms other methods across almost all levels except for the top level.

```{r}
#| label: tbl-labour-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results on a single test set (from August 2022 to July 2023) for Australian labour force data.

rmse_labour <- readRDS("results/labour_1_rmse.rds")
latex_table(rmse_labour)
```

Furthermore, based on the final test set, we present the number of series selected at each level and the optimal tuning parameter values obtained using different proposed methods, as shown in @tbl-labour-info. Here, we only showcase results from the Subset and Elasso methods, as they prove to be valuable in the labour force application in terms of the RMSE results. Note that the variation in the scale of the optimal parameters for different methods comes from the difference in the scales of objective. @tbl-labour-info shows that all Subset methods exclude some series when performing forecast reconciliation. Remarkably, the Elasso method consistently outperforms the others overall, even though it uses only $11$ series for forecast reconciliation. Additionally, it is worth noting that most of the series at the STT level are removed, while the majority of series at the Duration level are retained. This aligns with our data description, highlighting that the seasonal patterns in the Duration level series is more consistent and potentially easier to forecast compared to those at the STT level.

```{r}
#| label: tbl-labour-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Number of time series selected using different proposed methods and the optimal parameter values identified in the labour application, considering a single test set (from August 2022 to July 2023). The None row shows the original number of series in the structure.

labour_info <- readRDS("results/labour_info.rds")

options(knitr.kable.NA = '-')
labour_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("hold_position", "repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```

### Forecasting Australian domestic tourism {#sec-tourism}

In this section we consider Australian domestic tourism flows, measured as the number of overnight trips Australians spend away from home, and create a hierarchical structure using geographic divisions. The data are sourced from the National Visitor Survey and collected through computer-assisted telephone interviews involving approximately $120,000$ Australian residents aged $15$ years and older. The hierarchical structure starts with the national total tourism flow as the top-level aggregation, then disaggregates it into seven states and territories (referred to as *State* level hereafter), further divides them into $27$ zones, and finally, into $76$ regions, thus forming a natural geographical hierarchy.

Therefore, the hierarchy under consideration involves $76$ monthly time series at the bottom level and $111$ monthly series in total, i.e., $n_b=76$ and $n=111$. Each series in the hierarchy spans the period from January 1998 to December 2017, with a total of $240$ observations.

@fig-tourism-data shows the aggregate tourism flows for Australia as well as individual states, revealing pronounced seasonal patterns across the national total and states, albeit with varying seasonal patterns among the series. Notably, there was a significant growth starting from around 2010 for the national total flow and some states such as NSW, VIC, QLD, and WA. While flows are relatively flat for SA, TAS, and NT. Moreover, the time plot displays that there was a large decrease in tourism flows for WA occurred in 2016.

```{r}
#| label: fig-tourism-data
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 9
#| fig-height: 7
#| fig-pos: "!htb"
#| fig-cap: Domestic tourism flows from January 1998 to December 2017 for the whole of Australia as well as the states.
tourism_ts <- readRDS("results/tourism_hts.rds")
tourism_ts |>
  autoplot(Value) +
  facet_wrap(vars(Series), scales = "free_y", ncol = 2) +
  xlab("Time") +
  ylab("Australian domestic tourism flows ('000)") +
  theme_bw() +
  theme(legend.position = "none",
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold", size = 14),
        axis.title.x = element_text(face = "bold", size = 12),
        axis.text = element_text(face = "bold", size = 10),
        axis.ticks.x.top = element_blank())
```

Our objective is to forecast tourism flows for each series in the geographical hierarchy while ensuring coherence across all levels. We adopt the rolling forecast origin approach to evaluate the forecast accuracy of different methods. We start with a training set of $216$ months for each series to generate base forecasts by fitting the optimal ETS model. We then roll the forecast origin forward by one month and repeat the process until December 2016. The base forecasts are reconciled using our proposed methods and some state-of-the-art reconciliation methods.

@tbl-tourism-rmse-avg reports the average RMSE values for base forecasts generated by ETS models, along with the percentage relative improvements in average RMSE obtained by a particular reconciliation method relative to the base forecasts. Similar to @sec-labour, the MinT method and the respective proposed methods are not considered due to their poor performance. The results show that the OLS method outperforms other benchmark methods like WLSs, WLSv and MinTs, despite the fact that WLSv and MinTs account for the in-sample covariance of base forecast errors. This highlights the effectiveness of the OLS method despite its simplicity.

Overall, the Subset methods outperform their respective benchmark methods, especially for aggregation levels and for longer forecast horizons. The only exception is the OLS-subset method, which slightly reduces overall accuracy while still improving top-level forecasts. Moreover, the Intuitive and Lasso methods produce results almost identical to the corresponding benchmark methods, which is not surprising as ETS models typically do not yield extremely poor forecasts, making them challenging to be selected out using methods that tend to return dense estimates. When we relax the unbiasedness constraint, EMinT consistently performs the worst across all levels due to the evident lack of joint weak stationarity among the series in the hierarchy. The Elasso method presents significant improvement compared to the EMinT method, and it also outperforms other methods across almost all levels except for the bottom level.

```{r}
#| label: tbl-tourism-rmse-avg
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Average out-of-sample forecast results for Australian domestic tourism data.

rmse_tourism_avg <- readRDS("results/tourism_rmse.rds")
latex_table(rmse_tourism_avg)
```

We also present the results based on the last one training set spanning from January 2017 to December 2017 in @tbl-tourism-rmse. The reconciliation errors across each of the $111$ series and across the four levels in the hierarchy are displayed in @fig-tourism-rmse. The results shows a similar performance to the average results described above, indicating relatively high-quality forecasts from the Subset and Elasso methods.

```{r}
#| label: tbl-tourism-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results on a single test set (from January 2017 to December 2017) for Australian domestic tourism data.

rmse_tourism <- readRDS("results/tourism_1_rmse.rds")
latex_table(rmse_tourism)
```

```{r}
#| label: fig-tourism-rmse
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 10
#| fig-height: 4
#| fig-pos: "!htb"
#| fig-cap: Average out-of-sample forecasting performance, measured in terms of RMSE (from 1- to 12-step-ahead), for each series across different reconciliation methods. Time series are arranged along the horizontal axis.

tourism_heatmap <- readRDS("results/tourism_heatmap.rds")
ggplot(tourism_heatmap, aes(x = Series, y = Method, fill = RMSE)) +
  geom_tile() +
  geom_vline(xintercept = c(1.5, 8.5, 35.5), linetype = "dashed", linewidth = 0.5) +
  scale_fill_gradientn(colors = c("#f7d9a6",
                                  rev(hcl.colors(100, "Purples"))[c(seq(1, 50, 10), seq(51, 100, 1))])) +
  labs(x = "Time series", y = "") +
  scale_x_continuous(expand = c(0, 0),
                     breaks =  seq(20, 100, 20),
                     sec.axis = dup_axis(name = "",
                                         breaks = c(4.5, 11.5, 39),
                                         labels = c("States", "Zones", "Regions"))) +
  scale_y_discrete(expand = c(0, 0),
                   limits = rev(c("Base", "BU", "OLS", "OLS-subset",
                                  "WLSs", "WLSs-subset", "WLSv", "WLSv-subset",
                                  "MinTs", "MinTs-subset", "EMinT", "Elasso"))) +
  theme(
    plot.background = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, linewidth=1),
    axis.title.x = element_text(face = "bold", size = 12),
    legend.title = element_text(face = "bold", size = 10),
    legend.text = element_text(face = "bold", size = 10),
    legend.position = "bottom",
    axis.text = element_text(face = "bold", size = 10),
    axis.ticks.x.top = element_blank()
  ) +
  guides(fill = guide_colourbar(barwidth = 10,
                                barheight = 1.5))
```

Additionally, @tbl-tourism-info presents a summary of the number of series selected using different proposed methods for each level as well as the optimal tuning parameter values identified. Here we only give the results of the Subset and Elasso methods since they are useful in the tourism application. Note that the variation in the scale of the optimal parameters for different methods comes from the difference in the scales of objective. We observe that the OLS-subset and WLSs-subset methods exclude some series at the State and Zone levels for forecast reconciliation. In contrast, the WLSv and MinTs methods retain all series, which is reasonable because they take into account the in-sample covariance, making themselves allow for larger adjustments made to series with large in-sample forecast error variances in forecast reconciliation. Nonetheless, the WLSv and MinTs methods can still enhance the quality of reconciled forecasts due to the inclusion of shrinkage through additional ridge regularization. It is surprising that Elasso performs exceptionally well despite using only $13$ series for reconciliation.

```{r}
#| label: tbl-tourism-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Number of time series selected using different proposed methods and the optimal parameter values identified in the tourism application, considering a single test set (from  January 2017 to December 2017). The None row shows the original number of series in the structure.

tourism_info <- readRDS("results/tourism_info.rds")

options(knitr.kable.NA = '-')
tourism_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("hold_position", "repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```

## Conclusion {#sec-conclusion}

In the existing literature on hierarchical or grouped time series when we perform linear forecast reconciliation, we map all base forecasts into bottom-level disaggregated forecasts, which are then summed up by a summing matrix to yield coherent forecasts for the entire structure. Hence, the mapping step in forecast reconciliation can be conceptually regarded as a forecast combination. In practical applications, it is common that the base forecasts for some time series in the structure may perform poorly, especially in the context of large structures. This may reduce the overall effectiveness of forecast reconciliation methods. In this paper, we aimed to address this issue by introducing a selection mechanism in forecast reconciliation, i.e., incorparating time series selection when reconciling forecasts for hierarchical or grouped time series, while ensuring the generation of coherent forecasts for all series.

Under the unbiasedness constraint, we developed three reconciliation methods with selection mechanisms to keep forecasts for an automatically selected set of series unused in forming reconciled forecasts. These methods include group best-subset selection with ridge regularization (Subset), intuitive method with $L_0$ regularization (Intuitive), and group lasso method (Lasso). These methods formulated the problem based on out-of-sample base forecasts using different penalty functions designed to penalize the columns of the weighting matrix, $\bm{G}$, towards zero. Additionally, we relaxed the unbiasedness constraint and proposed the empirical group lasso method (Elasso) which selects series based on in-sample observations and fitted values.

Simulation experiments and two empirical applications demonstrated the superiority of the proposed methods over the reconciliation methods that do not involve series selection. Our methods were preferred, particularly when the error correlation within the hierarchical structure is negative. Furthermore, when model misspecification was introduced for some series in the hierarchy, our proposed methods guaranteed coherent forecasts that outperformed or, at the very least, matched their respective benchmark methods in the minimum trace reconciliation framework. In both empirical applications, where no apparent model misspecification was present, the Subset and Elasso methods were always preferred, particularly for aggregation levels and longer forecast horizons, while the Intuitive and Lasso methods yield results identical to the corresponding benchmark methods, as they tend to provide dense estimates.

A remarkable feature of the proposed methods is their ability to reduce the disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating the challenges associated with estimator selection, which is a prominent issue within the field of forecast reconciliation research.

In our study, we use Gurobi, one of the widely used commercial solvers, to address NP-Hard MIP problems.

As the structure of consideration gets bigger, solving these problems efficiently becomes challenging. Exact computation of these estimators remains a major hurdle. Despite various efforts in developing MIP-based approaches for solving $L_0$-regularized regression problems, extending these methods to incorporate additional constraints remains a challenge. We leave this aspect to be addressed in future research.

## References {.unnumbered}

::: {#refs}
:::

```{=tex}
\newpage
\appendix
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
```
## Appendix {.unnumbered}

The section provides additional results for the simulation data in @sec-simulations.

```{r}
#| label: tbl-s2-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario II, Setup 1.

rmse_s2 <- readRDS("results/sim_rmse_s2.rds")
latex_table(rmse_s2)
```

```{r}
#| label: tbl-s3-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Out-of-sample forecast results for the simulated data in Scenario III, Setup 1.

rmse_s3 <- readRDS("results/sim_rmse_s3.rds")
latex_table(rmse_s3)
```

```{r}
#| label: tbl-s2-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario II, Setup 1.

latex_sim_nos_table(selection_sim$out_s2$z, selection_sim$out_s2$n, "s2")
```

```{r}
#| label: tbl-s3-selection
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Scenario III, Setup 1.

latex_sim_nos_table(selection_sim$out_s3$z, selection_sim$out_s3$n, "s3")
```

```{r}
#| label: tbl-corr-selection-pos
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Proportion of time series being selected after using the proposed reconciliation methods with selection in Setup 2, with the error correlation being 0.8.

selection_corr_pos <- readRDS("results/corr_selection_pos.rds")
latex_sim_nos_table(selection_corr_pos$out_s0$z,
                    selection_corr_pos$out_s0$n,
                    "corr_pos")
```
