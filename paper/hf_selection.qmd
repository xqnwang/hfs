---
title: "Optimal forecast reconciliation with time series selection"
author:
- name: Xiaoqian Wang
  acknowledgements: |
      Corresponding author.
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
- name: Rob J Hyndman
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
- name: Shanika L Wickramasuriya
  affiliations:
    - name: Monash University
      department: Department of Econometrics \& Business Statistics
abstract: Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can enhance overall accuracy in hierarchical or grouped structures, the most substantial improvements occur in series with initially poor-performing base forecasts. Nevertheless, certain series may experience deteriorations in reconciled forecasts. In practical settings, series in a structure often exhibit poor base forecasts due to model misspecification or low forecastability. To prevent their negative impact, we propose two categories of forecast reconciliation methods that incorporate time series selection based on out-of-sample and in-sample information, respectively. These methods keep "poor" base forecasts unused in forming reconciled forecasts, while adjusting weights allocated to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods ameliorate disparities stemming from varied estimates of the base forecast error covariance matrix, alleviating challenges associated with estimator selection. Empirical evaluations through two simulation studies and applications using Australian labour force and domestic tourism data demonstrate improved forecast accuracy, particularly evident in higher aggregation levels, longer forecast horizons, and cases involving model misspecification.
keywords: "Forecasting, Hierarchical time series, Linear forecast reconciliation, Variable selection, Integer programming"
bibliography: references.bib
format:
  asa-pdf:
    keep-tex: true
    pdf-engine: pdflatex
    classoption: 11pt
    journal:
      blinded: false
include-in-header: preamble.tex
execute:
  echo: false
  warning: false
  message: false
  cache: false
date: last-modified
---

```{r}
#| label: load
# Load all required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(fabletools)
library(patchwork)
library(knitr)
library(kableExtra)
library(latex2exp)

source(here::here("R/nemenyi.R")) # MCB test
source(here::here("R/analysis.R")) # Other functions used for analysis
options(knitr.kable.NA = '-')

theme_heatmap <- theme(
  axis.text.x = element_text(face = "bold"),
  axis.text.y = element_text(face = "bold"),
  axis.ticks = element_blank(),
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  panel.grid.major = element_blank(),
  plot.title = element_text(hjust = 0.5),
  legend.direction = "horizontal",
  legend.justification = c(1, 0),
  legend.position = "inside",
  legend.position.inside = c(0.5, 0.7))
```

## Introduction {#sec-introduction}

Forecast reconciliation is a post-processing method that ensures forecasts of multivariate time series adhere to known linear constraints [@Hyndman2011-sd]. For example, the sum of regional unemployment forecasts should be equal to the national unemployment forecast.

@Hyndman2011-sd introduced optimal forecast reconciliation, whereby "base" forecasts of all series are generated independently, and then adjusted to satisfy the constraints, leading to a set of coherent reconciled forecasts. Subsequent research has extended and developed the idea in the context of cross-sectional data [@Hyndman2016-cz; @Wickramasuriya2019-fc; @Panagiotelis2021-mf], temporal data [@Athanasopoulos2017-jj], and cross-temporal data [@Di_Fonzo2023-vo]. @Athanasopoulos2024-sm provided a comprehensive introduction to the forecast reconciliation literature.

Reconciliation is known to improve overall forecast accuracy in collections of time series with aggregation constraints. On average, when the base forecasts are unbiased, the mean squared reconciled forecast error from the minimum trace reconciliation method [@Wickramasuriya2019-fc] is lower than that from the base forecasts [@Wickramasuriya2021-am]. Most of the improvements attributed to reconciliation are observed in series with initially poor-performing base forecasts [@Athanasopoulos2017-jj]. In practice, it is not uncommon for some series to have poor base forecasts due to challenges such as model misspecification or low signal-to-noise ratio (SNR). In such cases, it may be advantageous to exclude the worst base forecasts when performing reconciliation. This is the motivation for our proposed methods.

First, we propose forecast reconciliation methods that incorporate time series selection based on out-of-sample information, assuming unbiased base forecasts. We formulate this as an optimization problem, using diverse penalty functions to control the number of nonzero column entries in the weighting matrix for linear forecast reconciliation. We show that the number of selected time series is at least equal to the number of series at the bottom level, and we can reconstruct the entire structure by aggregating/disaggregating the selected series. Second, we relax the unbiasedness assumption and introduce an additional reconciliation method with selection, utilizing in-sample observations and their fitted values. This enables us to use the in-sample reconciliation performance for selection purposes. In this case, it is possible that fewer than the number of series at the bottom level are used for reconciliation. In an extreme scenario, the solution may resemble the traditional top-down approach. Through simulation experiments and two empirical applications, we demonstrate that our proposed methods guarantee coherent forecasts that outperform or match their respective benchmark methods. \todo[inline]{Reconsider the wording.} The improvements are particularly pronounced when focusing on higher aggregation levels, longer forecast horizons, and cases of model misspecification. A remarkable feature of the proposed methods is their ability to diminish disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating challenges associated with estimator selection, which is a prominent concern in the field of forecast reconciliation research. \todo[inline]{What about other concerns in forecast reconciliation research? It would be good to summarize these and address areas where the proposed methodology could support such concerns.}

The remainder of the paper is structured as follows. @sec-preliminaries presents the notations and a review of linear forecast reconciliation methods. @sec-methodology introduces our proposed methods to achieve time series selection in reconciliation, and provides some theoretical insights. @sec-simulations and @sec-applications show the results from simulations and two real-world datasets, respectively. @sec-discussion provides disucussions and thoughts on future research, followed by concluding remarks in @sec-conclusion. The R code for reproducing the results is available at <https://github.com/xqnwang/hfs>.

## Preliminaries {#sec-preliminaries}

### Notation

A *hierarchical time series* is an $n$-dimensional multivariate time series that adheres to known linear constraints. Let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations from all time series in the hierarchy at time $t$, and $\bm{b}_t \in \mathbb{R}^{n_b}$ be a vector comprising observations of only the most disaggregated (bottom-level) time series. The full hierarchy can be written as $$
\bm{y}_t = \bm{S}\bm{b}_t,
$$ for $t=1,2,\ldots,T$, where $T$ is the length of the time series, and $\bm{S}$ is an $n \times n_b$ *summing matrix* that defines the aggregation constraints. We can write the summing matrix as $\bm{S} = \left[\begin{array}{c}\bm{A} \\ \bm{I}_{n_b}\end{array}\right]$, where $\bm{A}$ is an $n_a \times n_b$ *aggregation matrix* with $n = n_a + n_b$, and $\bm{I}_{n_b}$ is an $n_b$-dimensional identity matrix.

![An example of a two-level hierarchical time series.](figs/hts_example.pdf){#fig-hts fig-align="center" width="30%"}

For example, @fig-hts shows a simple hierarchy with $n = 7$, $n_b = 4$, $n_a = 3$, $\bm{y}_t = [y_{\text{Total},t}, y_{\text{A},t}, y_{\text{B},t}, y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, $\bm{b}_t = [y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, and $$
\bm{S} = \left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\bm{I}_4}
\end{array}\right].
$$ The notation is general enough to include aggregation constraints that are non-hierarchical. Please refer to @Hyndman2021-fo for further details.

Hierarchical forecasting methods have been extensively applied across diverse domains. For instance, forecast reconciliation is widely implemented in tourism data [@Athanasopoulos2009-ps], where hierarchical time series arise due to geographic divisions. Total overnight trips for a whole nation can be disaggregated to states, and further subdivided into regions. In the context of a grocery retailer, the total sales of the "food" category can be subdivided into various subcategories and subsequently into distinct items [@Zhang2023-op; @Hollyman2021-un]. In electricity load forecasting, consumption is measured using smart meters which naturally fall within a comprehensive geographic hierarchy [@Taieb2021-tc]. For additional interesting application examples, please refer to @Athanasopoulos2024-sm.

### Linear forecast reconciliation

Let $\hat{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ be a vector of $h$-step-ahead *base forecasts* for all time series in the hierarchy, given observations up to and including time $T$, and stacked in the same order as $\bm{y}_t$. We can use any method to generate these forecasts, but in general they will not be coherent (i.e., they won't satisfy the aggregation constraints). Let $\tilde{\bm{y}}_{T+h \mid T} \in \mathbb{R}^n$ denote a vector of $h$-step-ahead *reconciled forecasts* given by $$
\tilde{\bm{y}}_{T+h \mid T} = \bm{S}\bm{G}_h\hat{\bm{y}}_{T+h \mid T},
$$ {#eq-lr} where $\bm{G}_h$ is an $n_b \times n$ *weighting matrix*.

In general, forecast reconciliation methods consider the loss function [@Ben_Taieb2019-be] given by $$
\begin{aligned}
& \mathrm{E}\left[\left\|\bm{y}_{T+h}-\tilde{\bm{y}}_{T+h \mid T}\right\|_2^2 \mid \bm{I}_T\right] \\
& =\underbrace{\left\|\bm{S G_h}\left(\mathrm{E}\left[\hat{\bm{y}}_{T+h \mid T} \mid \bm{I}_T\right]-\mathrm{E}\left[\bm{y}_{T+h} \mid \bm{I}_T\right]\right)+(\bm{S}-\bm{S G_h S}) \mathrm{E}\left[\bm{b}_{T+h} \mid \bm{I}_T\right]\right\|_2^2}_{\text{bias}} +\underbrace{\operatorname{Tr}\left(\operatorname{Var}\left[\bm{y}_{T+h}-\tilde{\bm{y}}_{T+h \mid T} \mid \bm{I}_T\right]\right)}_{\text{variance}},
\end{aligned}
$$ {#eq-loss} which includes two parts in its decomposition, bias and variance of the reconciled forecasts.

#### Minimum trace reconciliation {.unnumbered}

Let $\hat{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \hat{\bm{y}}_{t+h \mid t}$ denote the $h$-step-ahead in-sample *base forecast errors*, and $\tilde{\bm{e}}_{t+h \mid t} = \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h \mid t}$ denote the $h$-step-ahead *reconciled forecast errors*. Assuming the base forecasts are unbiased and imposing the constraint $\bm{G}_h\bm{ S}=\bm{I}_{n_b}$ to preserve the unbiasedness of the reconciled forecasts, the bias term in Equation \eqref{eq-loss} cancels. @Wickramasuriya2019-fc thus formulated the reconciliation problem as minimizing the trace (MinT) of the $h$-step-ahead covariance matrix of the reconciled forecast errors, leading to the unique solution given by $$
\bm{G}_h=\left(\bm{S}^{\prime} \bm{W}_h^{-1} \bm{S}\right)^{-1} \bm{S}^{\prime} \bm{W}_h^{-1},
$$ {#eq-mint} where $\bm{W}_h$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors.

The MinT problem can be reformulated as a least squares problem with linear constraints: $$
\min_{\tilde{\bm{y}}_{T+h \mid T}} \quad \frac{1}{2}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})^{\prime} \bm{W}_{h}^{-1}(\hat{\bm{y}}_{T+h \mid T}-\tilde{\bm{y}}_{T+h \mid T})
 \qquad \text{s.t.} \quad \tilde{\bm{y}}_{T+h \mid T}=\bm{S}\tilde{\bm{b}}_{T+h \mid T},
$$ {#eq-mint_op} where $\tilde{\bm{b}}_{T+h \mid T} \in \mathbb{R}^{n_b}$ comprises the $h$-step-ahead bottom-level reconciled forecasts, made at time $T$. The intuition behind MinT reconciliation is that the larger the estimated variance of the base forecast errors, the larger the range of adjustments permitted for forecast reconciliation.

It is challenging to estimate $\bm{W}_h$, especially for $h > 1$. It is common to assume $\bm{W}_h = k_h\bm{W}_1$, $\forall h$, where $k_h > 0$; then the MinT solution for $\bm{G}$ remains unchanged across different forecast horizons, $h$. Hence, we will drop the subscript $h$ for ease of exposition. @tbl-bench lists the most popularly used candidate estimators for $\bm{W}_h$. In principle, all optimization methods can be based on either in-sample or out-of-sample data. The methods discussed here are considered "out-of-sample" as they use genuine forecasts, $\hat{\bm{y}}_{T+h \mid T}$, rather than "in-sample" fitted values for the optimization process.

```{r}
#| label: tbl-bench
#| tbl-cap: Forecast reconciliation methods for which different estimators of $\bm{W}_h$ are used.

tbl <- tibble::tribble(
    ~method, ~cite, ~W,
    "OLS", "Hyndman2011-sd", "$\\bm{I}$",
    "WLSs", "Athanasopoulos2017-jj", "$\\operatorname{diag}(\\bm{S} \\bm{1})$",
    "WLSv", "Hyndman2016-cz", "$\\operatorname{Diag}(\\hat{\\bm{W}}_1)$",
    "MinT", "Wickramasuriya2019-fc", "$\\hat{\\bm{W}}_1$",
    "MinTs", "Wickramasuriya2019-fc", "$\\lambda\\operatorname{Diag}(\\hat{\\bm{W}}_1) + (1-\\lambda)\\hat{\\bm{W}}_1$"
  ) |>
  dplyr::mutate(
    method = glue::glue("\\textbf{{{method}}} \\citep{{{cite}}}"),
  ) |>
  dplyr::select(-cite)
kable(tbl,
    format = "latex",
    align = c("p{0.5\\linewidth}","r"),
    booktabs = TRUE,
    col.names = c("Reconciliation method",
    "$\\bm{W}_h\\propto$"),
    escape = FALSE
  ) |>
  kable_paper(full_width = FALSE) |>
  footnote(
    general = "\\\\footnotesize{Note: $\\\\bm{1}$ is a vector of 1s of size $n_b$, $\\\\operatorname{diag}(\\\\cdot)$ constructs a diagonal matrix using a given vector, $\\\\hat{\\\\bm{W}}_1$ denotes the unbiased covariance estimator based on the in-sample one-step-ahead base forecast errors (i.e., residuals), and $\\\\operatorname{Diag}(\\\\cdot)$ forms a diagonal matrix using the diagonal elements of the input matrix.}",
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE,
    footnote_as_chunk = T
  )
```

<!-- @Pritularga2021-lz demonstrated that the performance of forecast reconciliation is affected by two sources of uncertainties: the base forecast uncertainty and the reconciliation weight uncertainty. Recall that the uncertainty in the MinT solution in Equation \eqref{eq-mint} is introduced by the uncertainty in the weighting matrix as the summing matrix is fixed for a given structure. This indicates that OLS and WLSs estimators for $\bm{W}$ may lead to less volatile reconciliation performance compared to WLSv, MinT, and MinTs estimators. @Panagiotelis2021-mf provided a geometric intuition for reconciliation and showed that, when considering the Euclidean distance loss function, OLS reconciliation yields results that are at least as favorable as the base forecasts, whereas MinT reconciliation performs poorly relative to the base forecasts. However, when considering the expected Euclidean distance of the reconciled forecast error, @Wickramasuriya2021-am indicated that MinT reconciliation is better than OLS reconciliation. Therefore, which estimator for $\bm{W}$ to use hinges on the specific time series structure of interest, the targeted level or series, and the selected loss function. -->

#### Relaxation of the unbiasedness assumptions {.unnumbered}

@Ben_Taieb2019-be proposed a reconciliation method relaxing the assumption of unbiasedness. Their goal was to achieve a tradeoff between bias and variance by directly minimizing the mean squared reconciled forecast errors in Equation \eqref{eq-loss}. By expanding the training window incrementally, one observation at a time, they formulated the reconciliation problem as a regularized empirical risk minimization (RERM) problem: $$
\min_{\bm{G}_h} \frac{1}{(T-T_1-h+1)n}\left\|\bm{Y}_{h}^{*}-\hat{\bm{Y}}_{h}^{*} \bm{G}_{h}^{\prime} \bm{S}^{\prime}\right\|_F^2+\lambda\|\operatorname{vec}( \bm{G}_h)\|_1,
$$ where $T_1$ denotes the minimum number of observations used for model training, $\left\| \cdot \right\|_F$ is the Frobenius norm, $\|\cdot\|_1$ is the $L_1$ norm, $\operatorname{vec}(\cdot)$ denotes the vectorization of a matrix (stacking the columns of the matrix), $\bm{Y}_{h}^{*}=\left[\bm{y}_{T_1+h}, \ldots, \bm{y}_T\right]^{\prime}$, $\hat{\bm{Y}}_{h}^{*}=\left[\hat{\bm{y}}_{T_1+h \mid T_1}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime}$, and $\lambda \geq 0$ is a regularization parameter.

When $\lambda = 0$, the problem reduces to an empirical risk minimization (ERM) problem without regularization. Assuming that the series in the structure are jointly weakly stationary and $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is invertible, it has a closed-form solution given by $$
\hat{\bm{G}}_h = \bm{B}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\left(\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}\right)^{-1},
$$ where $\bm{B}_{h}^{*}=\left[\bm{b}_{T_1+h}, \ldots, \bm{b}_T\right]^{\prime}$. If $\hat{\bm{Y}}_{h}^{*\prime}\hat{\bm{Y}}_{h}^{*}$ is not invertible, a generalized inverse can be applied. When $\lambda > 0$, imposing the $L_1$ penalty on $\bm{G}_h$ will introduce sparsity and reduce estimation variance, albeit at the cost of introducing some bias.

Relaxing the assumption of unbiasedness of base forecasts, @Wickramasuriya2021-am proposed an empirical MinT (**EMinT**) solution by minimizing the trace of the covariance matrix of the reconciled forecast errors. Assuming the series are jointly weakly stationary, the solution is given by $$
\hat{\bm{G}}_{h} = \bm{B}_{h}^{\prime}\hat{\bm{Y}}_{h}\left(\hat{\bm{Y}}_{h}^{\prime}\hat{\bm{Y}}_{h}\right)^{-1},
$$ where $\bm{B}_{h}=\left[\bm{b}_{h}, \ldots, \bm{b}_T\right]^{\prime}$, and $\hat{\bm{Y}}_{h}=\left[\hat{\bm{y}}_{h \mid 0}, \ldots, \hat{\bm{y}}_{T \mid T-h}\right]^{\prime}$.

The difference between EMinT and ERM lies in the data sources. EMinT is an "in-sample" method in the sense that $\hat{\bm{Y}}_{h}$ are predictions in the form of fitted values, while ERM (and also RERM) is an "out-of-sample" method, with $\hat{\bm{Y}}_{h}^{*}$ being genuine forecasts generated on a holdout validation set. Both EMinT and ERM consider an estimate of $\bm{G}$ that changes over the forecast horizon, which is why we keep the subscript $h$ here.

In practical settings, some series in a hierarchy could have poor base forecasts due to model misspecification or low forecastability. Specifically, within a hierarchical structure, the influence of unforeseen events may prompt a forecaster to make a bad decision, leading to the use of a misspecified forecasting model for a specific time series and, consequently, yielding inferior forecasts. Moreover, lower-level time series are normally characterized by less apparent trend and seasonality, large intermittence, and volatility, rendering them more challenging to predict and resulting in poor forecasts.

A challenge in forecast reconciliation arises when some base forecasts perform poorly, as the weighting matrix $\bm{G}$ assimilates *all* base forecasts and maps them into bottom-level forecasts, which are subsequently summed by $\bm{S}$. While the RERM method introduces sparsity by shrinking some elements of $\bm{G}$ towards zero, it remains incapable of mitigating the adverse impact of underperforming base forecasts. Moreover, the method is time-consuming because it uses expanding windows to recursively generate out-of-sample base forecasts.

In addition to @Ben_Taieb2019-be, several other contributions have incorporated diverse forms of shrinkage or penalization in forecast reconciliation methodologies. For example, @Pang2022-hi introduced a group Lasso penalty on weights assigned to clusters artificially added in a hierarchy to select ideal clusters. Their objective function focuses on a new hierarchical structure encompassing geographic and data cluster hierarchies, while disregarding forecast errors associated with zero-weighted clusters. Furthermore, they derive the optimal weight vector and optimal bottom level forecasts by solving the objective successively, leading to a time-consuming method that does not permanently mitigate the negative impact of poorly performing clusters on reconciliation performance. To address the insufficient emphasis on coherence in machine learning methods, @Mishchenko2019-as and @Gleason2020-fo included a regularization term to penalize forecast incoherence. However, these soft constraints do not ensure coherence. @Nystrup2020-te and @Nystrup2021-di considered the autocorrelation in forecast errors and used a shrinkage estimator or eigendecomposition of the cross-correlation matrix, effectively overcoming estimation inefficiencies in approximating $\bm{W}$ within a temporal hierarchy. Nonetheless, none of the aforementioned contributions achieve time series selection in forecast reconciliation, failing to alleviate their adverse impact on forecast performance, while maintaining consideration for forecast errors across the entire initial hierarchy.

We therefore propose two types of forecast reconciliation methods involving time series selection: constrained "out-of-sample" reconciliation and unconstrained "in-sample" reconciliation. These methods aim to address the negative effect of some poor base forecasts on the overall performance of the reconciled forecasts. Additionally, through the incorporation of regularization in the objective function, our method improves reconciliation outcomes produced with a "poor" choice of $\bm{W}$.

## Forecast reconciliation with time series selection {#sec-methodology}

In this section, we introduce our methods for forecast reconciliation while automatically achieving time series selection. @sec-constrained introduces constrained "out-of-sample" reconciliation methods, formulated based on genuine forecasts, while @sec-unconstrained presents an unconstrained "in-sample" reconciliation method, where the problem is formulated using in-sample observations and predictions in the form of fitted values.

### Series selection under the unbiasedness assumption {#sec-constrained}

As $\bm{S}$ is fixed and $\hat{\bm{y}}_{T+h \mid T}$ is given, $\bm{G}_h$ determines the linear reconciliation performance, as shown in Equation \eqref{eq-lr}. We drop the subscript $h$ here as we assume $\bm{W}$ and $\bm{G}$ do not vary with the forecast horizon. A natural way to remove forecasts of some series is by controlling the number of nonzero column entries in $\bm{G}$. This leads to a generalization of the MinT optimization problem with an additional penalty term: $$
\min_{\bm{G}} \quad \frac{1}{2}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{SG}\hat{\bm{y}}\right)
+ \lambda\mathfrak{g}(\bm{G}) \qquad \text{s.t.} \quad \bm{G}\bm{S}=\bm{I},
$$ {#eq-op_u} where $\hat{\bm{y}}:=\hat{\bm{y}}_{T+1 \mid T}$, $\mathfrak{g}(\cdot)$ penalizes the columns of $\bm{G}$ towards zero, and $\lambda$ is a penalty parameter. The methods developed within this framework are "out‑of‑sample" in the sense that $\hat{\bm{y}}$ are genuine one-step-ahead forecasts. This can be considered *a grouped variable selection problem*, with each group corresponding to a column of $\bm{G}$. When $\lambda = 0$, the problem reduces to the MinT optimization problem in Equation \eqref{eq-mint_op} with a closed-form solution given by Equation \eqref{eq-mint}.

The constraint $\bm{G}\bm{S}=\bm{I}$ guarantees that the reconciled forecasts remain unbiased if the base forecasts are unbiased. Under this assumption and constraint, minimizing the loss function in Equation \eqref{eq-loss} simplifies to the MinT problem formulated in Equation \eqref{eq-mint_op}, which underpins the constrained "out-of-sample" reconciliation methods within the framework in equation \eqref{eq-op_u}.

::: {#prp-1}
If the assumption that forecast reconciliation preserves unbiasedness is imposed by enforcing $\bm{GS}=\bm{I}$, then the number of nonzero column entries of $\hat{\bm{G}}$ (the solution to Equation \eqref{eq-op_u}) will be no less than $n_b$. Moreover, the constraint $\bm{GS}=\bm{I}$ enforces that the selected columns of $\hat{\bm{G}}$ will correspond to variables that can "restore" the hierarchy.
:::

::: proof
See [Appendix A](#appendix-proofs), supplementary materials.
:::

For example, for the simple hierarchy shown in @fig-hts, the selected columns of $\hat{\bm{G}}$ will be at least $n_b=4$. Our constrained reconciliation methods might simultaneously zero out the columns of $\bm{G}$ corresponding to series AA and BA, but not to series AA and AB.

::: {#prp-2}
The optimization problem in Equation \eqref{eq-op_u} can be reformulated as a least squares problem with regularization and linear equality constraint as follows: $$
\begin{aligned}
& \min_{\operatorname{vec}(\bm{G})} \quad \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda\mathfrak{g}\left(\operatorname{vec}(\bm{G})\right) \\
& \text{s.t.} \quad \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}),
\end{aligned}
$$ {#eq-op_u_reg} which is characterized as a high-dimensional problem in which the number of features, denoted as $p = n_b \times n$, is much larger than the number of observations, $n$.
:::

::: proof
See [Appendix A](#appendix-proofs), supplementary materials.
:::

Next, we present three constrained "out-of-sample" reconciliation methods: (i) group best-subset selection with ridge regularization, (ii) parsimonious method with $L_0$ regularization, and (iii) group lasso method. These methods perform forecast reconciliation with series selection under the unbiasedness assumption, differing only in the regularization term employed.

#### Group best-subset selection with ridge regularization {.unnumbered}

In a high-dimensional context with $p \gg n$, it is common to assume that the true regression coefficient (i.e., $\operatorname{vec}(\bm{G})$ in our problem) is sparse. We apply a combination of $L_0$ and $L_2$ regularization to control the nonzero column entries in $\bm{G}$: $$ \begin{aligned}
\min_{\operatorname{vec}(\bm{G})} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda_0 \sum_{j=1}^n 1\left(\bm{G}_{\cdot j} \neq \bm{0}\right) + \lambda_2 \left\|\operatorname{vec}\left(\bm{G}\right)\right\|_2^2 \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}(\bm{I}_{n_b}),
\end{aligned}
$$ {#eq-subset} where $1(\cdot)$ is the indicator function, $\lambda_0 \geq 0$ controls the number of nonzero columns of $\bm{G}$, $\lambda_2 \geq 0$ controls the strength of the ridge regularization, and $\|\cdot\|_2$ is the $L_2$ norm. In a hierarchical or grouped time series context, $\operatorname{vec}(\bm{G})$ has an inherent non-overlapping grouping structure, wherein each group corresponds to a single column of $\bm{G}$, each of size $n_b$. Hence, we call this reconciliation method *group best-subset selection with ridge regularization*. In the results that follow, we label the **Subset** method differently based on various $\bm{W}$ estimators, referring to them as **OLS-subset**, **WLSs-subset**, **WLSv-subset**, **MinT-subset**, and **MinTs-subset**, respectively.

The best-subsets estimator, derived from an $L_0$-regularized least squares problem, is a natural and direct candidate for sparse learning. The $L_0$ penalty leads to models that have a subset of coefficients exactly equal to zero, effectively performing variable selection. The statistical properties of the best-subsets estimator have been extensively studied; see, for example, @Greenshtein2004-be, @Zhang2012-ge, and the references therein. However, @Mazumder2022-hx argued that the vanilla $L_0$ penalization could suffer from overfitting in low SNR settings. To address the issue, we incorporate a ridge regularization in Equation \eqref{eq-subset}, motivated by earlier work on best-subset selection [e.g., @Hazimeh2020-xd; @Mazumder2022-hx], which suggests that additional ridge regularization helps mitigate the poor predictive performance of best-subset selection method in low SNR regimes.

We present a Big-M based mixed integer programming (MIP) formulation for the problem in Equation \eqref{eq-subset}: $$
\begin{aligned}
\min_{\operatorname{vec}(\bm{G}), \bm{z}, \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j + \lambda_2 \bm{g}^{+\prime}\bm{g}^{+} \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right) \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bm{G}) = \check{\bm{e}} \\
& \sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M} z_j, \quad j \in[n] \\
& \bm{g}^{+} \geqslant \operatorname{vec}(\bm{G}) \\
& \bm{g}^{+} \geqslant-\operatorname{vec}(\bm{G}) \\
& z_j \in\{0,1\}, \quad j \in[n],
\end{aligned}
$$ {#eq-subset_mip} where $\mathcal{M}$ is a Big-M parameter (specified a-priori) that is sufficiently large that the optimal solution to Equation \eqref{eq-subset_mip}, $\bm{g}^{+*}$, satisfies $\max_{j \in [n]}\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. The binary variable $z_j=0$ implies that $\bm{G}_{\cdot j}=\bm{0}$, and $z_j=1$ implies that $\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. Such Big-M formulations are commonly used in MIP problems to model relations between discrete and continuous variables, and have been recently explored in regression with $L_0$ regularization [@Bertsimas2016-ig]. The problem is a mixed integer quadratic program (MIQP) that can be solved using commercial MIP solvers, e.g., Gurobi and CPLEX.

**Parameter tuning.** To avoid computationally expensive cross-validation, we tune the parameters to minimize the sum of squared reconciled forecast errors on the truncated training set, comprising only the $\max\{h, s\}$ observations closest to the forecast origin, where $s$ is the seasonal period for seasonal data and $s=T$ for non-seasonal data. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$, which captures the scale of the first term in the objective function, where $\tilde{\bm{y}}^{\text{bench}}$ is a vector of reconciled forecasts obtained using Equation \eqref{eq-mint} with the same estimator of $\bm{W}$, and define $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$. For the parameter $\lambda_0$, we consider a grid of $k+1$ values, $\{\lambda_{0}^{1},\dots,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. So $\lambda_{0}^{1},\dots,\lambda_{0}^{k}$ is a sequence decreasing on the log scale. We use a grid of six values for the parameter $\lambda_2$, $\{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$. Thus, we tune over a two-dimensional grid of $(k+1) \times 6$ values to find the optimal combination of $\lambda_0$ and $\lambda_2$.

**Computation details.** The MIQP problem in Equation \eqref{eq-subset_mip} is NP-hard and computationally intensive. @Bertsimas2016-ig showed that commercial MIP solvers are capable of tackling problem instances for $p$ up to a thousand. To address larger instances, there has been impressive work on developing MIP-based approaches for solving $L_0$-regularized regression problem; e.g., @Bertsimas2016-ig, @Hazimeh2020-xd, and @Hazimeh2022-hc. However, it is challenging to extend these approaches to accommodate additional constraints in the optimization problem. Despite potential challenges in handling large instances with commercial MIP solvers, in our experiments, we use Gurobi to solve Equation \eqref{eq-subset_mip} by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for cases with $p > 1000$. This allows to terminate the solver before reaching the global optimum and return a suboptimal solution instead. This strategy is motivated by our need to consider numerous parameter candidates, and the final solution will be validated against the training set, which prevents the use of a poor estimate of $\bm{G}$.

#### Parsimonious method with $L_0$ regularization {.unnumbered}

Instead of estimating the entire matrix $\bm{G}$ as above, we leverage the MinT solution in Equation \eqref{eq-mint} to streamline the optimization problem under consideration. Specifically, we define $\bar{\bm{S}} = \bm{A}\bm{S}$, where $\bm{A} = \operatorname{diag}(\bm{z})$ is an $n \times n$ diagonal matrix, and $\bm{z}$ is an $n$-dimensional vector with elements either equal to 0 or 1. Taking the MinT solution in Equation \eqref{eq-mint}, we have $\bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}$. Given fixed $\bm{S}$ and estimation of $\bm{W}$, $\bar{\bm{G}}$ is entirely determined by $\bm{A}$. Thus, when the $j$th diagonal element of $\bm{A}$ is zero, the $j$th column of $\bar{\bm{G}}$ becomes entirely composed of zeros. Therefore, the optimization problem can be reduced to an integer quadratic programming problem where all of the variables are restricted to being integers: \begin{align*}
\min_{\bm{A}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\bm{S}\bar{\bm{G}}\hat{\bm{y}}\right) + \lambda_0 \sum_{j=1}^n \bm{A}_{jj} \\
\text{s.t.} \quad & \bar{\bm{G}} = (\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S})^{-1}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \qquad\text{and}\qquad \bar{\bm{G}}\bm{S} = \bm{I},
\end{align*} where $\lambda_0 \geq 0$ controls the number of nonzero diagonal elements in $\bm{A}$, consequently affecting the number of nonzero columns (i.e., selected time series) in $\bm{G}$. We call this reconciliation method the *parsimonious method with* $L_0$ *regularization* due to its appeal in reducing the number of parameters. In the results that follow, we label the **Parsimonious** method differently based on various estimators for $\bm{W}$, referring to them as **OLS-parsim**, **WLSs-parsim**, **WLSv-parsim**, **MinT-parsim**, and **MinTs-parsim**, respectively.

In the Parsimonious method, the unknown matrix $\bm{A}$ is restricted to elements of $0$ or $1$. Thus the $L_2$ penalty is excluded from its optimization problem, unlike in the Subset method. We note that achieving time series selection with this optimization problem can be challenging, as identifying a solution $\hat{\bm{A}}$ with some zero diagonal elements while satisfying both the MinT solution and the constraint may be difficult. Thus, the resulting solution tends to be dense and may not have zero columns.

To ensure the invertibility of $\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1}\bm{A}\bm{S}$, and make the problem compatible with Gurobi, we reformulate the problem as $$
\begin{aligned}
\min_{\bm{A},\bar{\bm{G}},\bm{C},\check{\bm{e}},\bm{z}} \quad & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}^{-1}\check{\bm{e}} + \lambda_0 \sum_{j=1}^n z_j \\
\text{s.t.} \quad & \bar{\bm{G}}\bm{S} = \bm{I} \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right)\operatorname{vec}(\bar{\bm{G}}) = \check{\bm{e}} \\
& \bar{\bm{G}}\bm{A}\bm{S} = \bm{I} \\
& \bar{\bm{G}} = \bm{C}\bm{S}^{\prime}\bm{A}^{\prime}\bm{W}^{-1} \\
& z_j \in\{0,1\}, \quad j \in[n].
\end{aligned}
$$ {#eq-parsimonious_mip}

**Parameter tuning.** Similar to the setup in the group best-subset selection, we select the tuning parameter, $\lambda_0$, by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $\max\{h, s\}$ observations that occurred prior to the forecast origin. Let $\lambda_{0}^{1} = \frac{1}{2}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\tilde{\bm{y}}^{\text{bench}}\right)$, and $\lambda_{0}^{k} = 0.0001\lambda_{0}^{1}$, the collection of candidate values for $\lambda_0$ we consider is $\{\lambda_{0}^{1},\dots,\lambda_{0}^{k}, 0\}$, where $\lambda_{0}^{j} = \lambda_{0}^{1}\left(\lambda_{0}^{k} / \lambda_{0}^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$.

**Computation details.** Following a setup akin to that in the group best-subset selection, we employ Gurobi to solve Equation \eqref{eq-parsimonious_mip} by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds for problems with $p > 1000$.

#### Group lasso method {.unnumbered}

@Yuan2006-mw introduced the group lasso method, which extends lasso to situations with a grouped structure among variables. Similar to lasso, group lasso induces sparsity, but at the group level, leading to more interpretable models by reducing the number of non-zero groups of coefficients. @Lounici2011-or demonstrated that group lasso enhances prediction and estimation properties compared to the traditional lasso method. The statistical properties of the group-lasso estimator have been extensively studied in the literature [e.g., @Nardi2008-asymptotic].

When the problem of forecast reconciliation with time series selection is reframed as a least squares problem, our goal is to perform group-wise variable selection. Specifically, the unknown paramter $\operatorname{vec}(\bm{G})$ possesses an inherent grouping structure, with each group corresponding to a single column of $\bm{G}$, each of size $n_b$. Thus, we consider *a group lasso problem under the unbiasedness assumption* given by $$
\begin{aligned}
\min_{\bm{G}} \quad & \frac{1}{2}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right)^{\prime} \bm{W}^{-1}\left(\hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G})\right) + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2 \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right),
\end{aligned}
$$ {#eq-lasso} where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make the model more flexible, and the second term in the objective is the penalty function that is intermediate between the $L_1$-penalty that is used in the lasso and the $L_2$-penalty that is used in ridge regression. In the results that follow, we label the **Lasso** method based on various estimators for $\bm{W}$, referring to them as **OLS-lasso**, **WLSs-lasso**, **WLSv-lasso**, **MinT-lasso**, and **MinTs-lasso**, respectively.

Next, we present the second order cone programming (SOCP) formulation for the group lasso based estimators given by $$
\begin{aligned}
\min_{\operatorname{vec}(\bm{G}), \check{\bm{e}}, \bm{g}^{+}} & \frac{1}{2}\check{\bm{e}}^{\prime} \bm{W}_h^{-1}\check{\bm{e}} + \lambda \sum_{j=1}^n w_j c_j \\
\text{s.t.} \quad & \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G})=\operatorname{vec}\left(\bm{I}_{n_b}\right) \\
& \hat{\bm{y}}-\left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}) = \check{\bm{e}} \\
& c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}, \quad j \in[n].
\end{aligned}
$$ {#eq-lasso_socp} Equation \eqref{eq-lasso_socp} includes additional auxiliary variables $c_j \in \mathbb{R}_{\geq 0}$, $j \in [n]$, and second order cone constraints, $c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+2}}$ for $j \in[n]$.

Compared to the previous two methods above, the group lasso method is computationally friendlier. Nonetheless, @Hazimeh2023-ie demonstrated, both empirically and theoretically, that the group $L_0$-regularized method exhibits advantages over its group lasso counterpart across a range of regimes. Group lasso can either be highly dense or possess non-zero coefficients that are overly shrunk. This issue becomes more pronounced when the groups are correlated with each other, as group lasso tends to retain all correlated groups instead of seeking a more concise model.

**Penalty weights and parameter tuning.** In the context of group lasso, the default choice for the penalty weight, $w_j$, is $\sqrt{p_j}$, where $p_j$ is the size of each group (in our case, $p_j = n_b$). In our experiments, we allocate different penalty weights to each group using $w_j = 1/\|\bm{G}_{\cdot j}^{\text{bench}}\|_2$, which allows us to account for variations in scale across different time series in the structure.

We compute the group lasso over $k+1$ values of the tuning parameter $\lambda$, and select the parameter by optimizing the sum of squared reconciled forecast errors on a truncated training set, consisting only of $\max\{h, s\}$ observations occurred prior to the forecast origin. The collection of candidate values for $\lambda$ is $\{\lambda^{1},\dots,\lambda^{k}, 0\}$, where $\lambda^{1} = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}(\lambda^{k} / \lambda^{1})^{(j-1) / (k-1)}$ for $j \in [k]$.

::: {#prp-3}
Ignoring the constraint $\bm{G_h S}=\bm{I_{n_b}}$, we define $\lambda^{1}$ as the smallest $\lambda$ value such that all predictors in the group lasso problem have zero coefficients. Then we have $$
\lambda^{1} = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j,
$$ where $j^{*}$ denotes the column index of $\hat{\bm{y}}^{\prime} \otimes \bm{S}$ that corresponds to the $j$th column of $\bm{G}$.
:::

::: proof
See [Appendix A](#appendix-proofs), supplementary materials.
:::

**Computation details.** Due to the incorporation of the constraint, we can not directly use some open-source packages designed for group lasso. Consequently, we employ Gurobi to solve the SOCP problem, configuring it by setting OptimalityTol = $0.0001$.

### Series selection relaxing the unbiasedness assumption {#sec-unconstrained}

In this section, we relax the unbiasedness assumption, and introduce a reconciliation method with selection that relies on in-sample observations and fitted values. Let $\bm{Y} \in \mathbb{R}^{T \times n}$ denote a matrix comprising observations from all time series on the training set in the structure, and $\hat{\bm{Y}} \in \mathbb{R}^{T \times n}$ denote a matrix of in-sample one-step-ahead forecasts (i.e., fitted values) for all time series. The proposed *empirical group lasso* method considers the optimization problem $$
\min_{\bm{G}} \quad \frac{1}{2 T} \left\|\bm{Y}-\hat{\bm{Y}} \bm{G}^{\prime} \bm{S}^{\prime}\right\|_F^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$ where $\lambda \geq 0$ is a tuning parameter, $w_j \neq 0$ is the penalty weight assigned in $\bm{G}_{\cdot j}$ to make a more flexible model. We rewrite the problem as $$
\min_{\operatorname{vec}(\bm{G})} \quad \frac{1}{2 T} \left\|\operatorname{vec}(\bm{Y})-(\bm{S} \otimes \hat{\bm{Y}}) \operatorname{vec}\left(\bm{G}^{\prime}\right)\right\|_2^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$ which becomes a standard group lasso problem, with $\operatorname{vec}(\bm{Y})$ serving as the dependent variable and $\bm{S} \otimes \hat{\bm{Y}}$ as the covariate matrix. We denote this as **Elasso** in the results that follow.

Unlike the methods introduced in @sec-constrained, Elasso relaxes the unbiasedness conditions on both base and reconciled forecasts and operates as an "in-sample" method because $\hat{\bm{Y}}$ are predictions in the form of fitted values (i.e., $\bm{Y}$ is in training data when base forecasts are computed). Thus Elasso aims to directly minimize the mean squared reconciled forecast errors, as described in Equation \eqref{eq-loss}, rather than focusing solely on the variance term. This clarifies two points: (1) why the Elasso method omits the $\bm{GS}=\bm{I}$ constraint, which ensures the unbiasedness of reconciled forecasts when base forecasts are unbiased, and (2) why the Elasso method does not use a $\bm{W}$ matrix, which is introduced when deriving from $\operatorname{Var}(\tilde{\bm{e}}_{T+h \mid T})$ under the constraint.

We note that both the "in-sample" Elasso and EMinT methods require only one round of model training. Similarly, the Subset, Parsimonious, and Lasso methods also need only one round of training and forecasting, despite their reliance on genuine out-of-sample forecasts. In contrast, the "out-of-sample" RERM and ERM methods use an iterative approach with expanding windows for out-of-sample forecasts, demanding extensive rounds of model training and significant computation time. To ensure a fair comparison, we exclude RERM and ERM methods from simulation studies and empirical results.

Relaxing the unbiasedness assumption may result in fewer non-zero column entries in the $\bm{G}$ solution than the number of series at the bottom level. This differs from constrained reconciliation methods detailed in @sec-constrained. In an extreme scenario, the solution may take the form of a top-down $\bm{G}_{TD}=[\bm{p} \mid \bm{O}_{n_b \times (n-1)}]$, where only the column corresponding to the top level (most aggregated level) retains non-zero values, and $\bm{p} = (p_1, p_2, \ldots, p_{n_b})$ is a proportionality vector obtained based on in-sample reconciled forecast errors.

We also explored the empirical version of group best-subset selection with ridge regularization and the parsimonious method with $L_0$ regularization in which we omit the unbiasedness assumption. It is worth mentioning that @Hazimeh2023-ie presented an algorithmic framework for formulating the group $L_0$ problem with ridge regularization and provided the **L0Group** Python package for implementation. However, our experiments showed that this algorithm can not terminate within five hours for typical instances with $p \sim 10^4$. Therefore, in this paper, we only present the empirical group lasso method for series selection without the unbiasedness assumption.

**Penalty weights and parameter tuning.** Similar to the setup in the group lasso method, we assign different penalty weights to each group by setting $w_j = 1/\|\bm{G}_{\cdot j}^{\text{OLS}}\|_2$, where $\bm{G}^{\text{OLS}}$ is the solution obtained by the OLS estimator of $\bm{W}$. Given a fixed tuning parameter, we solve the target optimization problem by considering the initial $T-T_v$ observations, where $T_v = \max\{h, s\}$ for seasonal time series and $T_v = \lfloor \frac{1}{10}T \rfloor$ for non-seasonal time series. Then the tuning parameter, $\lambda$, is selected by minimizing the sum of squared reconciled forecast errors on a truncated training set, comprising only the $T_v$ observations closest to the forecast origin. Specifically, for $\lambda$ values, we consider $\{\lambda^{1},\dots,\lambda^{k}, 0\}$, where $\lambda^{1} = \max_{j=1, \ldots, n}\left\|-\frac{1}{N}\left(\left(\bm{S} \otimes \hat{\bm{Y}}\right)_{\cdot j*}\right)^{\prime} \operatorname{vec}(\bm{Y})\right\|_2 / w_j$, $\lambda^{k} = 0.0001\lambda^{1}$, and $\lambda^{j} = \lambda^{1}\left(\lambda^{k} / \lambda^{1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. Following the derivation in the proof of @prp-3, $\lambda^{1}$ is the smallest $\lambda$ value such that all predictors in the empirical group lasso problem have zero coefficients, i.e., $\bm{G} = \bm{O}$. Note that we need to resolve the optimization problem based on the whole training set by using the optimal tuning parameter to obtain the final solution.

**Computation details.** While there are open-source packages available for solving group lasso problems, they are still relatively slow when handling large instances. For example, given a specific value for the parameter, $\lambda$, our experiments observed that, using the **gglasso** R package, we can not obtain a solution within five hours for typical instances with $p \sim 10^4$. Instead, we use Gurobi to solve the problem using the SOCP formulation for the empirical group lasso which aligns with Equation \eqref{eq-lasso_socp} but omits the constraint.

## Monte Carlo simulations {#sec-simulations}

To assess the proposed reconciliation methods with time series selection outlined in @sec-methodology, we carry out two simulations with different designs. Both simulations consider a hierarchy comprising two levels of aggregation, as shown in @fig-hts. The bottom-level series are first generated and then summed to obtain the aggregated series at higher levels.

<!-- @sec-sim1 considers a setup where the bottom-level series are generated using a structural time series model, but model misspecification exists for some series within the structure. @sec-sim2 explores the impact of the correlation between series on the performance of reconciled forecasts. -->

### Setup 1: Exploring the effect of model misspecification {#sec-sim1}

We follow a simulation setup similar to @Wickramasuriya2019-fc, assuming that the bottom-level time series are generated using the basic structural time series model $$
\bm{b}_t=\bm{\mu}_t+\bm{\gamma}_t+\bm{\eta}_t,
$$ where $\bm{\mu}_t$ and $\bm{\gamma}_t$ are trend and seasonal components defined by \begin{align*}
\bm{\mu}_t & =\bm{\mu}_{t-1}+\bm{v}_t+\bm{\varrho}_t, &&& \bm{\varrho}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_{\varrho}^2 \bm{I}_4\right), \\
\bm{v}_t & =\bm{v}_{t-1}+\bm{\zeta}_t, &&& \bm{\zeta}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\zeta^2 \bm{I}_4\right), \\
\bm{\gamma}_t & =-\sum_{i=1}^{s-1} \bm{\gamma}_{t-i}+\bm{\omega}_t, &&& \bm{\omega}_t & \sim \mathcal{N}\left(\bm{0}, \sigma_\omega^2 \bm{I}_4\right),
\end{align*} $\bm{\varrho}_t$, $\bm{\zeta}_t$, and $\bm{\omega}_t$ are error terms independent of each other and over time, and $\bm{\eta}_t$ is generated independently from an $\text{ARIMA}(p,0,q)$ process, where $p$ and $q$ take values of $0$ or $1$ with equal probability. Coefficients in the ARIMA process are randomly sampled from a uniform distribution $U(0.5, 0.7)$, and the contemporaneous error covariance matrix is given by $$
\left[\begin{array}{llll}
5 & 3 & 2 & 1 \\
3 & 4 & 2 & 1 \\
2 & 2 & 5 & 3 \\
1 & 1 & 3 & 4
\end{array}\right],
$$ which enables correlations among time series in a hierarchical structure.

We set $s = 4$ for quarterly data with error variances $\sigma_{\varrho}^2=2$, $\sigma_\zeta^2=0.007$, and $\sigma_\omega^2=7$. Initial values for $\bm{\mu}_0$, $\bm{v}_0$, $\bm{\gamma}_0$, $\bm{\gamma}_1$, and $\bm{\gamma}_2$ are generated independently from a multivariate normal distribution with zero mean and identity covariance matrix. For each bottom-level series, we generate $180$ observations, with the last $h = 16$ observations forming the test set. The bottom-level series are then aggregated to form higher-level data. This entire process is repeated $500$ times.

We use ETS models to generate base forecasts for each hierarchy using default settings from the **forecast** R package [@Hyndman2023-fc]. To introduce model misspecification, we artificially degrade the performance of series A at the middle level by applying a $1.5$ multiplier to its in-sample and out-of-sample forecasts (i.e., fitted values and base forecasts). We also repeated the analysis with model misspecification for series AA at the bottom level and series Total at the top level, respectively. The results for these two scenarios are similar and reported in [Appendix B](#appendix-sim1).

As elaborated following the introduction of each method in @sec-methodology, we conduct parameter tuning by minimizing the sum of squared reconciled forecast errors on a truncated training set. We experimented with directly using the estimated weighting matrix $\bm{G}$ derived from various candidate parameter values for forecast reconciliation. The results indicated significant variability in reconciliation outcomes depending on the parameter values. However, the optimal parameters identified through our tuning method consistently yielded stable and well-performing reconciliation results.

```{r}
#| label: tbl-sim-rmse
#| results: asis
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for simulated data in Setup 1. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

sim_rmse <- readRDS(here::here("paper/results/sim_rmse.rds"))
latex_table(sim_rmse$s2)
```

```{r}
#| label: fig-sim-plots
#| results: hide
#| fig-width: 6
#| fig-height: 6
#| fig-cap: MCB test result and correlation matrix heatmap for simulated data in Setup 1.
#| fig-subcap: 
#|   - "MCB test for hierarchy"
#|   - "Correlation between forecast errors"
#| layout: [[48, -4, 48]]
#| layout-valign: bottom

# MCB test
sim_mcb <- readRDS(here::here("paper/results/sim_rmse_mcb.rds"))
nemenyi(sim_mcb$s2, conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
        Title = "",
        Xlab = "Mean ranks",
        Ylab = "")

# Correlation heatmap
sim_cormat <- readRDS(here::here("paper/results/sim_cormat.rds"))
ggplot(data = sim_cormat$s2, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(data = sim_cormat$s2,
            mapping = aes(Var1, Var2,
                          label = round(value, digit = 2)),
            color = "black") +
  scale_fill_gradient2(low = "white", high = "#452363",
                       name="Correlation") +
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5)) +
  theme_heatmap
```

@tbl-sim-rmse summarizes the average root mean squared error (RMSE) results for each level and the overall structure (denoted as Average). The BU row uses the "bottom-up" approach, where base forecasts at the bottom level are aggregated to generate higher-level forecasts. We also perform the multiple comparisons with the best (MCB) test at a $95\%$ confidence level, as shown in @fig-sim-plots-1, to assess the statistical significance of differences among the methods in Table @tbl-sim-rmse. With MCB, the ranking performances are statistically different if the intervals of two methods do not overlap.

We find that the BU method performs the best overall, which is unsurprising given that the aggregated series A is deteriorated in this setup, and the BU method does not use its information. WLSv, MinT, and MinTs also perform well, with no significant difference from BU, as they account for the in-sample covariance of base forecast errors, allowing for larger adjustments in reconciliation for forecasts with higher error variance. However, OLS and WLSs significantly underperform the other benchmark methods. Our proposed methods show significant improvements when using OLS and WLSs estimators of $\bm{W}$, which do not consider in-sample covariance. A key advantage of our forecast reconciliation methods with selection is their ability to reduce differences introduced by using different estimates of $\bm{W}$, thereby mitigating the risk of estimator selection. We can align the forecast accuracy achieved using different estimators, making them approach the best results we can obtain. By relaxing the unbiasedness assumption, Elasso performs similarly to EMinT overall, with slight improvements at aggregated levels, which is typically the primary concern for practitioners.

```{r}
#| label: tbl-sim-selection
#| results: asis
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 1. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

sim_selection <- readRDS(here::here("paper/results/sim_selection.rds"))
sim_rmsse <- readRDS(here::here("paper/results/sim_series_rmsse.rds"))

latex_sim_nos_table(sim_selection$s2$z, sim_selection$s2$n, sim_rmsse$s2, "s2")
```

In addition, we report the proportion of time series being selected by our proposed methods across $500$ simulation instances, along with the average root mean squared scaled error (RMSSE) for each series, as shown in @tbl-sim-selection. Our methods select fewer time series, while enhancing forecast accuracy compared to benchmarks. Subset methods, in particular, select fewer time series than Parsimonious and Lasso, which aligns with our expectations that the Parsimonious and Lasso methods tend to produce dense estimates. Notably, the misspecified series A shows higher RMSSE and is selected least frequently, except in cases where all bottom-level series are retained so series A leads to only slight deterioration. Series Total and B are also selected less often due to the high correlation of their forecast errors with other series, as shown in @fig-sim-plots-2.

### Setup 2: Exploring the effect of correlation {#sec-sim2}

We now simulate a hierarchical structure with correlated series, using a similar simulation to @Wickramasuriya2021-am, and the same hierarchical structure as shown in @fig-hts. We use a stationary VAR(1) data generating process for the time series at the bottom level: $$
\bm{b}_t= \bm{c} + \left[\begin{array}{cc}
\bm{A}_1 & \bm{0} \\
\bm{0} & \bm{A}_2
\end{array}\right] \bm{b}_{t-1} + \bm{\varepsilon}_t,
$$ where $\bm{c}$ is a constant vector with all entries set to $1$, $\bm{A}_1$ and $\bm{A}_2$ are $2 \times 2$ matrices with eigenvalues $z_{1,2}=0.6[\cos (\pi / 3) \pm i \sin (\pi / 3)]$ and $z_{3,4}=0.9[\cos (\pi / 6) \pm i \sin (\pi / 6)]$, respectively, $\bm{\varepsilon}_t \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$, where $$
\bm{\Sigma}=\left[\begin{array}{cc}
\bm{\Sigma}_1 & 0 \\0 & \bm{\Sigma}_2
\end{array}\right], \quad\text{and}\quad \bm{\Sigma}_1=\bm{\Sigma}_2=\left[\begin{array}{cc}2 & \sqrt{6} \rho \\\sqrt{6} \rho & 3\end{array}\right],
$$ and $\rho \in \{0, \pm 0.2, \pm 0.4, \pm 0.6, \pm 0.8\}$ controls the error correlation in the simulated hierarchy.

For each time series at the bottom level, we generate a total of $101$ observations, with the last observation serving as the test set, i.e., $T=100$ and $h=1$. Once again, the data at the higher levels are obtained by aggregating the bottom-level series. The process is repeated $500$ times for each candidate correlation, $\rho$.

```{r}
#| label: fig-corr-data
#| results: hide
#| fig-width: 9
#| fig-height: 6.5
#| fig-cap: An example hierarchical time series and its in-sample residuals in Setup 2.

data <- readRDS(here::here("paper/results/corr_data_neg.rds"))
resid <- readRDS(here::here("paper/results/corr_resid_neg.rds"))

theme_plot <- theme_bw() +
  theme(legend.position="bottom",
        legend.margin=margin(0,0,0,0),
        legend.box.spacing = unit(0, "pt"),
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())
p11 <- data |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: observations") +
  theme_plot

p12 <- data |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: observations") +
  theme_plot

p13 <- data |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: observations") +
  theme_plot

p21 <- resid |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "",
       x = "Time",
       title = "Top level: residuals") +
  theme_plot

p22 <- resid |>
  filter(Series %in% c("A", "B")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("A", "B"),
                     values=c("#1B9E77", "#D95F02")) +
  labs(y = "",
       x = "Time",
       title = "Middle level: residuals") +
  theme_plot

p23 <- resid |>
  filter(Series %in% c("AA", "AB", "BA", "BB")) |>
  autoplot(Value) +
  scale_color_manual(labels = c("AA", "AB", "BA", "BB"),
                     values=c("#7570B3", "#E7298A", "#66A61E", "#E6AB02")) +
  labs(y = "",
       x = "Time",
       title = "Bottom level: residuals") +
  theme_plot

(p11 + p21) / (p12 + p22) / (p13 + p23)
```

For each series, base forecasts are generated from ARMA models. We identify the best ARMA model using the automated algorithm implemented in the **forecast** R package [@HK08]. Additionally, when fitting ARMA models for time series Total, A, and BA, we introduce a slight bias by omitting the constant term. @fig-corr-data presents an illustrative example of a simulated hierarchical time series. The left panels depict time plots for each series at different levels of the structure, while the right panels show the residuals obtained from forecasting each series using the fitted ARMA model. Notably, despite our omission of the constant term when fitting ARMA models to series Total, A, and BA, the residuals derived from the identified optimal models still exhibit fluctuations around zero and do not display significant deviations in comparison to the residuals from other series. This is because the influence of the constant term is minimal, i.e., it is much smaller compared to the data variability. Thus, it may be challenging to identify the "poor" base forecasts and exclude them from reconciliation in this setup.

Similarly to @sec-sim1, we experimented with using the estimated weighting matrix obtained from different parameter values for forecast reconciliation. The results showed noticeable variability in the reconciliation outcomes depending on the parameter values. However, our tuning method consistently produced stable and well-performing reconciliation results.

```{r}
#| label: tbl-corr-rmse
#| results: asis
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for simulated data in Setup 2. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

rmse_corr <- readRDS(here::here("paper/results/corr_rmse.rds"))
latex_corr_table(rmse_corr)
```

```{r}
#| label: fig-corr-plots
#| results: hide
#| fig-width: 6
#| fig-height: 6
#| fig-cap: MCB test result and correlation matrix heatmap for simulated data in Setup 2, with the error correlation being -0.8.
#| fig-subcap: 
#|   - "MCB test for hierarchy"
#|   - "Correlation between forecast errors"
#| layout: [[48, -4, 48]]
#| layout-valign: bottom

# MCB test
corr_mcb <- readRDS(here::here("paper/results/corr_rmse_mcb.rds"))
nemenyi(corr_mcb$p1, conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
        Title = "",
        Xlab = "Mean ranks",
        Ylab = "")

# Correlation heatmap
corr_cormat <- readRDS(here::here("paper/results/corr_cormat.rds"))
ggplot(data = corr_cormat$p1, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(data = corr_cormat$p1,
            mapping = aes(Var1, Var2,
                          label = round(value, digit = 2)),
            color = "black") +
  scale_fill_gradient2(low = "white", high = "#452363",
                       name="Correlation") +
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5)) +
  theme_heatmap
```

The results across various error correlations are presented in @tbl-corr-rmse, with the MCB test for a correlation of $-0.8$ shown in @fig-corr-plots-1. The conclusion from the MCB test results for other error correlations are qualitatively similar, thus we report them in [Appendix C](#appendix-sim2). The results show that our proposed methods slightly outperform or are comparable to their respective benchmarks at all levels when using OLS, WLSs, and WLSv estimators. However, only the OLS-lasso method significantly outperforms the OLS benchmark. It is important to note the challenge of identifying the "poor" base forecasts in this simulation design, given that the omission of the constant term has minimal impact relative to the data variability. In addition, the MinT and MinTs methods perform especially well, and our proposed methods provide similar results. This is attributed to the use of in-sample covariance by MinT and MinTs, which allows for large adjustments in reconciliation for base forecasts with high estimated error variance. Elasso performs the best across the whole hierarchy, though not significantly better than EMinT due to the identification challenge. We have also considered alternative error correlation values, $\rho = -0.6, -0.2, 0.2, 0.4$, for this simulation setting, but to save space, we do not present all results. The omitted results follow a similar pattern and are available upon request.

@tbl-corr-selection-neg presents the proportion of time series being selected using our proposed methods, with an error correlation of $-0.8$. The correlation matrix heatmap for forecast errors is displayed in @fig-corr-plots-2. To save space, the results for other error correlations are similar and given in [Appendix C](#appendix-sim2). Despite the challenges in identifying poor-performing series in this setup, @tbl-corr-selection-neg shows that Subset and Parsimonious methods, using OLS, WLSs, and WLSv estimators, are able to exclude the series Total, A, and BA (which have higher RMSSE) in some instances. Notably, Total and A are selected less frequently than BA, likely due to their forecast errors being more correlated with other series, as shown in @fig-corr-plots-2. Subset methods outperform Parsimonious methods in selection. Lasso methods typically select all bottom-level series, given their tendency to yield dense estimates, as discussed in @sec-constrained. When dealing with a high positive error correlation, our methods remain effective in excluding the series that should be omitted in reconciliation. Hence, our methods are preferred, especially when the error correlation within the structure is negative.

```{r}
#| label: tbl-corr-selection-neg
#| results: asis
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 2, with the error correlation being -0.8. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

corr_selection <- readRDS(here::here("paper/results/corr_selection.rds"))
corr_rmsse <- readRDS(here::here("paper/results/corr_series_rmsse.rds"))
latex_sim_nos_table(corr_selection$p1$out_s0$z,
                    corr_selection$p1$out_s0$n,
                    corr_rmsse$p1$out_s0,
                    "corr_p1")
```

## Applications {#sec-applications}

In this section we describe two empirical applications: @sec-labour focuses on a grouped hierarchy built using the Australian labour force survey data released by the Australian Bureau of Statistics, while @sec-tourism considers Australian domestic tourism flows with a natural geographic hierarchy.

### Forecasting Australian labour force {#sec-labour}

The dataset from the Labour Force Survey, released by the Australian Bureau of Statistics, comprises monthly data on the number of unemployed persons in Australia from January 2010 to July 2023[^1]. We address the few missing observations using linear interpolation. Analyzing unemployment data by labour market region and duration of job search offers valuable insights into regional disparities, and the structural nuances underlying unemployment. Forecast reconciliation is crucial in such a case to ensure aligned decision making.

[^1]: The Labour Force Survey data is publicly available at <https://www.abs.gov.au/statistics/labour/employment-and-unemployment/labour-force-australia-detailed/aug-2023>.

We construct a grouped hierarchy by disaggregating the number of unemployed persons over two independent attributes, duration of job search (*Duration*), and State and Territory (*STT* ). At the bottom level, the data are disaggregated by both attributes. We refer to the bottom level as the *Duration* $\times$ *STT* level. Specifically, there are six different groups of job search duration, under 1 month, 1--3 months, 3--6 months, 6--12 months, 1--2 years, and 2 years and over. Additionally, the number of unemployed persons in Australia can be disaggregated by eight states and territories, i.e., NSW, VIC, QLD, SA, WA, TAS, NT, and ACT. So the final grouped hierarchy consists of the top series, six series at the Duration level, eight series at the STT level, and $48$ series at the Duration $\times$ STT level, giving $63$ time series in total, each of length $163$ observations.

<!-- NSW (New South Wales), VIC (Victoria), QLD (Queensland), SA (South Australia), WA (Western Australia), TAS (Tasmania), NT (Northern Territory), and ACT (Australian Capital Territory) -->

```{r}
#| label: fig-labour-data
#| results: hide
#| fig-width: 10
#| fig-height: 6
#| fig-pos: "!t"
#| fig-cap: Australia unemployed persons, disaggregated by state and territory, and by duration of job search.

labour_ts <- readRDS(here::here("paper/results/labour_gts.rds"))

p1 <- labour_ts |>
  filter(Series == "Total") |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Total unemployed persons") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p2 <- labour_ts |>
  filter(Series %in% c("NSW", "VIC", "QLD", "SA", "WA", "TAS", "NT", "ACT")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "State and territory") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p3 <- labour_ts |>
  filter(Series %in% c("Under 1 month", "1-3 months", "3-6 months", "6-12 months",
                       "1-2 years", "2 years and over")) |>
  autoplot(Value) +
  labs(y = "Number of unemployed persons ('000)",
       x = "Time",
       title = "Duration of job search") +
  theme_bw() +
  theme(plot.background = element_blank(),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.text = element_text(face = "bold"),
        axis.ticks.x.top = element_blank())

p1 / (p2 + p3)
```

The top panel in @fig-labour-data shows the total number of unemployed persons in Australia from January 2010 to July 2023, representing the top-level series in the hierarchical structure. The monthly series shows strong seasonality within each year, marked by prominent peaks occurring every January, attributable to school-leavers. Lower peaks occur in July, perhaps impacted by the start of the financial year. Amidst the backdrop of COVID-19's non-essential service shutdowns and trading restrictions, March and April 2020 saw a notable surge in unemployment. However, as coronavirus cases dwindled significantly and restrictions eased in the aftermath, employment made a remarkable recovery, leading to a subsequent decline in unemployment. The bottom-left panel displays the breakdown of unemployed individuals by state and territory, while the bottom-right panel presents the breakdown by the duration of job search. The plots display diverse and rich dynamics both within and between different levels of the hierarchy. For example, there was noticeable growth observed during 2020 for some states such as NSW, VIC, and QLD, whereas other states did not experience such significant growth. Additionally, there is a resemblance in the seasonal patterns between NSW and QLD, while the seasonal pattern in VIC differs. When comparing the series at the STT level and Duration level, the seasonal patterns in the Duration-level series are more consistent and potentially easier to forecast.

We assess the forecast accuracy of base forecasts and various reconciliation methods through a rolling forecast origin approach. Our aim is to generate $1$- to $12$-step-ahead forecasts for each of the $63$ series while ensuring coherence. Given the limited data compared to the forecast horizon, we initiate the process with a training set of $139$ observations for each series. The training set is used to select the optimal ETS model with the automatic algorithm implemented in the **forecast** package for R [@HK08]. Using these fitted ETS models, we generate base forecasts, and perform diverse forecast reconciliation methods. Then we roll the forecast origin forward by one month and repeat the process, until July 2022. We note that it may be challenging to identify the series with "poor" forecasts due to structural changes in the data caused by the COVID-19 pandemic, which affect the accuracy of forecasts across all time series.

```{r}
#| label: tbl-labour-rmse-avg
#| results: asis
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for Australian labour force data. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

rmse_labour_avg <- readRDS(here::here("paper/results/labour_rmse.rds"))
cols <- grepl("1--12", colnames(rmse_labour_avg$table_out)) | grepl("Method", colnames(rmse_labour_avg$table_out))
rmse_labour_avg$table_out <- rmse_labour_avg$table_out[, cols]
colnames(rmse_labour_avg$table_out) <- c("Method", rmse_labour_avg$levels)
latex_table_h(rmse_labour_avg)
```

The average results are presented in @tbl-labour-rmse-avg. The MCB test and error correlation calculations are not performed due to the limited data length. Results of MinT and the respective proposed methods are excluded due to poor performance, likely caused by the poor sample covariance estimator given the small sample size relative to the number of series in the structure. Subset methods using different estimators of $\bm{G}$ generally improve forecast accuracy over their benchmarks, particularly at aggregation levels, which are of paramount concern to practitioners. The exception is WLSs-subset, which shows slight deterioration, though still improving top-level forecasts. Moreover, the Parsimonious and Lasso methods almost always yield results identical to their benchmarks, because they tend to provide dense estimates, and ETS models typically avoid extremely poor forecasts. However, OLS-parsim shows deterioration at all levels except the top level. When we drop the unbiasedness assumption, EMinT performs the worst across all levels, as it assumes joint weak stationarity among series, which is evidently not the case here. Elasso ranks the best overall and significantly outperforms EMinT, with the most accurate coherent forecasts observed at the top level and STT level.

Results based on the final test set spanning from August 2022 to July 2023 are detailed in [Appendix D](#appendix-labour), allowing for model training with more data and analysis of post-COVID patterns. @tbl-labour-rmse shows RMSE results qualitatively similar to those in @tbl-labour-rmse-avg. @tbl-labour-info presents the number of series selected at each level, their average RMSSE values, and the identified optimal tuning parameters. Only Subset and Elasso methods are presented, as they differ from benchmark results. The scale variation in optimal parameters across methods is due to differences in objective function scales. Notably, all Subset methods exclude some series, and Elasso, using only $11$ series, performs best overall. With the exception of the STT level for WLSs-subset, and the Duration level for WLSv-subset and Elasso, the series selected by the proposed methods show reduced RMSSE values at each level. Moreover, series at the STT level are mostly removed, while most Duration level series are retained. This aligns with our data description, highlighting that the seasonal patterns at the Duration level are more consistent and potentially easier to forecast compared to those at the STT level.

### Forecasting Australian domestic tourism {#sec-tourism}

Australian domestic tourism flows are measured as the number of overnight trips Australians spend away from home. The data are sourced from the National Visitor Survey and collected through computer-assisted telephone interviews with approximately $120,000$ residents aged $15$ years and older. The data follow a geographic structure, with national total tourism flows at the top level, then disaggregated into seven states and territories (referred to as *State* level hereafter), further dividing into $27$ zones, and finally into $76$ regions. Thus, $n_b=76$ and $n=111$. Each series spans January 1998 to December 2017, with a total of $240$ observations.

```{r}
#| label: fig-tourism-data
#| results: hide
#| fig-width: 9
#| fig-height: 6
#| fig-cap: Domestic tourism flows from January 1998 to December 2017 for the whole of Australia as well as the states.
tourism_ts <- readRDS(here::here("paper/results/tourism_hts.rds"))
tourism_ts |>
  autoplot(Value) +
  facet_wrap(vars(Series), scales = "free_y", ncol = 2) +
  xlab("Time") +
  ylab("Australian domestic tourism flows ('000)") +
  theme_bw() +
  theme(legend.position = "none",
        plot.background = element_blank(),
        axis.title.y = element_text(face = "bold", size = 14),
        axis.title.x = element_text(face = "bold", size = 12),
        axis.text = element_text(face = "bold", size = 10),
        axis.ticks.x.top = element_blank())
```

@fig-tourism-data shows aggregate tourism flows for Australia and individual states, revealing pronounced seasonal patterns across both national and states levels, though patterns vary across series. Significant growth began around 2010 for the national flow and some states such as NSW, VIC, QLD, and WA, while flows for SA, TAS, and NT are relatively flat. Moreover, there is a notable decline in tourism flows for WA in 2016.

Our objective is to forecast tourism flows for each series in the geographic hierarchy while ensuring coherence across all levels. We use a rolling forecast origin to evaluate the forecast accuracy of different methods. We start with a training set of $216$ months for each series, and compute base forecasts from optimal ETS models. We then roll the forecast origin forward, month by month, until December 2016. The base forecasts are reconciled using our proposed methods and some existing reconciliation methods.

@tbl-tourism-rmse-avg reports average RMSE results. Similar to @sec-labour, MinT and the respective proposed methods are excluded due to poor performance, and the MCB test and error correlation calculations are omitted due to limited data. The results show that OLS outperforms other benchmarks, despite WLSv and MinTs accounting for in-sample covariance of base forecast errors, highlighting its effectiveness. Overall, the Subset methods outperform benchmarks, especially for aggregation levels. The only exception is OLS-subset, which slightly reduces overall accuracy while improving top-level forecasts. Parsimonious and Lasso methods produce results almost identical to their benchmarks, which is not surprising as ETS models rarely yield extremely poor forecasts, making them challenging to be selected out using methods that tend to return dense estimates. When we relax the unbiasedness assumption, EMinT consistently performs worst across all levels due to the lack of joint weak stationarity among the series in the hierarchy. Elasso, however, shows large improvement compared to EMinT, and ranks best across most levels except the bottom level.

```{r}
#| label: tbl-tourism-rmse-avg
#| results: asis
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for Australian domestic tourism data. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

rmse_tourism_avg <- readRDS(here::here("paper/results/tourism_rmse.rds"))
cols <- grepl("1--12", colnames(rmse_tourism_avg$table_out)) | grepl("Method", colnames(rmse_tourism_avg$table_out))
rmse_tourism_avg$table_out <- rmse_tourism_avg$table_out[, cols]
colnames(rmse_tourism_avg$table_out) <- c("Method", rmse_tourism_avg$levels)
latex_table_h(rmse_tourism_avg)
```

We present the results based on the last one training set from January 2017 to December 2017 in [Appendix E](#appendix-tourism). @tbl-tourism-rmse shows the RMSE results, and @fig-tourism-rmse displays the reconciliation errors across $111$ series. The results are consistent with the average results mentioned earlier, indicating relatively high-quality forecasts from Subset and Elasso methods. In addition, @tbl-tourism-info summarizes the number of series selected at each level, their average RMSSE values, and the optimal tuning parameters. Here we focus on the Subset and Elasso methods since they are useful in the tourism application. We observe that OLS-subset and WLSs-subset exclude some series at the State and Zone levels, yielding lower RMSSE values for the selected series at each level. Notably, WLSs-subset selects only the best-performing series at the State level. Despite using only $13$ time series, Elasso performs exceptionally well, with reduced RMSSE values at each level for the selected series. Thus, both the Subset and Elasso methods are able to exclude series with high forecast errors. In contrast, the WLSv and MinTs methods retain all series. Nonetheless, the inclusion of shrinkage through ridge regularization in the WLSv and MinTs methods still enhances the quality of reconciled forecasts.

## Discussion {#sec-discussion}

```{=tex}
\todo[inline]{
- list concerns in forecast reconciliation research, summarize these and address areas where the proposed methodology could support such concerns.

- emphasize the contributions of the current paper. (Grouped hierarchy. The selected series in our proposed method may come from different levels of the hierarchy.
It preserves the unbiasedness of forecasts, is able to restore the hierarchy.) Discuss the practical (OR) significance of the current paper. Unused series. Practical scenario in demand forecasting where intermittent series predominate at the most disaggregated level and their forecasts are worse.

- discuss the practical repercussions of this work as well as the potential limitations it faces. Probabilistic forecasting.
}
```
While our proposed methods opt to leave "poor" base forecasts unused in forming reconciled forecasts, the approach by @Zhang2023-op is primarily focused on keeping "good" base forecasts unchanged after reconciliation. Both methods frame the problem as a quadratic programming problem to enhance reconciled forecast performance by somehow managing the impact of some time series during the process, ultimately providing reconciled forecasts for all series in the hierarchy. However, our methods differ from the approach by @Zhang2023-op in four main ways. First, our proposed methods alter the poor-performing base forecasts, ensuring these do not influence other nodes, whereas the approach by @Zhang2023-op keeps the forecasts of certain nodes immutable, which then impact others. Second, our methods automate the selection of series unused during reconciliation, while the latter method requires an additional, yet-to-be-determined procedure to identify the immutable set of series. Third, we use only base forecasts of the selected series for reconciliation, while @Zhang2023-op use base forecasts of all series, even though some remain unchanged after reconciliation. Finally, beyond preserving the unbiasedness of forecasts, we also relax the unbiasedness assumption and propose an unconstrained "in-sample" reconciliation method.

Relaxing the assumption of unbiasedness of base forecasts, the Elasso method directly minimizes the mean squared reconciled forecast errors while incorporating regularization terms for time series selection. Unlike constrained "out-of-sample" reconciliation methods, Elasso does not use a $\bm{W}$ matrix in its optimization problem as $\bm{W}$ is introduced when deriving the loss function (Equation \eqref{eq-loss}) under the unbiasedness condition for both base and reconciled forecasts. Future research could explore including a $\bm{W}$ matrix in the Elasso method by considering the following optimization problem $$
\min_{\bm{G}} \quad \frac{1}{2 T} \operatorname{Tr}\left( \left(\bm{Y}-\hat{\bm{Y}} \bm{G}^{\prime} \bm{S}^{\prime}\right)\bm{W}^{-1}\left(\bm{Y}-\hat{\bm{Y}} \bm{G}^{\prime} \bm{S}^{\prime}\right)^{\prime}\right) + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$ where $\operatorname{Tr}(\cdot)$ denotes the trace of a matrix. In this context, $\bm{W}$ assigns different weights to reconciled forecast errors of various series in the hierarchical structure. Conversely, Elasso and other proposed methods currently treat reconciled forecast errors equally across different series. This suggests a potential new direction for research that differs from the problems addressed in this paper.

@Panagiotelis2021-mf highlighted the critical need for bias correction prior to forecast reconciliation when dealing with biased forecasts. Given that the unbiasedness preserving property is dropped in the Elasso method, a possible direction for future research could be to extend Elasso by including a bias correction mechanism. This can be achieved by formulating the optimization problem as follows: $$
\min_{\bm{G}} \quad \frac{1}{2 T} \left\|\bm{Y}-\left(\bm{D} + \hat{\bm{Y}} \bm{G}^{\prime}\right)\bm{S}^{\prime}\right\|_F^2 + \lambda \sum_{j=1}^n w_j \left\|\bm{G}_{\cdot j}\right\|_2,
$$ where $\bm{D} \in \mathbb{R}^{T \times n_b}$ is a shift parameter, with each row equal to an $n_b$-dimensional $\bm{d}$. This vector $\bm{d}$ can be trained alongside $\bm{G}$ and act as a bias correction.

Additionally, the concept of bi-level variable selection can be applied to enhance the methodology's capabilities. Such methodologies are well‑documented in the literature, including the sparse group lasso [@Simon2013-sp], hierarchical lasso [@Zhou2010-vs], and group bridge [@Huang2009-vs]. By treating each column of $\bm{G}$ as a group and its elements as individuals, we have introduced group‑wise sparsity in our methods through column-wise shrinkage towards zero. In this case, within-group sparsity can be achieved by simply including an additional lasso penalty to shrink individual elements, as suggested in @Simon2013-sp. This bi-level selection mechanism could potentially offer deeper insights into the contributions of individual base forecasts, particularly regarding their significance when mapped to bottom‑level disaggregated forecasts.

## Conclusion {#sec-conclusion}

\todo[inline]{Tighten up conclusions.}

In existing forecast reconciliation literature, we map all base forecasts into bottom-level disaggregated forecasts, which are then summed to yield coherent forecasts for the entire structure. The mapping step can be conceptually regarded as a forecast combination. It is common that the base forecasts for some time series perform poorly. This may reduce overall reconciliation effectiveness. In this paper, we have addressed this issue by introducing a selection mechanism to forecast reconciliation; i.e., incorporating time series selection when reconciling forecasts, while ensuring coherent forecasts for all series.

Under the unbiasedness assumption, we developed three reconciliation methods with selection mechanisms to automatically remove some base forecasts when forming reconciled forecasts. These methods include group best-subset selection with ridge regularization (Subset), an parsimonious method with $L_0$ regularization (Parsimonious), and a group lasso method (Lasso). These methods use different penalty functions designed to penalize the columns of the weighting matrix, $\bm{G}$, towards zero. Additionally, we relaxed the unbiasedness assumption and proposed the empirical group lasso method (Elasso) which selects series based on in-sample observations and fitted values.

Simulation experiments and two empirical applications demonstrated the superiority of the proposed methods over existing reconciliation methods without series selection. When model misspecification was introduced for some series in the hierarchy, our proposed methods ensured coherent forecasts that outperformed or, at least, matched their respective benchmarks in the minimum trace reconciliation framework. In both empirical applications, where no apparent model misspecification was present, Subset and Elasso methods were always preferred, especially for aggregation levels and longer forecast horizons, while Parsimonious and Lasso methods yielded results identical to corresponding benchmarks, as they tend to provide dense estimates.

A feature of the proposed methods is their ability to reduce the disparities arising from using different estimates of the base forecast error covariance matrix, thereby mitigating the challenges associated with estimator selection, which is a prominent issue within the field of forecast reconciliation research.

As the number of series grows, solving these problems efficiently becomes challenging, and the exact computation of these estimators remains a hurdle. In our study, we have used Gurobi, one of the most widely used commercial solvers, to address NP-hard MIP problems. Despite various efforts to develop MIP-based approaches for solving $L_0$-regularized regression problems, extending these methods to incorporate additional constraints remains a challenge. We leave this aspect to be addressed in future research.

## Supplementary materials {.unnumbered}

**Appendix:** Additional results obtained in @sec-simulations and @sec-applications. (.pdf file)

## Acknowledgments {.unnumbered}

Rob J Hyndman was supported by the Australian Research Council Industrial Transformation Training Centre in Optimisation Technologies, Integrated Methodologies, and Applications (OPTIMA), Project ID IC200100009.

## References {.unnumbered}

::: {#refs}
:::

```{=tex}
\newpage
\appendix
\pagenumbering{arabic}% resets `page` counter to 1
\setcounter{section}{0}
\renewcommand{\thesubsection}{\Alph{subsection}}
\renewcommand{\thetable}{\Alph{subsection}.\arabic{table}}
\renewcommand{\thefigure}{\Alph{subsection}.\arabic{figure}}
\numberwithin{figure}{subsection}
\numberwithin{table}{subsection}
\setcounter{figure}{0}
\setcounter{table}{0}
```

## Appendix {.unnumbered}

### Proofs of Propositions

#### Proof of Proposition 3.1 {#appendix-proofs}

::: proof
Let $\bm{X}_{\cdot \mathbb{S}} \in \mathbb{R}^{r \times |\mathbb{S}|}$ denote the submatrix of the $r \times c$ matrix $\bm{X}$ with the columns indexed by the set $\mathbb{S}$, where $|\mathbb{S}|$ is the cardinality of the set $\mathbb{S}$. Similarly, let $\bm{X}_{\mathbb{S}\cdot} \in \mathbb{R}^{|\mathbb{S}| \times c}$ denote the submatrix of $\bm{X}$ with the rows indexed by $\mathbb{S}$. If $\mathbb{S}$ is the set of indices of nonzero columns in the solution $\hat{\bm{G}}$ to Equation \eqref{eq-op_u}, then the following equations hold: $$
\hat{\bm{G}}\bm{S} = \hat{\bm{G}}_{\cdot \mathbb{S}}\bm{S}_{\mathbb{S}\cdot} = \bm{I}_{n_b},
\qquad\text{and}\qquad
\min \left(\operatorname{rank}(\hat{\bm{G}}_{\cdot \mathbb{S}}), \operatorname{rank}(\bm{S}_{\mathbb{S}\cdot})\right) \geq \operatorname{rank}(\bm{I}_{n_b})=n_b.
$$ This indicates that the number of nonzero columns of $\hat{\bm{G}}$ should be no less than $n_b$, i.e., $|\mathbb{S}| \geq n_b$.

Moreover, we have $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) = n_b$ because $\operatorname{rank}(\bm{S}_{\mathbb{S}\cdot}) \leq n_b$, given that $\bm{S}$ has $n_b$ columns. If the solution to Equation \eqref{eq-op_u} yields a $\hat{\bm{G}}$ with exactly $n_b$ nonzero columns (i.e., $|\mathbb{S}|=n_b$), then $\bm{S}_{\mathbb{S}\cdot}$ is a full rank square matrix and thus invertible. Applying Theorem 2 in @Zhang2023-op, $\bm{y}_{\mathbb{S}\cdot}$ is valid for constructing the full hierarchy using nothing but the information embedded in the aggregation constraints. If the solution yields a $\hat{\bm{G}}$ with more than $n_b$ nonzero columns, we should be able to identify more than one subset $\mathbb{S}^* \subset \mathbb{S}$ with $|\mathbb{S}^*|=n_b$ to construct an invertible square matrix $\bm{S}_{\mathbb{S}^{*}\cdot}$ and thereby restore the full hierarchy using the valid $\bm{y}_{\mathbb{S}^{*}\cdot}$. Therefore, the constraint $\bm{GS}=\bm{I}$ ensures that the selected columns of $\hat{\bm{G}}$ correspond to variables that can restore the full hierarchy.
:::

#### Proof of Proposition 3.2

::: proof
We have\vspace*{-0.4cm}\enlargethispage{0.4cm} $$
\begin{aligned}
& \bm{SG}\hat{\bm{y}} = \operatorname{vec}\left(\bm{SG}\hat{\bm{y}}\right) = \left(\hat{\bm{y}}^{\prime} \otimes \bm{S}\right) \operatorname{vec}(\bm{G}), \\
& \operatorname{vec}\left(\bm{G}\bm{S}\right) = \operatorname{vec}\left(\bm{I}_{n_b}\bm{G}\bm{S}\right) = \left(\bm{S}^{\prime} \otimes \bm{I}_{n_b}\right) \operatorname{vec}(\bm{G}).
\end{aligned}
$$ Substituting these into Equation \eqref{eq-op_u}, the previous problem now takes the form of a regression problem with an additional regularization term and an equality constraint on the coefficients, as shown in Equation \eqref{eq-op_u_reg}.
:::

#### Proof of Proposition 3.3

::: proof
Denote $\bm{\beta} = \operatorname{vec}(\bm{G})$, and the first term in the objective of Equation \eqref{eq-lasso} as $L\left(\bm{\beta} \mid \bm{D}\right)$, where $\bm{D}$ is the working data $\{\hat{\bm{y}} , \hat{\bm{y}}^{\prime} \otimes \bm{S}\}$. Ignoring the constraint $\bm{G_h S}=\bm{I_{n_b}}$, we define $\lambda^{1}$ as the smallest $\lambda$ value such that all predictors in the group lasso problem have zero coefficients, i.e., the solution at $\lambda^{1}$ is $\hat{\bm{\beta}}^{1}=\bm{0}$. (Note that there is no intercept in our problem.) Under the Karush-Kuhn-Tucker conditions, we have
$$
\lambda^{1}
 = \max_{j=1, \ldots, n}\big\|\big[\nabla L(\hat{\bm{\beta}}^{1} \mid \bm{D})\big]^{(j)}\big\|_2 / w_j
 = \max_{j=1, \ldots, n}\big\|-\big((\hat{\bm{y}}^{\prime} \otimes \bm{S})_{\cdot j^{*}}\big)^{\prime} \bm{W}^{-1} \hat{\bm{y}}\big\|_2 / w_j.
$$
:::

\clearpage

### Results from simulation Setup 1 {#appendix-sim1}

The section provides additional results obtained in @sec-sim1.

```{r}
#| label: tbl-sim-rmse-total
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for simulated data in Setup 1, with model misspecification in series Total. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

latex_table(sim_rmse$s3)
```

```{r}
#| label: fig-sim-plots-total
#| results: hide
#| fig-pos: "!h"
#| fig-width: 6
#| fig-height: 6
#| fig-cap: MCB test result and correlation matrix heatmap for simulated data in Setup 1, with model misspecification in series Total.
#| fig-subcap: 
#|   - "MCB test"
#|   - "Correlation between forecast errors"
#| layout: [[48, -4, 48]]
#| layout-valign: bottom

# MCB test
nemenyi(sim_mcb$s3, conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
        Title = "",
        Xlab = "Mean ranks",
        Ylab = "")

# Correlation heatmap
ggplot(data = sim_cormat$s3, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(data = sim_cormat$s3,
            mapping = aes(Var1, Var2,
                          label = round(value, digit = 2)),
            color = "black") +
  scale_fill_gradient2(low = "white", high = "#452363",
                       name="Correlation") +
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5)) +
  theme_heatmap
```

```{r}
#| label: tbl-sim-selection-total
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 1, with model misspecification in series Total. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(sim_selection$s3$z, sim_selection$s3$n, sim_rmsse$s3, "s3")
```

```{r}
#| label: tbl-sim-rmse-AA
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Performance of proposed (gray-shaded) and benchmark methods for simulated data in Setup 1, with model misspecification in series AA. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

latex_table(sim_rmse$s1)
```

```{r}
#| label: fig-sim-plots-AA
#| results: hide
#| fig-pos: "!h"
#| fig-width: 6
#| fig-height: 6
#| fig-cap: MCB test result and correlation matrix heatmap for simulated data in Setup 1, with model misspecification in series AA.
#| fig-subcap: 
#|   - "MCB test"
#|   - "Correlation between forecast errors"
#| layout: [[48, -4, 48]]
#| layout-valign: bottom

# MCB test
nemenyi(sim_mcb$s1, conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
        Title = "",
        Xlab = "Mean ranks",
        Ylab = "")

# Correlation heatmap
ggplot(data = sim_cormat$s1, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(data = sim_cormat$s1,
            mapping = aes(Var1, Var2,
                          label = round(value, digit = 2)),
            color = "black") +
  scale_fill_gradient2(low = "white", high = "#452363",
                       name="Correlation") +
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5)) +
  theme_heatmap
```

```{r}
#| label: tbl-sim-selection-AA
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 1, with model misspecification in series AA. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(sim_selection$s1$z, sim_selection$s1$n, sim_rmsse$s1, "s1")
```

\clearpage

### Results from simulation Setup 2 {#appendix-sim2}

The section provides additional results obtained in @sec-sim2.

```{r}
#| label: tbl-corr-selection-p3
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 2, with the error correlation being -0.4. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(corr_selection$p3$out_s0$z,
                    corr_selection$p3$out_s0$n,
                    corr_rmsse$p3$out_s0,
                    "corr_p3")
```

```{r}
#| label: tbl-corr-selection-p5
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 2, with the error correlation being 0. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(corr_selection$p5$out_s0$z,
                    corr_selection$p5$out_s0$n,
                    corr_rmsse$p5$out_s0,
                    "corr_p5")
```

```{r}
#| label: tbl-corr-selection-p7
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 2, with the error correlation being 0.4. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(corr_selection$p7$out_s0$z,
                    corr_selection$p7$out_s0$n,
                    corr_rmsse$p7$out_s0,
                    "corr_p7")
```

```{r}
#| label: tbl-corr-selection-p9
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Proportion of time series being selected using proposed reconciliation methods for simulated data in Setup 2, with the error correlation being 0.8. The numbers in parentheses show RMSSE values for each series. The last column displays a stacked barplot of the total selected series from 500 hierarchies, with darker sub-bars indicating higher counts.

latex_sim_nos_table(corr_selection$p9$out_s0$z,
                    corr_selection$p9$out_s0$n,
                    corr_rmsse$p9$out_s0,
                    "corr_p9")
```

```{r}
#| label: fig-corr-plots-others
#| results: hide
#| fig-width: 15
#| fig-height: 4
#| fig-cap: MCB test result and correlation matrix heatmap for simulated data in Setup 2, with the error correlation being -0.4, 0, 0.4, 0.8, respectively.
#| fig-subcap: 
#|   - "MCB test"
#|   - "Correlation between forecast errors"
#| layout-nrow: 2

# MCB test
par(mfrow=c(1, 4), mar=c(4,0,0.1,0.3))
ind <- c(3, 5, 7, 9); p <- c(-0.4, 0, 0.4, 0.8)
for (i in 1:4) {
  nemenyi(corr_mcb[[paste0("p", ind[i])]], conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
        Title = TeX(sprintf(r'($\rho = %f$)', p[i])),
        Xlab = "Mean ranks",
        Ylab = "")
}
par(mfrow=c(1, 1))

# Correlation heatmap
heatmap_list <- list() 
for (i in 1:4) {
  heatmap_list[[i]] <- ggplot(data = corr_cormat[[paste0("p", ind[i])]], aes(x = Var1, y = Var2, fill = value)) +
    geom_tile(color = "white") +
    geom_text(data = corr_cormat[[paste0("p", ind[i])]],
              mapping = aes(Var1, Var2,
                            label = round(value, digit = 2)),
              color = "black") +
    ggtitle(TeX(sprintf(r'($\rho = %f$)', p[i]))) +
    scale_fill_gradient2(low = "white", high = "#452363",
                         name="Correlation") +
    guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                 title.position = "top", title.hjust = 0.5)) +
    theme_heatmap
}
library(gridExtra)
grid.arrange(grobs = heatmap_list, ncol = 4)
```

\clearpage

### Results from Australian labour force data {#appendix-labour}

```{r}
#| label: tbl-labour-rmse
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Out-of-sample forecast results on a single test set (from August 2022 to July 2023) for Australian labour force data. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

rmse_labour <- readRDS(here::here("paper/results/labour_1_rmse.rds"))
cols <- grepl("1--12", colnames(rmse_labour$table_out)) | grepl("Method", colnames(rmse_labour$table_out))
rmse_labour$table_out <- rmse_labour$table_out[, cols]
colnames(rmse_labour$table_out) <- c("Method", rmse_labour$levels)
latex_table_h(rmse_labour)
```

```{r}
#| label: tbl-labour-info
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Number of time series selected by proposed methods and the optimal parameters identified in the labour application, considering a single test set (from August 2022 to July 2023). The None row shows the original number of series in the structure. The numbers in parentheses show RMSSE values for different levels.

labour_info <- readRDS(here::here("paper/results/labour_info.rds"))
labour_rmsse <- readRDS(here::here( "paper/results/labour_selected_rmsse.rds")) |>
  round(2) |> format(2)
labour_rmsse <- cbind(labour_rmsse, matrix("", 6, 4))
lb <- cbind(matrix("(", 6, 4), matrix("", 6, 4))
rb <- cbind(matrix(")", 6, 4), matrix("", 6, 4))

labour_tbl <- paste_matrices(labour_info, lb, sep = " ") |>
  paste_matrices(labour_rmsse, sep = "") |>
  paste_matrices(rb, sep = "")
labour_tbl <- sub("NA", "-", labour_tbl)
colnames(labour_tbl) <- colnames(labour_info)
rownames(labour_tbl) <- rownames(labour_info)
labour_tbl |>
  kable(format = "latex",
        booktabs = TRUE,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```

\clearpage

### Results from Australian domestic tourism data {#appendix-tourism}

```{r}
#| label: tbl-tourism-rmse
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Out-of-sample forecast results on a single test set (from January 2017 to December 2017) for Australian domestic tourism data. The Base row shows average RMSE of base forecasts, while entries below show RMSE percentage decrease (negative) or increase (positive) for reconciliation methods. Blue entries highlight the best-performing methods; bold entries indicate proposed methods outperforming bechmarks.

rmse_tourism <- readRDS(here::here("paper/results/tourism_1_rmse.rds"))
cols <- grepl("1--12", colnames(rmse_tourism$table_out)) | grepl("Method", colnames(rmse_tourism$table_out))
rmse_tourism$table_out <- rmse_tourism$table_out[, cols]
colnames(rmse_tourism$table_out) <- c("Method", rmse_tourism$levels)
latex_table_h(rmse_tourism)
```

```{r}
#| label: fig-tourism-rmse
#| results: hide
#| fig-width: 10
#| fig-height: 3
#| fig-pos: "!h"
#| fig-cap: Average out-of-sample forecasting performance, measured in terms of RMSE (from 1- to 12-step-ahead), for each series across different reconciliation methods. Time series are arranged along the horizontal axis.

tourism_heatmap <- readRDS(here::here("paper/results/tourism_heatmap.rds"))
ggplot(tourism_heatmap, aes(x = Series, y = Method, fill = RMSE)) +
  geom_tile() +
  geom_vline(xintercept = c(1.5, 8.5, 35.5), linetype = "dashed", linewidth = 0.5) +
  scale_fill_gradientn(colors = c("#f7d9a6",
                                  rev(hcl.colors(100, "Purples"))[c(seq(1, 50, 10), seq(51, 100, 1))])) +
  labs(x = "Time series", y = "") +
  scale_x_continuous(expand = c(0, 0),
                     breaks =  seq(20, 100, 20),
                     sec.axis = dup_axis(name = "",
                                         breaks = c(4.5, 11.5, 39),
                                         labels = c("States", "Zones", "Regions"))) +
  scale_y_discrete(expand = c(0, 0),
                   limits = rev(c("Base", "BU", "OLS", "OLS-subset",
                                  "WLSs", "WLSs-subset", "WLSv", "WLSv-subset",
                                  "MinTs", "MinTs-subset", "EMinT", "Elasso"))) +
  theme(
    plot.background = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, linewidth=1),
    axis.title.x = element_text(face = "bold", size = 12),
    legend.title = element_text(face = "bold", size = 10),
    legend.text = element_text(face = "bold", size = 10),
    legend.position = "right",
    axis.text = element_text(face = "bold", size = 10),
    axis.ticks.x.top = element_blank()
  ) +
  guides(fill = guide_colourbar(barwidth = 1.5,
                                barheight = 10))
```

```{r}
#| label: tbl-tourism-info
#| results: asis
#| tbl-pos: "!h"
#| tbl-cap: Number of time series selected by proposed methods and the optimal parameters identified in the tourism application, considering a single test set (from January 2017 to December 2017). The None row shows the original number of series in the structure. The numbers in parentheses show RMSSE values for different levels.

tourism_info <- readRDS(here::here("paper/results/tourism_info.rds"))
tourism_rmsse <- readRDS(here::here( "paper/results/tourism_selected_rmsse.rds")) |>
  round(2) |> format(2)
tourism_rmsse <- cbind(tourism_rmsse, matrix("", 6, 4))
lb <- cbind(matrix("(", 6, 4), matrix("", 6, 4))
rb <- cbind(matrix(")", 6, 4), matrix("", 6, 4))

tourism_tbl <- paste_matrices(tourism_info, lb, sep = " ") |>
  paste_matrices(tourism_rmsse, sep = "") |>
  paste_matrices(rb, sep = "")
tourism_tbl <- sub("NA", "-", tourism_tbl)
colnames(tourism_tbl) <- colnames(tourism_info)
rownames(tourism_tbl) <- rownames(tourism_info)
tourism_tbl |>
  kable(format = "latex",
        booktabs = TRUE,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("repeat_header"), font_size = 10) |>
  add_header_above(c("", "Number of time series retained" = 5,
                     "Optimal parameters" = 3),
                   align = "c")
```
