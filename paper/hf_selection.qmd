---
title: "Optimal forecast reconciliation with time series selection"
author:
- familyname: Wang
  othernames: Xiaoqian
  address:
    - Monash University, VIC 3800, Australia
  email: xiaoqian.wang@monash.edu
  correspondingauthor: true
- familyname: Hyndman
  othernames: Rob J
  address:
    - Monash University, VIC 3800, Australia
  email: rob.hyndman@monash.edu
- familyname: Wickramasuriya
  othernames: Shanika L
  address:
    - Monash University, VIC 3145, Australia
  email: shanika.wickramasuriya@monash.edu
abstract: "Abstract"
keywords: "Keyword 1, Keyword 2"
wpnumber: no/yr
blind: false
cover: true
toc: false
number-sections: true
fig-height: 5
fig-width: 8
cite-method: biblatex
bibliography: references.bib
biblio-style: authoryear-comp
keep-tex: true
format:
  wp-pdf:
    knitr:
      opts_chunk:
        dev: "CairoPDF"
execute:
  echo: false
  warning: false
  message: false
  cache: true
---

```{r}
#| label: load
#| cache: false
# Load all required packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(latex2exp)

theme_set(theme_get() + theme(text = element_text(family = "Source Sans Pro")))
```

# Introduction {#sec-introduction}

Hierarchical time series and forecast reconciliation. Post-processing.

Single-level approaches, least squares-based reconciliation approaches, geometric intuition, other extensions with constraints.

However... Two issues. The choice of W can have significant effect on the quality of the reconciled forecasts. Some time series perform poorly.

In this paper, our focus will be on...

The remainder of the paper is structured as follows.

# Preliminaries {#sec-preliminaries}

## Notation

We denote the set $\{1,\ldots,k\}$ by $[k]$ for any non-negative integer $k$. A *hierarchical time series* can be considered as an $n$-dimensional multivariate time series, $\{\boldsymbol{y}_t, t \in [T]\}$, that adheres to known linear constraints. Let $\boldsymbol{y}_t \in \mathbb{R}^n$ be a vector comprising observations of all time series in the hierarchy at time $t$, and $\boldsymbol{b}_t \in \mathbb{R}^{n_b}$ be a vector comprising observations of all bottom-level time series at time $t$. The full hierarchy at time $t$ can be written as

$$
\boldsymbol{y}_t = \boldsymbol{S}\boldsymbol{b}_t,
$$

where $\boldsymbol{S}$ is an $n \times n_b$ *summing matrix* that shows aggregation constraints present in the structure. We can write the summing matrix as $\boldsymbol{S} = \left[\begin{array}{c}\boldsymbol{A} \\ \boldsymbol{I}_{n_b}\end{array}\right]$, where $\boldsymbol{A}$ is an $n_a \times n_b$ *aggregation matrix* with $n = n_a + n_b$, and $\boldsymbol{I}_{n_b}$ is an $n_b$-dimensional identity matrix.

![An example of a two-level hierarchical time series.](figs/hts_example.pdf){#fig-hts fig-align="center" width="50%"}

To clarify these notations, consider the example of the hierarchy in @fig-hts. For this two-level hierarchy, $n = 7$, $n_b = 4$, $n_a = 3$, $\boldsymbol{y}_t = [y_{\text{Total},t}, y_{\text{A},t}, y_{\text{B},t}, y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, $\boldsymbol{b}_t = [y_{\text{AA},t}, y_{\text{AB},t}, y_{\text{BA},t}, y_{\text{BB},t}]^{\prime}$, and

$$
\boldsymbol{S} = \left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I}_4}
\end{array}\right].
$$

![An example of a two level grouped time series.](figs/gts_example.pdf){#fig-gts fig-align="center" width="100%"}

When data structure does not naturally disaggregate in a unique hierarchical manner, we can combine these hierarchical structures to form a *grouped time series*. Thus, grouped time series can also be considered as hierarchical time series with more than one grouping structure. @fig-gts shows an example of a two level grouped time series with two alternative aggregation structures. For this example, $n = 9$, $n_b = 4$, $n_a = 5$, $\boldsymbol{y}_t = [y_{\text{Total},t}, y_{\text{A},t}, y_{\text{B},t}, y_{\text{X},t}, y_{\text{Y},t}, y_{\text{AX},t}, y_{\text{AY},t}, y_{\text{BX},t}, y_{\text{BY},t}]^{\prime}$, $\boldsymbol{b}_t = [y_{\text{AX},t}, y_{\text{AY},t}, y_{\text{BX},t}, y_{\text{BY},t}]^{\prime}$, and

$$
\boldsymbol{S} = \left[
\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 \\
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 \\
\multicolumn{4}{c}{\boldsymbol{I}_4}
\end{array}\right].
$$

## Linear forecast reconciliation

Let $\hat{\boldsymbol{y}}_{T+h \mid T} \in \mathbb{R}^n$ be a vector of $h$-step-ahead *base forecasts* for all time series in the hierarchy, given observations up to time $T$, and stacked in the same order as $\boldsymbol{y}_t$. We can use any method to generate these forecasts, but In general they will not add up especially when we forecast each series independently.

When forecasting hierarchical time series, we expect the forecasts to be *coherent* (i.e., aggregation constraints are satisfied). Let $\tilde{\boldsymbol{y}}_{T+h \mid T} \in \mathbb{R}^n$ denote a vector of $h$-step-ahead *reconciled forecasts* which are coherent by construction, $\psi$ a mapping that reconciles base forecasts, $\hat{\boldsymbol{y}}_{T+h \mid T}$. Then we have *forecast reconciliation* $\tilde{\boldsymbol{y}}_{T+h \mid T}=\psi(\hat{\boldsymbol{y}}_{T+h \mid T})$, which is essentially a post-processing method. In this paper, we focus on linear forecast reconciliation given by

$$
\tilde{\boldsymbol{y}}_{T+h \mid T} = \boldsymbol{S}\boldsymbol{G}_h\hat{\boldsymbol{y}}_{T+h \mid T},
$$

where

-   $\boldsymbol{G}_h$ is an $n_b \times n$ weighting matrix that maps the base forecasts into the bottom level. In other words, it combines all base forecasts to form reconciled forecasts for bottom-level series.
-   $\boldsymbol{S}$ is an $n \times n_b$ summing matrix that sums up bottom-level reconciled forecasts to produce coherent forecasts of all levels. It identifies the linear constraints involved in the hierarchy.

### Minimum trace reconciliation

Let the $h$-step-ahead *base forecast errors* be defined as $\hat{\boldsymbol{e}}_{T+h \mid T} = \boldsymbol{y}_{T+h} - \hat{\boldsymbol{y}}_{T+h \mid T}$, and the $h$-step-ahead *reconciled forecast errors* be defined as $\tilde{\boldsymbol{e}}_{T+h \mid T} = \boldsymbol{y}_{T+h} - \tilde{\boldsymbol{y}}_{T+h \mid T}$. @Wickramasuriya2019-fc formulated a linear reconciliation problem as minimizing the trace (MinT) of the $h$-step-ahead covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\boldsymbol{e}}_{T+h \mid T})$. Under the assumption of unbiasedness, the unique solution of the minimization problem is given by

$$
\boldsymbol{G}_h=\left(\boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1} \boldsymbol{S}\right)^{-1} \boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1},
$$ {#eq-mint} where $\boldsymbol{W}_h$ is the positive definite covariance matrix of the $h$-step-ahead base forecast errors, $\operatorname{Var}(\hat{\boldsymbol{e}}_{T+h \mid T})$.

The trace minimization problem can be reformulated as a least squares problem with linear constraints given by

$$
\begin{aligned}
& \min _{\tilde{\boldsymbol{y}}_{T+h \mid T}} \quad \frac{1}{2}(\hat{\boldsymbol{y}}_{T+h \mid T}-\tilde{\boldsymbol{y}}_{T+h \mid T})^{\prime} \boldsymbol{W}_{h}^{-1}(\hat{\boldsymbol{y}}_{T+h \mid T}-\tilde{\boldsymbol{y}}_{T+h \mid T}) \\
& \text { s.t. } \quad \tilde{\boldsymbol{y}}_{T+h \mid T}=\boldsymbol{S}\tilde{\boldsymbol{b}}_{T+h \mid T},
\end{aligned}
$$ {#eq-mint_op}

where $\tilde{\boldsymbol{b}}_{T+h \mid T} \in \mathbb{R}^{n_b}$ is the vector comprising $h$-step-ahead bottom-level reconciled forecasts, made at time $T$. Focusing on $\boldsymbol{W}_h$, the intuitive behind the MinT reconciliation is that the larger the estimated variance of the base forecast errors, the larger the range of adjustments permitted for forecast reconciliation.

It's challenging to estimate $\boldsymbol{W}_h$, especially for $h > 1$. Assuming that $\boldsymbol{W}_h = k_h\boldsymbol{W}_1$, $\forall h$, where $k_h > 0$, the MinT solution of $\boldsymbol{G}$ does not change with the forecast horizon. Hence, we will drop the subscript $h$ for the ease of exposition. The most popularly used candidate estimators for $\boldsymbol{W}$ in the forecast reconciliation literature are listed as follows.

1.  $\boldsymbol{W}_{\text{OLS}} = \boldsymbol{I}$ is the *OLS estimator* proposed by @Hyndman2011-sd, assuming that the base forecast errors are uncorrelated and equivariant. In what follows, we denote this as **OLS**.
2.  $\boldsymbol{W}_{\text{WLSs}} = \operatorname{diag}(\boldsymbol{S} \mathbf{1})$ is the *WLS estimator applying structural scaling* proposed by @Athanasopoulos2017-jj. This estimator depends only on the aggregation structure of the hierarchy. It assumes that the variance of each bottom-level base forecast error is equivalent and uncorrelated between nodes. We denote this method as **WLSs**.
3.  $\boldsymbol{W}_{\text{WLSv}} = \operatorname{diag}(\hat{\boldsymbol{W}}_1)$ is the *WLS estimator applying variance scaling* proposed by @Hyndman2016-cz, where $\hat{\boldsymbol{W}}_1$ denotes the unbiased covariance estimator based on the in-sample one-step-ahead base forecast errors (i.e., residuals). In the results that follow, we denote this as **WLSv**.
4.  $\boldsymbol{W}_{\text{MinT}} = \hat{\boldsymbol{W}}_1$ is referred to as the *MinT estimator* based on the sample covariance matrix proposed by @Wickramasuriya2019-fc. We denote this method as **MinT** in the results that follow.
5.  $\boldsymbol{W}_{\text{MinTs}} = \lambda\operatorname{diag}(\hat{\boldsymbol{W}}_1) + (1-\lambda)\hat{\boldsymbol{W}}_1$ is the *MinT shrinkage estimator* suggested by @Wickramasuriya2019-fc, in which off-diagonal elements of $\hat{\boldsymbol{W}}_1$ are shrunk toward zero. We refer to this method as **MinTs**.

It's hard to say which estimator for $\boldsymbol{W}$ works better. @Pritularga2021-lz demonstrated that the performance of forecast reconciliation is affected by two sources of uncertainties, i.e., the base forecast uncertainty and the reconciliation weight uncertainty. Recall that the uncertainty in the MinT solution in @eq-mint is introduced by the uncertainty in the reconciliation weighting matrix as the summing matrix is fixed for a certain hierarchy. This indicates that OLS and WLSs estimators for $\boldsymbol{W}$ may lead to less volatile reconcliation performance compared to WLSv, MinT, and MinTs estimators. @Panagiotelis2021-mf provided a geometric intuition for reconciliation and showed that, when considering the Euclidean distance loss function, OLS reconciliation yields results that are at least as favorable as the base forecasts, whereas MinT reconciliation performs poorly relative to the base forecasts. However, when considering the mean squared reconciled forecast error, @Wickramasuriya2021-am indicated that MinT reconciliation is better than OLS reconciliation. Therefore, which estimator for $\boldsymbol{W}$ to use hinges on the specific hierarchical time series of interest, the targeted level or series, and the selected loss function.

### Relaxation of the unbiasedness assumptions

Both @Hyndman2011-sd and @Wickramasuriya2019-fc impose two unbiasedness conditions, i.e., the base forecasts and the reconciled forecasts are unbiased. @Ben_Taieb2019-be proposed a reconciliation method relaxing the assumption of unbiasedness. Specifically, by expanding the training window forward by one observation until $T-h$, they formulated the reconciliation problem as a regularized empirical risk minimization (RERM) problem given by

$$
\min _{\boldsymbol{G}_h} \frac{1}{(T-T_1-h+1)n}\left\|\boldsymbol{Y}_{h}^{*}-\hat{\boldsymbol{Y}}_{h}^{*} \mathbf{G}_{h}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2+\lambda\|\operatorname{vec}( \boldsymbol{G}_h)\|_1,
$$

where $T_1$ denotes the minimum number of observations used for model training, $\left\| \cdot \right\|_F$ is the Frobenius norm, $\boldsymbol{Y}_{h}^{*}=\left[\boldsymbol{y}_{T_1+h}, \ldots, \boldsymbol{y}_T\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$, $\hat{\boldsymbol{Y}}_{h}^{*}=\left[\hat{\boldsymbol{y}}_{T_1+h \mid T_1}, \ldots, \hat{\boldsymbol{y}}_{T \mid T-h}\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$, and $\lambda \geq 0$ is a regularization parameter.

When $\lambda = 0$, the problem reduces to an empirical risk minimization (ERM) problem without regularization. Assuming that the series in the hierarchy are jointly weakly stationary and $\hat{\boldsymbol{Y}}_{h}^{*\prime}\hat{\boldsymbol{Y}}_{h}^{*}$ is invertible, it has a closed-form solution given by

$$
\hat{\boldsymbol{G}}_h = \boldsymbol{B}_{h}^{*\prime}\hat{\boldsymbol{Y}}_{h}^{*}\left(\hat{\boldsymbol{Y}}_{h}^{*\prime}\hat{\boldsymbol{Y}}_{h}^{*}\right)^{-1},
$$

where $\boldsymbol{B}_{h}^{*}=\left[\boldsymbol{b}_{T_1+h}, \ldots, \boldsymbol{b}_T\right]^{\prime} \in \mathbb{R}^{\left(T-T_1-h+1\right) \times n}$. If $\hat{\boldsymbol{Y}}_{h}^{*\prime}\hat{\boldsymbol{Y}}_{h}^{*}$ is not invertible, they suggested using a generalized inverse.

When $\lambda > 0$, imposing such a $L_1$ penalty on $\boldsymbol{G}_h$ will introduce sparsity and reduce estimation variance, albeit at the cost of introducing some bias. In addition, they also proposed another strategy that penalizes the matrix $\boldsymbol{G}_h$ towards the solution obtained by bottom-up method, i.e., $\boldsymbol{G}_{\text{BU}} = \left[\boldsymbol{0}_{n_b \times n_a} \mid \boldsymbol{I}_{n_b}\right]$.

Following the work, @Wickramasuriya2021-am proposed an empirical MinT (EMinT) without the unbiasedness constraint by minimizing the trace of the covariance matrix of the reconciled forecast errors, $\operatorname{Var}(\tilde{\boldsymbol{e}}_{T+h \mid T})$. Assuming that the series are jointly weakly stationary, she derived the solution given by

$$
\hat{\boldsymbol{G}}_{h} = \boldsymbol{B}_{h}^{\prime}\hat{\boldsymbol{Y}}_{h}\left(\hat{\boldsymbol{Y}}_{h}^{\prime}\hat{\boldsymbol{Y}}_{h}\right)^{-1},
$$

where $\boldsymbol{B}_{h}=\left[\boldsymbol{b}_{h}, \ldots, \boldsymbol{b}_T\right]^{\prime} \in \mathbb{R}^{\left(T-h+1\right) \times n}$, and $\hat{\boldsymbol{Y}}_{h}=\left[\hat{\boldsymbol{y}}_{h \mid 0}, \ldots, \hat{\boldsymbol{y}}_{T \mid T-h}\right]^{\prime} \in \mathbb{R}^{\left(T-h+1\right) \times n}$. The difference between EMinT and ERM lies in the data sources used, as EMinT uses in-sample observations and base forecasts, while ERM relies on observations and base forecasts from a holdout validation set. We note that both ERM and EMinT consider an estimate of $\boldsymbol{G}$ that changes over the forecast horizon, which is why we keep the subscript $h$ here.

In practice, a prevalent challenge in forecast reconciliation arises when the base forecasts of some time series within the hierarchical structure may perform poorly, especially for large hierarchies. This can be attributed to either the inherent complexity of forecasting these series or potential model misspecification. In such cases, the effectiveness of forecast reconciliation may diminish, as the role of the weighting matrix $\boldsymbol{G}$ is to assimilate *all* base forecasts and map them into bottom-level disaggregated forecasts which are subsequently summed by $\boldsymbol{S}$. While the RERM method proposed by @Ben_Taieb2019-be introduces sparsity by shrinking some elements of $\boldsymbol{G}$ towards zero, it remains incapable of mitigating the adverse impact of underperforming base forecasts on the quality of the reconciled forecasts. Moreover, the method is time-consuming because it uses expanding windows to recursively generate out-of-sample base forecasts, which are then used in the minimization problem.

We therefore propose two branches of innovative methods, constrained (out-of-sample-based) and unconstrained (in-sample-based) reconciliation with selection. These methods aim to identify and address the negative effect of some base forecasts of poor performance in a hierarchy on the overall performance of the reconciled forecasts. Additionally, through the incorporation of regularization in our objective function, our method has the potential to enhance reconciliation outcomes produced by using a "bad" choice of $\boldsymbol{W}$, thus reducing the risk of choosing estimator of $\boldsymbol{W}$. Moreover, our method generalizes to grouped hierarchies.

# Forecast reconciliation with time series selection {#sec-methodology}

In this section, we introduce our methods for keeping forecasts of an automatically selected set of series, identified as harmful to reconciliation, unused in forming reconciled forecasts, i.e., forecast reconciliation with series selection. @sec-constrained introduces constrained reconciliation methods with selection that formulate the problem based on out-of-sample base forecasts, while @sec-unconstrained presents an unconstrained reconciliation method with selection that formulates the problem based on in-sample observations and base forecasts.

## Series selection with unbiasedness constraint {#sec-constrained}

As $\boldsymbol{S}$ is fixed and $\hat{\boldsymbol{y}}_{T+h \mid T}$ is given, the estimation of $\boldsymbol{G}$ carries the linear reconciliation performance, as shown in @eq-mint. (Subscript $h$ is dropped as we assume $\boldsymbol{W}$ and $\boldsymbol{G}$ do not change over the forecast horizon.) A natural way to keep forecasts of some series unused in reconciliation is through controlling the number of nonzero column entries in $\boldsymbol{G}$. This leads to a generalization of the MinT minimization problem by applying an additional penalty to the objective function. More precisely, we consider the optimization problem given by

$$
\begin{aligned}
& \min _{\boldsymbol{G}} \quad \frac{1}{2}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\boldsymbol{SG}\hat{\boldsymbol{y}}_{T+h \mid T}\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\boldsymbol{SG}\hat{\boldsymbol{y}}_{T+h \mid T}\right)
+ \lambda\mathfrak{g}(\boldsymbol{G}) \\
& \text { s.t. } \quad \boldsymbol{GS}=\boldsymbol{I},
\end{aligned}
$$ {#eq-op_u}

where $\mathfrak{g}(\cdot)$ is defined as an exterior penalty function designed to penalize the columns of $\boldsymbol{G}$ towards zero, with $\lambda$ is the corresponding penalty coefficient. Thus, this can be considered as a grouped variable selection problem, with each group corresponding to a column of $\boldsymbol{G}$. The constraint, $\boldsymbol{GS}=\boldsymbol{I}$, reflects the assumption that the base forecasts and reconciled forecasts are unbiased. When $\lambda = 0$, $\forall h$, the problem reduces to the MinT optimization problem in @eq-mint_op with a closed-form solution given by @eq-mint.

**Proposition 1.** *Under the assumption of unbiasedness, the count of nonzero column entries of* $\boldsymbol{G}$ (*i.e., the number of time series selected for reconciliation*), *derived through solving @eq-op_u, is at least equal to the number of time series at the bottom level. In addition, we can restore the full hierarchical structure by aggregating/disaggregating the selected time series.*

*Proof*. According to the unbiasedness constraint $\boldsymbol{GS}=\boldsymbol{I}$, we have

$$
\min \left(\operatorname{rank}(\boldsymbol{G}), \operatorname{rank}(\boldsymbol{S})\right) \geq \operatorname{rank}(\boldsymbol{I}_{n_b})=n_b,
$$

which indicates that the count of nonzero column entries of $\boldsymbol{G}$ is at least equal to $n_b$.

Let $\boldsymbol{X}_{\cdot \mathbb{S}} \in \mathbb{R}^{r \times |\mathbb{S}|}$ denote the submatrix of the $r \times c$ matrix $\boldsymbol{X}$ with column indices forming a set $\mathbb{S}$ (and when $\mathbb{S} = \{j\}$, we simply use $\boldsymbol{X}_{\cdot j}$). Here, $|\mathbb{S}|$ denotes the size of the set $\mathbb{S}$. Similarly, let $\boldsymbol{X}_{\mathbb{S}\cdot} \in \mathbb{R}^{|\mathbb{S}| \times c}$ denote the submatrix of $\boldsymbol{X}$ whose rows are indexed by a set $\mathbb{S}$ (and when $\mathbb{S} = \{i\}$, we simply use $\boldsymbol{X}_{i\cdot}$). Assuming that the set $\mathbb{S}$ involves the indices of nonzero columns in the solution of @eq-op_u, the following equations hold:

$$
\begin{aligned}
& \boldsymbol{G}\boldsymbol{S} = \boldsymbol{G}_{\cdot \mathbb{S}}\boldsymbol{S}_{\mathbb{S}\cdot} \text{ and } \\
& \min \left(\operatorname{rank}(\boldsymbol{G}_{\cdot \mathbb{S}}), \operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot})\right) \geq \operatorname{rank}(\boldsymbol{I}_{n_b})=n_b.
\end{aligned}
$$

Additionally, we have $\operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot}) \leq n_b$ as $\boldsymbol{S}$ has $n_b$ columns. Therefore, we can conclude that $\operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot}) = n_b$, which implies that the hierarchical structure can be fully restored by aggregating/disaggregating the selected time series, $(\boldsymbol{y}_{t})_{\mathbb{S}}$.

For example, consider the simple hierarchy shown in @fig-hts, it is not possible for our constrained reconciliation methods with selection to simultaneously zero out columns of $\boldsymbol{G}$ associated with series AA and AB. However, it is possible to zero out columns related to series AA and BA simultaneously.

**Proposition 2.** *The optimization problem in @eq-op_u can be reformulated as*

$$
\begin{aligned}
& \min _{\boldsymbol{G}} \quad \frac{1}{2}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) + \lambda\mathfrak{g}\left(\operatorname{vec}(\boldsymbol{G})\right) \\
& \text { s.t. } \quad \left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}(\boldsymbol{I}_{nb}),
\end{aligned}
$$ {#eq-op_u_reg}

*Proof.* Let $\operatorname{vec}(\boldsymbol{A})$ denote the vectorization of a matrix $\boldsymbol{A}$, which stacks the columns of $\boldsymbol{A}$ on top of one another. We have

$$
\begin{aligned}
& \operatorname{vec}\left(\hat{\boldsymbol{y}}_{T+h \mid T}\right) = \hat{\boldsymbol{y}}_{T+h \mid T}, \\
& \operatorname{vec}\left(\boldsymbol{SG}\hat{\boldsymbol{y}}_{T+h \mid T}\right) = \left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G}), \\
& \operatorname{vec}\left(\boldsymbol{GS}\right) = \operatorname{vec}\left(\boldsymbol{I}_{nb}\boldsymbol{GS}\right) = \left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G}).
\end{aligned}
$$ Substituting the terms in @eq-op_u with these expressions, the previous problem now takes the form of a regression problem with an additional regularization term and an equality constraint on the coefficients, as shown in @eq-op_u_reg.

Moving forward, we present three classes of regularizations we use to establish forecast reconciliation with series selection, resulting in the consideration of three optimization problems: (i) group best-subset selection with ridge regularization, (ii) intuitive method with $L_0$ regularization, and (iii) group lasso method.

### Group best-subset selection with ridge regularization

We propose to apply a combination of $L_0$ and $L_2$ regularization as the exterior penalty function to control the nonzero column entries in $\boldsymbol{G}$:

$$
\begin{aligned}
\min _{\operatorname{vec}(\boldsymbol{G})} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \\
& + \lambda_0 \sum_{j=1}^n \mathbf{1}\left(\boldsymbol{G}_{\cdot j} \neq \mathbf{0}\right) + \lambda_2 \left\|\operatorname{vec}\left(\boldsymbol{G}\right)\right\|_2^2 \\
\text { s.t. } \quad & \left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}(\boldsymbol{I}_{nb}),
\end{aligned}
$$ {#eq-subset}

where $\mathbf{1}(\cdot)$ is the indicator function, $\lambda_0 \geq 0$ controls the number of nonzero groups selected (each group corresponds to a column of $\boldsymbol{G}$), and $\lambda_2 \geq 0$ controls the strength of the ridge regularization. In a hierarchical time series context, the parameter of interest in @eq-subset, $\operatorname{vec}(\boldsymbol{G})$, has an inherent non-overlapping group structure, wherein each group corresponds to a single column of $\boldsymbol{G}$, each with a size of $n_b$. Therefore, we refer to this reconciliation method as *group best-subset selection with ridge regularization*. In the results that follow, we label the method differently based on various estimators for $\boldsymbol{W}$, referring to them as **OLS-subset**, **WLSs-subset**, **WLSv-subset**, **MinT-subset**, and **MinTs-subset**, respectively.

The inclusion of the ridge term in @eq-subset is motivated by earlier work on best-subset selection [@Hazimeh2020-xd; @Mazumder2022-hx], which suggest that additional ridge regularization can mitigate the poor predictive performance of best-subset selection in the low signal-to-noise ratio (SNR) regimes.

We present a Big-M based mixed integer programming (MIP) formulation for problem in @eq-subset given by

$$
\begin{aligned}
\min _{\operatorname{vec}(\boldsymbol{G}), \boldsymbol{z}, \check{\boldsymbol{e}}, \boldsymbol{g}^{+}} & \frac{1}{2}\check{\boldsymbol{e}}^{\prime} \boldsymbol{W}^{-1}\check{\boldsymbol{e}} + \lambda_0 \sum_{j=1}^n z_j + \lambda_2 \boldsymbol{g}^{+\prime}\boldsymbol{g}^{+} \\
\text { s.t. } \quad & \left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}\left(\boldsymbol{I}_{n_b}\right) \\
& \hat{\boldsymbol{y}}_{T+h \mid T}-\left(\hat{\boldsymbol{y}}_{T+h \mid T}^{\prime} \otimes \boldsymbol{S}\right)\operatorname{vec}(\boldsymbol{G}) = \check{\boldsymbol{e}} \\
& \sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M} z_j, \quad j \in[n] \\
& \boldsymbol{g}^{+} \geqslant \operatorname{vec}(\boldsymbol{G}) \\
& \boldsymbol{g}^{+} \geqslant-\operatorname{vec}(\boldsymbol{G}) \\
& z_j \in\{0,1\}, \quad j \in[n]
\end{aligned}
$$ {#eq-subset_mip}

where $\mathcal{M}$ is a Big-M parameter (a-priori specified) that is sufficiently large such that some optimal solution, say $\boldsymbol{g}^{+*}$, to @eq-subset_mip satisfies $\max _{j \in [n]}\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$, the binary variable $z_j$ controls whether all the regression coefficients $\operatorname{vec}(\boldsymbol{G})$ in group $j$ are zero or not, i.e., $z_j=0$ implies that $\boldsymbol{G}_{\cdot j}=\mathbf{0}$, and $z_j=1$ implies that $\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. Such Big-M formulations are commonly used in MIP problems to model relations between discrete and continuous variables, and have been recently explored in regression with $L_0$ regularization. The problem is a mixed integer quadratic program (MIQP) that can be solved using commercial MIP solvers, e.g., Gurobi and CPLEX.

**Parameter tuning.** $\lambda_0 \geq 0$ and $\lambda_2 \geq 0$ are tuning parameters. To avoid computationally-expensive cross-validation, we tune the parameters to minimize the sum of squared reconciled forecast errors on the training set. Let $\lambda_{0,1} = \frac{1}{2}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\tilde{\boldsymbol{y}}_{T+h \mid T}^{\text{bench}}\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}_{T+h \mid T}-\tilde{\boldsymbol{y}}_{T+h \mid T}^{\text{bench}}\right)$ that captures the scale of first term in the objective, where $\tilde{\boldsymbol{y}}_{T+h \mid T}^{\text{bench}}$ is a vector of reconciled forecasts obtained using @eq-mint with same estimator of $\boldsymbol{W}$, and define $\lambda_{0,k} = 0.0001\lambda_{0,1}$. For the parameter $\lambda_0$, we consider a grid of $k+1$ values, $\{\lambda_{0,1},...,\lambda_{0,k}, 0\}$, where $\lambda_{0, j} = \lambda_{0,1}\left(\lambda_{0,k} / \lambda_{0,1}\right)^{(j-1) / (k-1)}$ for $j \in [k]$. So $\lambda_{0,1},...,\lambda_{0,k}$ is a sequence decreasing from $\lambda_{0,1}$ to $\lambda_{0,k}$ on the log scale. We use a grid of six values for the parameter $\lambda_2$, i.e., $\{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$. Therefore, we tune over a two-dimensional grid of $(k+1) \times 6$ values to find the optimal combination of $\lambda_0$ and $\lambda_2$.

**Computation details.** The MIQP problem in @eq-subset_mip is NP-Hard and computationally intensive. @Bertsimas2016-ig showed that commercial MIP solvers are capable of tackling problem instances for $p$ (number of features, i.e., $p = n_b \times n$ in our problem) up to a thousand. To address larger instances, there has been impressive work on developing MIP-based approches for solving $L_0$-regularized regression problem, e.g., @Bertsimas2016-ig, @Hazimeh2020-xd, and @Hazimeh2022-hc. However, it is challenging to extend their approaches to accommodate additional constraints within the optimization problem. Despite the potential sluggishness of handling large instances with commercial MIP solvers, in our experiments, we use Gurobi to solve our problem in @eq-subset_mip by configuring parameters such as MIPGap = $0.001$ and TimeLimit = $600$ seconds. So we can stop the solver before reaching the global optimum and obtain a suboptimal solution. This strategy is motivated by our need to consider numerous parameter candidates, with the final solution being validated against the training set, thus preventing the utilization of bad estimates of $\boldsymbol{G}$.

### Intuitive method with $L_0$ regularization

optimization problems considered

MIP

disadvantage?

**Parameter tuning.**

**Computation details.**

### Group lasso method

-   Optimization problems considered

-   Mixed Integer Programming/MIP Formulations

-   HyperParameters

-   Remark. limitations, computational issues

## Series selection method without unbiasedness constraint {#sec-unconstrained}

# Monte Carlo simulations {#sec-simulations}

## Model misspecification in a hierarchy

## Exploring the effect of correlation

# Applications {#sec-applications}

## Forecasting Australian domestic tourism

## Forecasting Australian labour force

# Conclusion {#sec-conclusion}

\textbf{\large{Acknowledgement}}
