\documentclass[11pt,a4paper,]{article}
\usepackage{float}
%\usepackage[sfdefault,lf,t]{carlito}
\usepackage[default,tabular,lf]{sourcesanspro}
\usepackage[cmintegrals]{newtxsf}
\usepackage[italic,eulergreek]{mathastext}
\usepackage{setspace}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{csquotes}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage{fancybox}
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Response to reviewers},
            pdfauthor={Optimal forecast reconciliation with time series selection},
            colorlinks=true,
            linkcolor=blue,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,backend=biber, natbib=true,]{biblatex}
\addbibresource{../references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\title{Response to reviewers}

\author{Optimal forecast reconciliation with time series selection}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\usepackage{bm,url}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\lfoot{}\cfoot{}\rfoot{}
\lhead{\textsf{Response to reviewers}}
\rhead{\textsf{\thepage}}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*3}{*1}
\titlespacing{\subsection}{0pt}{*1}{*0.5}

%% LINE AND PAGE BREAKING
\sloppy
\raggedbottom
\usepackage[bottom]{footmisc}
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

\usepackage[showonlyrefs]{mathtools}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false}
\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[book]{url}{}
%\DeclareFieldFormat[inbook]{url}{}
%\DeclareFieldFormat[incollection]{url}{}
%\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
%\DeclareFieldFormat{extrayear}{}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}
\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}
\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}

%%% Change title format
\usepackage{color,titling,framed}
\usepackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

\pretitle{%

\vspace*{-1.2cm}

\LARGE\bfseries}
\posttitle{\vspace*{0.3cm}\par}
\preauthor{\large}
\postauthor{\hfill}
\predate{\small}
\postdate{\vspace*{0.1cm}}

\raggedbottom

\usepackage[australian]{babel}
\date{2024-08-27}

\usepackage{color}
\renewenvironment{quote}
               {\list{}{\rightmargin\leftmargin}%
                \item\relax\color[RGB]{0,150,0}}
               {\endlist}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}
\usepackage{todonotes}
\usepackage{bm}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother


\begin{document}
\vspace*{-2cm}
\definecolor{shadecolor}{RGB}{210,210,210}
\begin{snugshade}\sffamily
\maketitle
\end{snugshade}\vspace*{0.5cm}
\definecolor{shadecolor}{RGB}{248,248,248}
\setstretch{1}


We thank the reviewers for their careful reading of our paper; their
comments have led to several improvements and corrections. In this
revision, we have addressed all the comments raised by the reviewers,
and we provide a point-to-point response to each comment of the review
team. Reviewer comments are in black, our responses are in green.

\section*{Reviewer 2}\label{reviewer-2}
\addcontentsline{toc}{section}{Reviewer 2}

The paper reformulates forecast reconciliation as a grouped variable
selection problem. Regularisation and subset selection are carried out
using state-of-the-art optimisation techniques, including Mixed Integer
Programming. Recognising that reconciled forecasts \(\tilde{\bm{y}}\)
are simply linear combinations of base forecasts \(\hat{\bm{y}}\) via
\(\tilde{\bm{y}}=\bm{SG}\hat{\bm{y}}\), setting columns of \(\bm{G}\) to
zero can eliminate heavily misspecified forecasts from the combination.
The simulation results and empirical examples are thorough. In so far as
selection methods work, they do so when a diagonal matrix is plugged in
to the objective function in place of the forecast covariance matrix.

\textbf{Main Comments}

\begin{itemize}
\item
  The discussion around Proposition 1 is confusing. The discussion mixes
  up the roles of 1) the constraint \(\bm{GS}=\bm{I}\) and 2) the
  property of preserving the unbiasedness of forecasts after
  reconciliation. The proposition as it is currently phrased states:
  \emph{``Under the assumption of unbiasedness, the count of nonzero
  column entries of} \(\bm{G}\)\emph{,\ldots{} derived through solving
  equation 4 is at least equal to the number of time series at the
  bottom level.''}. However, equation 4 is solved with the constraint
  \(\bm{GS}=\bm{I}\). This is what guarantees that unbiased base
  forecasts will remain unbiased after reconciliation. The wording
  implies that this reasoning operates the other way around.

  A further implication of imposing the constraint \(\bm{GS}=\bm{I}\) is
  that \(\bm{G}\) must have no less than \(n_b\) non-zero columns, as is
  correctly argued in the proof to the proposition. However, an
  alternative and more precise wording of Proposition 1 would be
  \emph{``If the assumption that forecast reconciliation preserves
  unbiasedness is imposed by enforcing} \(\bm{GS}=\bm{I}\)\emph{, then
  the number of nonzero column entries of} \(\hat{\bm{G}}\) \emph{will
  be no less than} \(n_b\)\emph{''}.
\end{itemize}

\begin{quote}
Thank you for these helpful comments. We have refined the wording of
Proposition 3.1 as per your suggestions.
\end{quote}

\begin{itemize}
\item
  The second part of the Proposition 1 states \emph{``In addition, we
  can restore the full hierarchal structure by
  aggregating/disaggregating the selected time series''}. This again is
  somewhat imprecise and possibly refers to a different issue to that
  proven here. I suspect that what is meant by `restore' here, is the
  following. If the solution to Equation 4 yields a \(\hat{\bm{G}}\)
  with exactly \(n_b\) non-zero columns, then these correspond to
  variables from which the full hierarchy can be obtained using nothing
  but the information embedded in the constraints. As the authors
  suggest, it may be possible that the zero columns correspond to series
  AA and BA, but not to series AA and AB, since in the later case, the
  aggregation constraints alone are insufficient for forecasts of AA and
  AB to be recovered. This is not a consequence of ``assuming
  unbiasedness'' or even that \(\hat{\bm{G}}_{\cdot \mathbb{S}}\) has
  \(n_b\) columns. It is a consequence of enforcing the constraint
  \(\bm{GS}=\bm{I}\).

  To this I would add the following observations. First, solving
  equation (4) could lead to a solution for \(\hat{\bm{G}}\) that has
  more than \(n_b\) non-zero columns. In this case there are in fact too
  many series and a while a coherent forecast can be `restored', this
  cannot be done uniquely. Second, is the point that it is the
  constraint \(\bm{GS}=\bm{I}\) that enforces that the selected columns
  of \(\hat{\bm{G}}\) will correspond to variables that can restore the
  hierarchy. The mechanism by which it does so, is not rigorously proven
  here, however, a proof should be possible by leaning on some of the
  arguments made in \textcite{Zhang2023-op}, which is cited by the
  authors.
\end{itemize}

\begin{quote}
We have now corrected Proposition 3.1 to make it clearer.

Regarding the proof (located in Appendix A.1 of the revised version), we
have added explanations to address the two possible cases: one resulting
in a \(\hat{\bm{G}}\) matrix with exactly \(n_b\) nonzero columns, and
the other resulting in a \(\hat{\bm{G}}\) with more than \(n_b\) nonzero
columns. Additionally, we have rewritten the proof of the second part of
Proposition 3.1 and applied Theorem 2 from \textcite{Zhang2023-op} to
substantiate it.
\end{quote}

\begin{itemize}
\tightlist
\item
  As well as the statement of Proposition 1, the proof could be made
  tighter and clarified. In particular the same equation is essentially
  presented twice, once with \(\hat{\bm{G}}\) and the second time with
  \(\hat{\bm{G}}_{\cdot \mathbb{S}}\). Only the second of these is
  needed.
\end{itemize}

\begin{quote}
We have streamlined the proof of Proposition 1 and eliminated the
redundant equations.
\end{quote}

\begin{itemize}
\tightlist
\item
  Finally, in terms of tightening up the mathematics on page 8, at the
  very bottom, the line \(\operatorname{vec}(\hat{\bm{y}})=\hat{y}\),
  while correct, adds nothing since \(\operatorname{vec}(\hat{\bm{y}})\)
  is not used in equation (5). Also, on the second line, it would be
  worthwhile to make it explicit that
  \(\bm{SG}\hat{\bm{y}}=\operatorname{vec}(\bm{SG}\hat{\bm{y}})=(\hat{\bm{y}} \otimes \bm{S}) \operatorname{vec}(\bm{G})\),
  currently the second and third terms are present but not the first.
\end{itemize}

\begin{quote}
Regarding the proof of Proposition 3.2 (located in Appendix A.2 of the
revised version), we have removed the line
\(\operatorname{vec}(\hat{\bm{y}})=\hat{y}\) and made the final equation
explicit, as suggested.
\end{quote}

\begin{itemize}
\tightlist
\item
  Some of the methods discussed in the literature review are `in-sample'
  methods in the sense that \(\hat{\bm{y}}_{t+h|t}\) are predictions in
  the form of fitted values (\(\bm{y}_{t+h}\) is in training data when
  base forecasts are computed). Others (for example the RERM method) are
  `out-of-sample' in the sense that \(\hat{\bm{y}}\) are genuine
  forecasts. In principle all optimisation methods could use either an
  in-sample or out-of-sample approach. I believe that in this paper only
  `in-sample' methods are considered. This is reasonable, however, this
  should be clearly stated at some point (and it would provide
  motivation for not using RERM in the simulation studies and empirical
  results).
\end{itemize}

\begin{quote}
(Done) Point out the ``in-sample'' and ``out-of-sample'' methods when
discussing methods in the literature review and introducing proposed
methods.

(Done) Provide reason for not using RERM and ERM in the simulation
studies and empirical results. They demand extensive rounds of model
training and significant computation time.
\end{quote}

\begin{itemize}
\tightlist
\item
  More guidance could be given on the similarities between methods. For
  example Elasso seems to be the same as OLS-lasso with the important
  difference that only the latter retains the \(\bm{GS}=\bm{I}\)
  constraint. This also begs two further questions. The first is why a
  \(\bm{W}\) matrix is not used in the Elasso method. The second is why
  the \(\bm{GS}=\bm{I}\) constraint is not dropped for other
  regularisation approaches.
\end{itemize}

\begin{quote}
Clarify that the difference between Elasso and OLS-lasso lies in the
presence of the constraint GS=I and also data sources.

(Done) Discuss similarities between methods. (1) Similarities between
three constrained ``out-of-sample'' reconciliation methods. (2) The
difference between Elasso and the Subset, Parsimonious, and Lasso
methods, which also explains two further questions raised by the
reviewer.

(Done) How to use a W matrix for Elasso? Discussion.
\end{quote}

\begin{itemize}
\tightlist
\item
  Where the unbiasedness preserving property is dropped, the authors
  could also consider including an \(n_b\)-dimensional shift parameter
  \(\bm{d}\), such that
  \(\tilde{\bm{y}}=\bm{S}(\bm{d}+\bm{G}\tilde{\bm{y}})\). Then
  \(\bm{d}\) can be trained alongside with
  \(\operatorname{vec}(\bm{G})\) and act as a bias correction. The
  problem should still be able to be written down as a least squares
  problem, optimising w.r.t. (\(\bm{d}\),
  \(\operatorname{vec}(\bm{G})\)).
\end{itemize}

\begin{quote}
(Done) Extend the Elasso method by including a shift parameter as a bias
correction. Part of discussion.
\end{quote}

\begin{itemize}
\tightlist
\item
  In the intuitive method it is on the one hand stated that `\emph{when
  the jth diagonal element of} \(\bm{A}\) \emph{is zero, the jth column
  of} \(\bar{\bm{G}}\) \emph{becomes entirely composed of zeros'}.
  However, later it is stated that \emph{``implementing grouped variable
  selection\ldots{} can be challenging because it imposes restrictions
  of} \(\bar{\bm{G}}\) \emph{to ensure it adheres rigorously to the
  analytical solution of MinT while making the selection''}. The second,
  quite confusing statement, seems to contradict the first, if a zero
  element in \(\bm{A}\) implies a zero column of \(\bar{\bm{G}}\) then
  why is grouped selection even necessary? Also, when calling a method
  `intuitive' it is important to discuss what makes it intuitive. Little
  intuition is given to motivate this method, rather an appeal is made
  to reduce the number of parameters - perhaps `parsimonious' method
  would be a more appropriate name.
\end{itemize}

\begin{quote}
(Done) Reword the second statement for the Intuitive method.

(Done) Rename the Intuitive method to the Parsimonious method.

(Done) Rename in all results.
\end{quote}

\begin{itemize}
\tightlist
\item
  On page 24 the statement is made that \emph{``the Elasso method
  consistently outperforms the others overall''}. Given that the Elasso
  performs poorly at short horizons and for some groups of bottom level
  variables, I think the use of `consistently' is not warranted here.
\end{itemize}

\begin{quote}
(Done) Improve the result explanation to make it more rigorous. Now
``the Elasso method outperforms the others when evaluated on average
results across the entire hierarchy and forecast horizon''.
\end{quote}

\begin{itemize}
\tightlist
\item
  The results for most methods are very close to one another. Some
  testing on whether the observed differences are significant should be
  added.
\end{itemize}

\begin{quote}
(Done) Add MCB test for simulations.
\end{quote}

\begin{itemize}
\tightlist
\item
  While it is valuable to report the series that tend to be selected
  more often, some additional context would be useful - in particular
  the forecast variance of each series (in order to determine whether
  series with high forecast error are dropped) and the forecast
  correlation (to determine whether uncorrelated series are selected).
\end{itemize}

\begin{quote}
(Done) Report MASE of each series and correlation heatmap for
one-step-ahead forecast errors. Hint: Higher-level series tends to have
larger variance. The methods are preferred when the error correlation
within the structure is negative.
\end{quote}

\section*{Reviewer 3}\label{reviewer-3}
\addcontentsline{toc}{section}{Reviewer 3}

This paper proposes novel forecast reconciliation methods incorporating
time series selection. Two categories of such methods are proposed - one
category is based on out-of-sample information while the other category
of methods is based on in-sample information. These are illustrated via
simulation studies and two empirical applications. The findings are
argued to demonstrate improved forecast accuracy, especially at higher
aggregation levels, longer forecast horizons and in situations with
model misspecification.

The paper focuses on an important area (i.e., forecast reconciliation),
is theoretically rigorous and provides a good summary of the relevant
research. Incorporating time series selection to strengthen forecast
reconciliation has both theoretical and practical significance. While
the authors are to be commanded on their theoretical approach, the paper
lacks any substantive discussion on the practical significance of the
proposed methods. What are the precise implications of their proposed
methods for analysts working in labour economics and tourism, for
example (since their application data is from these two fields). Other
users of such forecasts? What could be some potential implications for
decision making? Policy making? These would be vital to enhancing the
paper's reach and impact.

\begin{quote}
Done
\end{quote}

\begin{itemize}
\tightlist
\item
  P.2 The authors claim ``Through simulation experiments and two
  empirical applications, we demonstrate that our proposed methods
  guarantee coherent forecasts that outperform or match their respective
  benchmark methods'' - how is such a guarantee provided? May wish to
  reconsider the wording.
\end{itemize}

\begin{quote}
Done
\end{quote}

\begin{itemize}
\tightlist
\item
  p.2-3 ``A remarkable feature of the proposed methods is their ability
  to diminish disparities arising from using different estimates of the
  base forecast error covariance matrix, thereby mitigating challenges
  associated with estimator selection, which is a prominent concern in
  the field of forecast reconciliation research.'' What about other
  concerns in forecast reconciliation research? It would be good to
  summarize these and address areas where the proposed methodology could
  support such concerns.
\end{itemize}

\begin{quote}
Done
\end{quote}

\begin{itemize}
\tightlist
\item
  Conclusion section needs to be extended to discuss the practical
  repercussions of this work as well as the potential limitations it
  faces. The authors briefly touch upon one aspect in the final
  paragraph, but there would be other challenges and such an extended
  discussion would be critical in both highlighting the limitations as
  well as emphasizing the contributions of the current paper.
\end{itemize}

\begin{quote}
Done
\end{quote}

\section{Reviewer 4}\label{reviewer-4}

\textbf{Summary}

This manuscript aims to eliminate the negative effects of initially
poor-performing base forecasts through time series selection. In the
first group of methods, based on out-of-sample information, the authors
formulate the problem as an optimization problem using diverse penalty
functions. The second group of methods relax the unbiasedness assumption
and introduces an additional reconciliation method with selection,
utilizing in-sample observations and their fitted values. Both
simulation and empirical studies show the great potential of the
proposed methods.

Overall, this paper provides deep insights into forecast reconciliation
methods and fits well with EJOR. However, a fair bit of work is required
to get published in EJOR. Specifically, the following major and minor
points could be considered to improve its exposition.

\textbf{Major comments}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  While the proposed methodology opts to leave \emph{poor} base
  forecasts unused in the creation of reconciled forecasts, the approach
  by \textcite{Zhang2023-op} is primarily focused on preserving
  \emph{good} base forecasts unchanged during the reconciliation
  process. The key difference lies in the handling of forecasts: the
  former method alters the forecasts of \emph{poor} base forecasts,
  ensuring these do not influence other nodes, whereas the latter method
  keeps the forecasts of certain nodes immutable, which then impacts
  others. It is crucial for the authors to emphasize these distinctions
  and interconnections theoretically, empirically or through
  discussions.
\end{enumerate}

\begin{quote}
(Done) Highlight the differences and connections between the proposed
methodology and the approach by \textcite{Zhang2023-op}. Discussion.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The authors' discussions on variable selection raise the question of
  whether they have considered a bi-level variable selection approach.
  Specifically, this entails allowing for both grouplevel and individual
  variable selection within those groups, an approach that could
  potentially enhance the precision and interpretability of the
  forecasting model. Such methodologies are well-documented in the
  literature, including the sparse group lasso \autocite{Simon2013-sp},
  hierarchical Lasso \autocite{Zhou2010-vs}, and the group bridge
  approach by \textcite{Huang2009-vs}. Adopting a bi-level selection
  mechanism could provide deeper insights into the contribution of
  individual base forecasts, especially in terms of their significance
  when mapped to bottom-level disaggregated forecasts.
\end{enumerate}

\begin{quote}
Thank you for these helpful comments. In the context of forecast
reconciliation, bi-level variable selection can be approached from two
perspectives.

First, following the idea of our methodology, we treat each time series
in a given hierarchy as a variable. To achieve time series selection
during reconciliation, we formulate an optimization problem that
controls the number of nonzero column entries in the weighting matrix
\(\bm{G}\), which can be considered individual variable selection. For
group-wise variable selection, we first have to define groups of time
series within the hierarchy. The task is challenging as both the
grouping and the number of series within each group are unknown and can
be determined subjectively. Thus, we choose not to consider bi-level
variable selection from this perspective in our paper.

Second, by delving deeper into the optimization problem, we can treat
each column of \(\bm{G}\) as a group and each element as an individual.
In this perspective, group-wise sparsity can be introduced by shrinking
some columns of \(\bm{G}\) towards zero, and within-group sparsity by
shrinking some elements towards zero. This can be achieved by simply
including an additional lasso penalty, as suggested in
\textcite{Simon2013-sp}, to address within-group sparsity. This provides
insights into the contribution of individual base forecasts to the
bottom-level reconciled forecasts. However, this would shift the focus
away from our primary objective of time series selection, introducing
additional hyperparameters and increasing computational complexity.
Therefore, we decide to discuss this idea as a potential future research
direction in Section 6.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Implementation of the GitHub repos provided by the authors tells that
  computation is somehow an issue in practice. Based on this, I suggest
  the authors consider the following.

  \begin{itemize}
  \tightlist
  \item
    Report the computational time would provide useful guides for the
    readers as well as the practitioners. Give discussions in terms of
    complexity and scalability, especially in the context of large-scale
    forecasting. This is crucial for applications in real-world
    scenarios.
  \end{itemize}

  \begin{quote}
  (Done) Report the computational time, discuss complexity and
  scalability.
  \end{quote}

  \begin{itemize}
  \tightlist
  \item
    \textcite{Ida2019-fa} proposes a fast Block Coordinate Descent for
    Sparse Group Lasso, which efficiently skips the updates of the
    groups whose parameters must be zeros by using the parameters in one
    group. They claim their approach reduces the processing time by up
    to \(97\%\) from the standard approach. I suggest the authors check
    whether it is helpful in improving computation.
  \end{itemize}

  \begin{quote}
  (Done) Not helpful for Elasso.
  \end{quote}
\item
  The authors claim the proposed methods keep poor base forecasts unused
  in generating reconciled forecasts. They should check whether this is
  the case at the end of their experimental studies. Specifically, an
  analysis should be conducted to ascertain whether the forecasts that
  are not selected for reconciliation indeed correspond to suboptimal
  ones. This could involve a detailed examination of the performance
  metrics of excluded forecasts compared to those included, to ensure
  that the selection process aligns with the stated objective of
  excluding poor base forecasts. Such an investigation validates the
  method's effectiveness and strengthens the reliability of the proposed
  approach in practical forecasting scenarios.
\end{enumerate}

\begin{quote}
(Done) Check whether the series with poor forecasts are not selected,
present results of each series. Consider scale-independent measures?
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Given the variability in time series data---ranging from seasonal
  patterns, trend components, to noise levels---the authors could
  investigate how their methods perform across a diverse set of
  conditions. Examples are:

  \begin{itemize}
  \tightlist
  \item
    Examining the performance stability of the proposed reconciliation
    methods across time series with different levels of seasonality.
  \item
    Assessing the impact of signal-to-noise ratios on the efficacy of
    the proposed methods.
  \end{itemize}
\end{enumerate}

\begin{quote}
We agree on the importance of investigating the performance stability of
the developed methods across diverse conditions. We believe that our
simulations and applications encompass a wide range of data variations.
First, within a given hierarchy, we account for variations in
seasonality, trend, and noise levels across different series. Such
variations reflect the inherent nature of hierarchical time series. For
example, in the Australian labor force data, the total number of
unemployed persons (the most aggregated series) shows much stronger
seasonality and higher signal-to-noise ratios compared to the unemployed
persons data for individual states and territories. Second, we address
variations in seasonality, trend, and noise levels across different
hierarchies in the paper. We generate stationary data in Section 4.2,
while examining quarterly data in Section 4.1 and analyzing monthly data
in two applications in Section 5. Additionally, in Section 4.2, we
explore the impact of correlation on the performance of the proposed
reconciliation methods by controlling the error correlation in the
simulated hierarchy. While the simulated data in Section 4.2 shows no
trend, the real-world datasets in Section 5 display noticeable trends.
Given the impracticality of considering all possible conditions and the
constraints on the paper's length, we decide to retain the experiments
as originally presented.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  The current methodology section and experiments provide a foundational
  overview of the proposed forecast reconciliation approaches and how
  they are implemented. However, it would benefit significantly from a
  more detailed exposition on several fronts to enhance the reader's
  understanding. Specific areas include:

  \begin{itemize}
  \tightlist
  \item
    More detailed justification for the choice of penalty functions and
    the theoretical underpinnings that motivated these choices.
  \end{itemize}

  \begin{quote}
  (Done) Provide justification for the choice of penalty functions.
  \end{quote}

  \begin{itemize}
  \tightlist
  \item
    The inclusion of a sensitivity analysis regarding the
    hyperparameters associated with each method, such as penalty
    parameters in the optimization problem. Understanding how variations
    in these parameters affect the outcomes could provide valuable
    insights into the robustness and flexibility of the proposed
    approaches.
  \end{itemize}

  \begin{quote}
  (Done) Include a sensitivity analysis regarding penalty parameters.
  \end{quote}
\end{enumerate}

\textbf{Minor comments}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  I suggest the authors put \emph{Variable selection} as one of the
  keywords.
\end{enumerate}

\begin{quote}
We have added ``Variable selection'' and removed ``Grouped time series''
from the keywords.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  When cross-referencing equations, I suggest using \textbackslash eqref
  from \textbf{amsmath} instead of \textbackslash ref in the LaTeX
  Kernel to match the equation reference exactly.
\end{enumerate}

\begin{quote}
Thanks. Now fixed.
\end{quote}

\printbibliography

\end{document}
