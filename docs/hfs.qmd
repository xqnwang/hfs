---
title: "Forecast reconciliation with subset selection"
format: 
  pdf:
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 3
    keep-tex: true
    df-print: paged
execute:
    freeze: auto
editor: visual
bibliography: hfs.bib
---

# Group best-subset selection

## MinT reconciliation

The unique solution of MinT is $\boldsymbol{G} = \left(\boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1} \boldsymbol{S}\right)^{-1} \boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1}$, which has a similar representation to a GLS estimator of a least square problem.

Consider a hierarchy consisting of $n$ time series in total and $n_b$ time series in the bottom level. Let $\boldsymbol{y}_t \in \mathbb{R}^n$ denote a vector of observations at time $t$ of all time series in the hierarchy, $\boldsymbol{b}_t \in \mathbb{R}^{n_b} \ (n_b < n)$ denote a vector of observations at time $t$ of only the most disaggregated bottom-level series.

Therefore, the trace minimization problem can be reformulated in terms of a Quadratic Programming (QP) problem as follows:

$$
\begin{aligned}
\min _{\boldsymbol{G}\hat{\boldsymbol{y}}_h} & \quad \left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right) \\
\text{ s.t. } & \quad \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b}.
\end{aligned}
$$ {#eq-mint}

Note that the variable of interest is $\boldsymbol{G}\hat{\boldsymbol{y}}_h$ rather than $\boldsymbol{G}$. So we can get a unique solution of reconciled forecasts at the bottom level, while infinitely many least-squares solutions of $\boldsymbol{G}$ as the columns of $\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}$ are not linearly independent.

## Best-subset selection

To eliminate the negative effect of some underperforming base forecasts on the performance of the reconciled forecasts, we want to **zero out some columns of** $\boldsymbol{G}$. Thus, the corresponding base forecasts in $\hat{\boldsymbol{y}}_h$ are not used to form the reconciled bottom-level forecasts and, moreover, are not used for all reconciled forecasts.

One way to achieve this goal is by considering an $\ell_0$-norm regularization. **Best-subset selection** generally performs well in high signal-to-noise (SNR) ratio regimes, while lasso performs better in low SNR regimes. We also include an additional $\ell_2$-norm regularization (in addition to the $\ell_0$ penalty), which is motivated by some related works [@hastie2020; @mazumder2023], which suggest that when the SNR is low, additional ridge regularization can improve the prediction performance of best-subset selection.

## Group best-subset selection with ridge regularization

The vectorization is frequently used together with the Kronecker product to express matrix multiplication as a linear transformation on matrices $\operatorname{vec}(A B C)=\left(C^{\prime} \otimes A\right) \operatorname{vec}(B)$. Therefore, the QP minimization problem can be reduced to a regression problem as follows:

$$
\begin{aligned}
\min _{\boldsymbol{G}} & \quad \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \\
\text { s.t. } & \quad \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b}.
\end{aligned}
$$ {#eq-mintreg}

Here, we consider the following $\ell_0\ell_2$**-regularized regression problem** of the following form to achieve selection in hierarchical forecasting:

$$
\begin{aligned}
\min _{\boldsymbol{G}} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \\
& \quad + \lambda_0 \sum_{j=1}^n \left\|\boldsymbol{G}_{\cdot j}\right\|_0 + \lambda_2\left\|\operatorname{vec}\left(\boldsymbol{G}\right)\right\|_2^2 \\
\text { s.t. } \quad & \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b},
\end{aligned}
$$ {#eq-subsetreg}

where $\lambda_0 > 0$ controls the number of non-zero columns of $\boldsymbol{G}$, and $\lambda_2 \geqslant 0$ controls the strength of the ridge regularization, $\sum_{j=1}^n \left\|\boldsymbol{G}_{\cdot j}\right\|_0$ is the number of non-zero columns of $\boldsymbol{G}$. In a hierarchy setting, the target variable, $\operatorname{vec}(\boldsymbol{G})$, in the minimization problem has a natural group structure, i.e., each column of $\boldsymbol{G}$ is a group. Thus, the $n \times n_b$ predictors in the regularized regression problem are divided into $n$ pre-specified, non-overlapping groups, with each group consisting of $n_b$ predictors. Therefore, the target problem is essentially **a group best-subset selection with ridge regularization**.

## Mixed integer program

We propose MIP formulations to solve @eq-subsetreg. We first present a **Big-M based MIP formulation** for problem @eq-subsetreg:

$$
\begin{aligned}
\min _{\boldsymbol{G}, \boldsymbol{z}, \check{\boldsymbol{e}}, \boldsymbol{g}^{+}} & \frac{1}{2}\check{\boldsymbol{e}}^{\prime} \boldsymbol{W}_h^{-1}\check{\boldsymbol{e}} + \lambda_0 \sum_{j=1}^n z_j + \lambda_2 \boldsymbol{g}^{+\prime}\boldsymbol{g}^{+} \\
\text { s.t. } \quad & \hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G}) = \check{\boldsymbol{e}}  \quad \cdots (C1)\\
& \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b} \Leftrightarrow\left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}\left(\boldsymbol{I}_{n_b}\right) \quad \cdots (C2) \\
& \sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M} z_j, \quad j \in[n] \quad \cdots (C3) \\
& \boldsymbol{g}^{+} \geqslant \operatorname{vec}(\boldsymbol{G}) \quad \cdots (C4) \\
& \boldsymbol{g}^{+} \geqslant-\operatorname{vec}(\boldsymbol{G}) \quad \cdots (C5) \\
& z_j \in\{0,1\}, \quad j \in[n] \quad \cdots (C6)
\end{aligned}
$$ {#eq-subsetmip}

where, $\mathcal{M}$ is a priori specified constant (leading to the name "Big-M") such that some optimal solution, say $\boldsymbol{g}^{+*}$, to @eq-subsetmip satisfies $\max _{j \in [n]}\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$, the binary variable $z_j$ controls whether all the regression coefficients in group $j$ are zero or not: $z_j=0$ implies that $\boldsymbol{G}_{\cdot j}=\mathbf{0}$, and $z_j=1$ implies that $\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{+} \leqslant \mathcal{M}$. Such Big-M formulations are commonly used in mixed integer programming to model relations between discrete and continuous variables, and have been recently used in $\ell_0$-regularized regression.

This is a **Mixed Integer Quadratic Program (MIQP)** and then get solved using some efficient commercial solvers such as Gurobi, CPLEX, and MOSEK. Note that the best subset selection is an **NP-hard problem**, which is computationally intensive.

## Hyperparameter

1.  **Former strategy**

-   $\lambda_0 = \{0, 10^{k-3}, 10^{k-2}, 10^{k-1}, 10^{k}, 10^{k+1}\}$, where $k$ is the number of digits before the decimal point for $\frac{1}{2 n_b}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)$. [(Reason)]{.underline}

-   $\lambda_2 = \{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$

To avoid cross-validation, we select the best combination of $\lambda_0$ and $\lambda_2$ by minimizing the sum of squared reconciled forecast errors in the training set, even though fitted values are often not true one-step ahead forecasts.

2.  **New strategy**

-   $\lambda_{0\max} = \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)$, $\lambda_{0\min} = 0.0001\lambda_{0\max}$, We compute solutions over a grid of $k-1$ values between $\lambda_{0\min }$ and $\lambda_{0\max }$, where $\lambda_{0, j}=\lambda_{0\max }\left(\lambda_{0\min } / \lambda_{0\max }\right)^{j / (k-1)}$ for $j=0, \ldots, k-1$. Thus, $\lambda_0=\{0, \lambda_{0,0}, \ldots, \lambda_{0,k-1}\}$. In our implementation, the default value for $k$ is 20.

-   $\lambda_2 = \{0, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}, 10^{2}\}$

## Simulation results

### Data simulation

**Structure:**

-   Top: Total
-   Middle: A, B
-   Bottom: AA, AB, BA, BB

**Data generation:**

The bottom-level series were generated using the basic structural time series model

$$
\boldsymbol{b}_t=\boldsymbol{\mu}_t+\boldsymbol{\gamma}_t+\boldsymbol{\eta}_t
$$

where $\boldsymbol{\mu}_t, \boldsymbol{\gamma}_t$, and $\boldsymbol{\eta}_t$ are the trend, seasonal, and error components, respectively,

$$
\begin{aligned}
\boldsymbol{\mu}_t & =\boldsymbol{\mu}_{t-1}+\boldsymbol{v}_t+\boldsymbol{\varrho}_t, & \boldsymbol{\varrho}_t & \sim \mathcal{N}\left(\mathbf{0}, \sigma_{\varrho}^2 \boldsymbol{I}_4\right), \\
\boldsymbol{v}_t & =\boldsymbol{v}_{t-1}+\boldsymbol{\zeta}_t, & \boldsymbol{\zeta}_t & \sim \mathcal{N}\left(\mathbf{0}, \sigma_\zeta^2 \boldsymbol{I}_4\right), \\
\boldsymbol{\gamma}_t & =-\sum_{i=1}^{s-1} \boldsymbol{\gamma}_{t-i}+\boldsymbol{\omega}_t, & \boldsymbol{\omega}_t & \sim \mathcal{N}\left(\mathbf{0}, \sigma_\omega^2 \mathbf{I}_4\right),
\end{aligned}
$$

and $\varrho_t, \zeta_t$, and $\omega_t$ are errors independent of each other and over time.

**Other details:**

-   $\sigma_{\varrho}^2=2, \sigma_\zeta^2=0.007$, and $\sigma_\omega^2=7$.
-   $s=4$ for quarterly data, $n=180$, $h=16$.
-   The initial values for $\boldsymbol{\mu}_0, \boldsymbol{v}_0, \boldsymbol{\gamma}_0, \boldsymbol{\gamma}_1, \boldsymbol{\gamma}_2$ were generated independently from a multivariate normal distribution with mean zero and covariance matrix, $\Sigma_0=I_4$.
-   Each component of $\boldsymbol{\eta}_t$ was generated from an $\operatorname{ARIMA}(p, 0, q)$ process with $p$ and $q$ taking values of 0 and 1 with equal probability.
-   The bottom-level series were then appropriately summed to obtain the data for higher levels.
-   This process was repeated 500 times.

### Scenarios:

-   Scenario 0: ETS

    -   ETS models are used to generate base forecasts.

-   Scenario I: D-AA

    -   Base forecasts (and also fitted values) of **series AA** multiplied by 1.5 to achieve deterioration.

-   Scenario II: D-A

    -   Base forecasts (and also fitted values) of **series A** multiplied by 1.5 to achieve deterioration.

-   Scenario III: D-Total

    -   Base forecasts (and also fitted values) of **series Total** multiplied by 1.5 to achieve deterioration.

## Tourism data results

### Data description

Australian domestic tourism (only considering hierarchical structure)

-   Monthly series from 1998 Jan to 2017 Dec.
-   240 months (20 years) for each series.
-   Hierarchy: Total/State/Zone/Region, 4 levels, n = 111 series in total.
-   Training set: 1998 Jan-2016 Dec.
-   Test set: 2017 Jan-2017 Dec.

## Some issues

**NP-hard problem.**

**Setup:**

-   gurobipy: parameters

    -   WarmStart: (1) Bottom-up, (2) All retained, (3) Relaxed problem `ifelse(z >= 0.001, 1, 0)`.
    -   TimeLimit = 600s.
    -   MIPGap = $\left|z_P-z_D\right| / \left|z_P\right| = 0.01$ for large hierarchy (default value is $10^{-4}$), where $z_P$ is the primal objective bound (i.e., the incumbent objective value, which is the upper bound for minimization problems), and $z_D$ is the dual objective bound (i.e., the lower bound for minimization problems),
    -   MIPFocus = 3: when the best objective bound is moving very slowly (or not at all), this focus more on the bound.
    -   Cuts = 2: aggressive cut generation.

-   Bound of variables

    -   should be as tight as possible to speed up computation.
    -   $\check{e} = \hat{y} - \tilde{y}: \quad [-|y|_{\max}, |y|_{\max}]$.
    -   $\mathcal{M}_k$ for each element in $\boldsymbol{G}$: $[-|G|_{\max}^{\text{bench}} - 1, |G|_{\max}^{\text{bench}} + 1]$.
    -   $\mathcal{M}$ for sum of absolute values of each column: $[-n_b, n_b]$.

# Group lasso

## Out-of-sample based method

### Group lasso with the unbiasedness constraint

The best-subset selection method is a NP-hard problem, which is computationally intensive.

Instead of involving an $\ell_0$ penalty, we consider the following $\ell_1$**-regularized regression problem** of the following form to achieve selection in hierarchical forecasting:

$$
\begin{aligned}
\min _{\boldsymbol{G}} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \\
& \quad + \lambda_1 \sum_{j=1}^n w_j \left\|\boldsymbol{G}_{\cdot j}\right\|_2 \\
\text { s.t. } \quad & \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b},
\end{aligned}
$$ {#eq-lassoreg}

where $\lambda_1 \geqslant 0$ is a tuning parameter, the $w_j$ terms account for the varying group sizes. The $\ell_1$-norm penalty induces sparsity in the solution. By introducing such a penalty, group lasso achieves sparse selection not of individual covariates but rather their groups. The target problem is essentially **a group lasso problem with the unbiasedness constraint**.

### Second-order cone program

We propose Second-order Cone Program (SOCP) formulations to solve @eq-lassoreg.

$$
\begin{aligned}
\min _{\boldsymbol{G}, \boldsymbol{z}, \check{\boldsymbol{e}}, \boldsymbol{g}^{+}} & \frac{1}{2}\check{\boldsymbol{e}}^{\prime} \boldsymbol{W}_h^{-1}\check{\boldsymbol{e}} + \lambda_1 \sum_{j=1}^n w_j c_j \\
\text { s.t. } \quad & \hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G}) = \check{\boldsymbol{e}}  \quad \cdots (C1) \\
& c_j = \sqrt{\sum_{i=1}^{n_b} g_{i + (j-1) n_b}^{2}}, \quad j \in[n] \quad \cdots (C2) \\
& \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b} \Leftrightarrow\left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}\left(\boldsymbol{I}_{n_b}\right) \quad \cdots (C3)
\end{aligned}
$$ {#eq-lassosocp}

where constraint (C2) is a second-order cone.

### Strategy & hyperparameter

**Problem: about** $w_j$ **and (C3)**

In a hierarchy setting, we can consider $w_j = 1$ as each group has the same size. After experiments on simulation data and tourism data, it shows that:

-   it can **hardly** shrink some columns of G to 0 when including **the unbiasedness constraint**.
-   if we remove the unbiasedness constraint, it frequently gives a **top-down** G or a G with only few number of non-zero columns, and the performance is very **poor and unstable**, especially for longer horizons.

**Reason: penalty term** $\lambda_1 \sum_{j=1}^n w_j \left\|\boldsymbol{G}_{\cdot j}\right\|_2$.

-   For a given $j$, if $\boldsymbol{G}_{\cdot j}$ is zeroed out, then other columns of $\boldsymbol{G}$ tend to be changed, which may have larger $\ell_2$-norm values.
-   Top level (first column) tends to have smaller $\ell_2$-norm when other columns are zeroed out.

#### Penalty weights

-   Consider a more flexible group lasso by putting different penalty weights $w_j$ on each group, e.g., $w_j = 1/\left\|\boldsymbol{G}_{\cdot j}^{\text{bench}}\right\|_2$, and also include the unbiasedness constraint.

#### Bound of variables

-   should be as tight as possible to speed up computation
-   $\check{e} = \hat{y} - \tilde{y}: \quad [-|y|_{\max}, |y|_{\max}]$.
-   $\mathcal{M}_k$ for each element in $\boldsymbol{G}$: $[-|G|_{\max}^{\text{bench}} - 1, |G|_{\max}^{\text{bench}} + 1]$.
-   $\mathcal{M}$ for sum of absolute values of each column: $[-n_b, n_b]$.

#### $\lambda$ Sequence

1.  **Former strategy**

-   $\lambda_1 = \{0, 10^{k-3}, 10^{k-2}, 10^{k-1}, 10^{k}, 10^{k+1}\}$, where $k$ is the number of digits before the decimal point for $\frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)$

2.  **New strategy**

The method used in [@yang2014] to decide a sequence of $\lambda$ values for group-lasso penalize learning problem is shown below.

The objective function is

$$
L(\boldsymbol{\beta} \mid \mathbf{D})+\lambda \sum_{j=1}^n w_j\left\|\boldsymbol{\beta}^{(j)}\right\|_2.
$$

We define $\lambda^{[1]}$ as the smallest $\lambda$ value such that all predictors (without intercept) have zero coefficients. Then the solution at $\lambda^{[1]}$ is $\widehat{\boldsymbol{\beta}}^{[1]} = \boldsymbol{0}$ as the null model estimates. Our strategy is to select a minimum value $\lambda_{\min}$, and construct a sequence of $K$ values of $\lambda$ decreasing from $\lambda_{max}$ to $\lambda_{\min}$ on the log scale.

$$
\lambda_{\max}=\lambda^{[1]}=\max _{j=1, \ldots, n}\left\|\left[\nabla L\left(\widehat{\boldsymbol{\beta}}^{[1]} \mid \mathbf{D}\right)\right]^{(j)}\right\|_2 / w_j, \quad w_j \neq 0
$$

For the out-of-sample based group lasso method, we ignore the unbiasedness constraint when we decide $\lambda_{\max}$. As $\boldsymbol{W}^{-1}$ is symmetric,

$$
\lambda_{\max}=\max _{j=1, \ldots, n}\left\|-\left(\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right)_{\cdot cj}\right)^{\prime} \boldsymbol{W}^{-1} \hat{\boldsymbol{y}}\right\|_2 / w_j,
$$

where $cj$ is the column indices of $\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right)$ corresponding to the $j$th column of $\boldsymbol{G}$.

## In-sample based method

### Empirical group lasso

Here, we consider using the in-sample residuals to formulate the problem. Let $\boldsymbol{Y}$ denote $N \times n$ matrix of historical data of all the time series in the structure, and $\hat{\boldsymbol{Y}}$ denote the matrix of in-sample 1-step-ahead forecasts of all the time series, where $N$ is the number of historical observations for each series, and $n$ is the number of time series in the hierarchy of interest.

Assuming that **the series in the structure are jointly weakly stationary**, the minimization problem can be given by:

$$
\begin{aligned}
\min _{\boldsymbol{G}} & \quad \frac{1}{2 N} \left\|\boldsymbol{Y}-\hat{\boldsymbol{Y}} \boldsymbol{G}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2 + \lambda_1 \sum_{j=1}^n w_j \left\|\boldsymbol{G}_{\cdot j}\right\|_2 \\
\Downarrow \\
\min _{\boldsymbol{G}} & \quad \frac{1}{2 N} \left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 + \lambda_1 \sum_{j=1}^n w_j \left\|\boldsymbol{G}_{\cdot j}\right\|_2,
\end{aligned}
$$

After reformulation, it reduced to a standard group lasso problem with $\operatorname{vec}(\boldsymbol{Y})$ as dependent variable and $\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}$ as design matrix.

### Strategy & hyperparameter

-   We can use the `gglasso` package, specifically we set `intercept = FALSE`, `pf = w`, `lambda.factor = 1e-05`, `eps = 1e-04`, and `foldid` to ensure each fold contains t number of observations from each variable (time series). [It's very slow when we set penalty factor by setting the `pf` parameter.]{style="color:red;"}
-   Thus, we consider proposing SOCP formulations to solve it, as in the out-of-sample based method.

#### Penalty weights

-   Consider putting different penalty weights $w_j$ on each group, e.g., $w_j = 1/\left\|\boldsymbol{G}_{\cdot j}^{\text{OLS}}\right\|_2, j = 1,2,\ldots,n$.

#### $\lambda$ Sequence

1.  **Former strategy**

$\lambda_1$ sequence: $\lambda_1 = \{0, 10^{k-3}, 10^{k-2}, 10^{k-1}, 10^{k}, 10^{k+1}\}$, where $k$ is the number of digits before the decimal point for $\frac{1}{2 N^{\text{train}}}\left\|\boldsymbol{Y}^{\text{train}}-\hat{\boldsymbol{Y}}^{\text{train}} \boldsymbol{G}^{\prime\text{MinT}} \boldsymbol{S}^{\prime}\right\|_F^2$.

2.  **New strategy**

For the in-sample based group lasso method,

$$
\lambda_{\max}=\max _{j=1, \ldots, n}\left\|-\frac{1}{N}\left(\left(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}\right)_{\cdot cj}\right)^{\prime} \operatorname{vec}(\boldsymbol{Y})\right\|_2 / w_j
$$

where $cj$ is the column indices of $(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}})$ corresponding to the $j$th column of $\boldsymbol{G}$.

# Intuitive method

## Methodology

Let $\bar{\boldsymbol{S}} = \boldsymbol{A}\boldsymbol{S}$, where $\boldsymbol{A} = \text{diag}(z_i)$ is a **diagonal matrix** with $z_i \in \{0, 1\}$. Then $\bar{\boldsymbol{G}} = (\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1}\boldsymbol{A}\boldsymbol{S})^{-1}\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1}$.

The problem of estimating the whole $\boldsymbol{G}$ reduces to estimating an appropriate $\boldsymbol{A}$ and then obtaining $\bar{\boldsymbol{G}}$.

Therefore, the problem can be reduced to a minimization problem as follows:

$$
\begin{aligned}
\min _{\boldsymbol{A}} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}-\boldsymbol{S}\bar{\boldsymbol{G}}\hat{\boldsymbol{y}}\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}-\boldsymbol{S}\bar{\boldsymbol{G}}\hat{\boldsymbol{y}}\right) + \lambda_0 \sum_{j=1}^n z_i \\
\text { s.t. } \quad & \bar{\boldsymbol{G}} = (\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1}\boldsymbol{A}\boldsymbol{S})^{-1}\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1} \\
& \bar{\boldsymbol{G}}\boldsymbol{S} = \boldsymbol{I}
\end{aligned}
$$

The main problem with the calculation is that we need to ensure that $(\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1}\boldsymbol{A}\boldsymbol{S})$ is invertible. In Gurobi, we can formulate the problem as follows.

$$
\begin{aligned}
\min _{\boldsymbol{A},\bar{\boldsymbol{G}},\boldsymbol{C}} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\bar{\boldsymbol{G}})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\bar{\boldsymbol{G}})\right) + \lambda_0 \sum_{j=1}^n z_i \\
\text { s.t. } \quad & 
\boldsymbol{C}(\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1}\boldsymbol{A}\boldsymbol{S}) = \boldsymbol{I} \\
& \bar{\boldsymbol{G}} = \boldsymbol{C}\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1} \\
& \bar{\boldsymbol{G}}\boldsymbol{S} = \boldsymbol{I}
\end{aligned}
$$

To be able to use Gurobi, we rewrite the problem (constraints) as follows.

$$
\begin{aligned}
\min _{\boldsymbol{A},\bar{\boldsymbol{G}},\boldsymbol{C}} \quad & \frac{1}{2}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\bar{\boldsymbol{G}})\right)^{\prime} \boldsymbol{W}^{-1}\left(\hat{\boldsymbol{y}}-\left(\hat{\boldsymbol{y}}^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\bar{\boldsymbol{G}})\right) + \lambda_0 \sum_{j=1}^n z_i \\
\text { s.t. } \quad & 
\bar{\boldsymbol{G}}\boldsymbol{A}\boldsymbol{S} = \boldsymbol{I} \\
& \bar{\boldsymbol{G}} = \boldsymbol{C}\boldsymbol{S}'\boldsymbol{A}'\boldsymbol{W}^{-1} \\
& \bar{\boldsymbol{G}}\boldsymbol{S} = \boldsymbol{I}
\end{aligned}
$$

## Hyperparameter

-   $\lambda_{0\max} = \frac{1}{2 n_b}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\tilde{\boldsymbol{y}}_h^{\text{MinT}}\right)$, $\lambda_{0\min} = 0.0001\lambda_{0\max}$, We compute solutions over a grid of $k-1$ values between $\lambda_{0\min }$ and $\lambda_{0\max }$, where $\lambda_{0, j}=\lambda_{0\max }\left(\lambda_{0\min } / \lambda_{0\max }\right)^{j / (k-1)}$ for $j=0, \ldots, k-1$. Thus, $\lambda_0=\{0, \lambda_{0,0}, \ldots, \lambda_{0,k-1}\}$. In our implementation, the default value for $k$ is 20.

# More on the unbiasedness constraint

The unbiasedness constraint is

$$
\boldsymbol{G}\boldsymbol{S} = \boldsymbol{I}
$$

and our focus is to estimate $\boldsymbol{G}$ by minimizing a loss function with several constraints.

According to the unbiasedness constraint, we have

1.  **Rank --- Number of non-zero columns**

    Conclusion: $\operatorname{rank}(\boldsymbol{G}) \geq n_b$ which means that the **number** of non-zero columns of the estimated $\boldsymbol{G}$ should be at least $n_b$.

    Proof: $\min (\operatorname{rank}(\boldsymbol{G}), \operatorname{rank}(S)) \geq \operatorname{rank}(\boldsymbol{I})=n_b$

2.  **Hierarchical structure --- Location of non-zero columns**

    Let $\boldsymbol{G}_{\cdot \mathbb{S}} \in \mathbb{R}^{n_b \times |\mathbb{S}|}$ denote the submatrix of $\boldsymbol{G}$ whose columns are indexed by a set $\mathbb{S}$ (and when $\mathbb{S} = \{j\}$, we simply use $\boldsymbol{G}_{j}$), and $\boldsymbol{G}_{\mathbb{S}\cdot} \in \mathbb{R}^{|\mathbb{S}| \times n}$ denote the submatrix of $\boldsymbol{G}$ whose rows are indexed by a set $\mathbb{S}$ (and when $\mathbb{S} = \{i\}$, we simply use $\boldsymbol{G}_{i\cdot}$),

    Conclusion: If the set $\mathbb{S}$ involves the indices of non-zero columns of the estimated $\boldsymbol{G}$, then $\operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot}) = n_b$ which means that we should make sure that the whole hierarchy is still **revertible** after removing the nodes corresponding to zero columns of $\boldsymbol{G}$.

    Proof: If the set $\mathbb{S}$ involves the indices of non-zero columns of the estimated $\boldsymbol{G}$, then $\boldsymbol{G}\boldsymbol{S} = \boldsymbol{G}_{\cdot \mathbb{S}}\boldsymbol{S}_{\mathbb{S}\cdot}$ and $\min (\operatorname{rank}\left(\boldsymbol{G}_{\cdot \mathbb{S}}), \operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot})\right) \geq \operatorname{rank}(\boldsymbol{I})=n_b$.

    Recall that $\operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot}) \leq n_b$ as $\boldsymbol{S}$ has $n_b$ columns. Hence, $\operatorname{rank}(\boldsymbol{S}_{\mathbb{S}\cdot}) = n_b$.

# Results

```{r}
#| label: setup
#| include: false

knitr::opts_knit$set(root.dir = '../')
```

```{r}
#| label: package
#| echo: false
#| message: false
#| warning: false

library(dplyr)
library(reshape)
library(ggplot2)
library(tidyverse)
library(knitr)
library(kableExtra)
library(latex2exp)
```

```{r}
#| label: function
#| echo: false
#| message: false
#| warning: false

## MCB test
source("R/nemenyi.R")

## Other functions used for analysis
source("R/analysis.R")
```

## Model misspecification in a hierarchy

```{r}
#| label: tbl-s0-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the simulation data."

measure <- "rmse"
data_label <- "simulation"
scenario <- NULL
horizons <- c(1, 4, 8, 16)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-s1-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the  simulation data in Scenario I (Base forecasts and also fitted values of series AA are deteriorated)."

measure <- "rmse"
data_label <- "simulation"
scenario <- "s1"
horizons <- c(1, 4, 8, 16)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-s2-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the  simulation data in Scenario II (Base forecasts and also fitted values of series A are deteriorated)."

measure <- "rmse"
data_label <- "simulation"
scenario <- "s2"
horizons <- c(1, 4, 8, 16)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-s3-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the  simulation data in Scenario III (Base forecasts and also fitted values of series Total are deteriorated)."

measure <- "rmse"
data_label <- "simulation"
scenario <- "s3"
horizons <- c(1, 4, 8, 16)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-s0-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Ratio of being retained for each time series after subset selection in 500 simulation data instances."

data_label <- "simulation"
methods <- c("subset", "intuitive", "lasso")
scenarios <- c("s0", "s1", "s2", "s3")
series_name <- c("Top", "A", "B", "AA", "AB", "BA", "BB")
simulation_info <- combine_z(data_label, methods, scenarios, series_name)
latex_sim_nos_table(simulation_info$out_s0$z,
                    simulation_info$out_s0$n)
```

```{r}
#| label: tbl-s1-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Ratio of being retained for each time series after subset selection in 500 simulation data instances (Scenario I)."

latex_sim_nos_table(simulation_info$out_s1$z,
                    simulation_info$out_s1$n)
```

```{r}
#| label: tbl-s2-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Ratio of being retained for each time series after subset selection in 500 simulation data instances (Scenario II)."

latex_sim_nos_table(simulation_info$out_s2$z,
                    simulation_info$out_s2$n)
```

```{r}
#| label: tbl-s3-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Ratio of being retained for each time series after subset selection in 500 simulation data instances (Scenario III)."

latex_sim_nos_table(simulation_info$out_s3$z,
                    simulation_info$out_s3$n)
```

```{r}
#| label: fig-sim-MCB
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 16
#| fig-height: 3.5
#| fig-cap: "MCB test conducted on the methods examined using the simulation data, the ranks are computed considering average accuracy for all the time series in 500 hierarchies, i.e. a total of 7 series × 500 hierarchies = 3500 instances."

data_label <- "simulation"
methods <- c("subset", "intuitive", "lasso")
scenarios <- c("s0", "s1", "s2", "s3")
highlight <- c("OLS", "WLSs", "WLSv", "MinT", "MinTs")
target <- sapply(highlight, function(len){
  c(paste0(len, c("", "-subset", "-intuitive", "-lasso")))
}) |> as.vector()
target <- c("Base", "BU", target, "EMinT", "Elasso") |> rev()
h <- 16

RMSE_sim_s0 <- RMSE_MCB_sim("simulation", methods=methods, scenario="s0", h=16)[,target]
RMSE_sim_s1 <- RMSE_MCB_sim("simulation", methods=methods, scenario="s1", h=16)[,target]
RMSE_sim_s2 <- RMSE_MCB_sim("simulation", methods=methods, scenario="s2", h=16)[,target]
RMSE_sim_s3 <- RMSE_MCB_sim("simulation", methods=methods, scenario="s3", h=16)[,target]

par(mfrow=c(1, length(scenarios)),
    mar=c(5,0,0.1,0)
    #mar=c(5.1,2.1,4.1,2.1)
)

measure <- "RMSE"
scenarios <- c("s0", "s1", "s2", "s3")
for(j in scenarios){
  char <- paste0(measure, "_sim_", j)
  x <- get(char)
  
  if (j == "s0"){
    title <- "Simulation"
  } else if (j == "s1"){
    title <- "Scenario I"
  } else if (j == "s2"){
    title <- "Scenario II"
  } else if (j == "s3"){
    title <- "Scenario III"
  }
  
  nemenyi(x, conf.level = 0.95, plottype = "vmcb",
          sort = FALSE, 
          shadow = FALSE,
          group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
          Title = title,
          Xlab = "Mean ranks",
          Ylab = "")
}
```

## Exploring the effect of correlation

```{r}
#| label: tbl-corr-rmse
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the VAR simulation data."

measure <- "rmse"
data_label <- "corr"
corr <- seq(-0.8, 0.8, 0.2)
index <- c(1, 3, 5, 7, 9)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_corr_table(data_label, methods, corr, index, measure)
latex_corr_table(out_all)
```

```{r}
#| label: fig-corr-MCB
#| echo: false
#| eval: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 12
#| fig-height: 7
#| fig-cap: "MCB test conducted on the methods examined using the VAR simulation data, the ranks are computed considering average accuracy for all the time series in 500 hierarchies, i.e. a total of 7 series × 500 hierarchies = 3500 instances."

data_label <- "corr"
corr <- c(-0.8, -0.4, 0, 0.8, 0.4)
index <- c(1, 3, 5, 9, 7)
methods <- c("subset", "intuitive", "lasso")
highlight <- c("OLS", "WLSs", "WLSv", "MinT", "MinTs")
target <- sapply(highlight, function(len){
  c(paste0(len, c("", "-subset", "-intuitive", "-lasso")))
}) |> as.vector()
target <- c("Base", "BU", target, "EMinT", "Elasso") |> rev()

par(mfrow=c(2, 3),
    mar=c(5,0,0.1,0.5)
    #mar=c(5.1,2.1,4.1,2.1)
)

for(i in 1:length(index)){
  x <- RMSE_MCB_corr(paste0("corr_", index[i]), methods=methods, h=1)[,target]
  title <- paste0("r = ", corr[i])
  
  nemenyi(x, conf.level = 0.95, plottype = "vmcb",
          sort = FALSE, 
          shadow = FALSE,
          group = list(1:2, 3:6, 7:10, 11:14, 15:18, 19:22, 23:24),
          Title = title,
          Xlab = "Mean ranks",
          Ylab = "")
}
```

```{r}
#| label: tbl-corr-info
#| echo: false
#| message: false
#| warning: false
#| results: asis
#| tbl-cap: Ratio of being retained for each time series after subset selection in 500 VAR simulation data instances (A and BA, $\\rho = -0.8$).

data_label <- "corr_1"
methods <- c("subset", "intuitive", "lasso")
series_name <- c("Top", "A", "B", "AA", "AB", "BA", "BB")
corr_info <- combine_z(data_label, methods, scenarios = "s0", series_name)
latex_sim_nos_table(corr_info$out_s0$z,
                    corr_info$out_s0$n)
```

## Tourism data

```{r}
#| label: tbl-tourism-rmse
#| echo: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the tourism data."

measure <- "rmse"
data_label <- "tourism"
scenario <- NULL
horizons <- c(1, 4, 8, 12)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-tourism-info
#| echo: false
#| results: asis
#| tbl-cap: "Number of time series retained after subset selection and the optimal hyperparameter values for the tourism data."

tourism_subset_reconsf <- readRDS(file = "data_new/tourism_subset_reconsf.rds")
tourism_lasso_reconsf <- readRDS(file = "data_new/tourism_lasso_reconsf.rds")
Top <- 1
State <- 2:8
Zone <- 9:35
Region <- 36:111
Total <- 1:111

tourism_subsetonly_reconsf <- c(tourism_subset_reconsf[[1]][grepl("_subset", names(tourism_subset_reconsf[[1]]))],
                                Elasso = list(tourism_lasso_reconsf[[1]]$Elasso))
tourism_subset_z <- sapply(tourism_subsetonly_reconsf, function(lentry) lentry$z) |> t()
tourism_subset_lambda <- sapply(tourism_subsetonly_reconsf, function(lentry) lentry$lambda_opt[c("l0", "l1", "l2")] |> unname()) |> t()
tourism_subset_info <- data.frame(tourism_subset_z, tourism_subset_lambda)

None <- c(Top = length(Top), State = length(State), Zone = length(Zone), Region = length(Region),
          Total = length(Total), "$\\lambda_0$" = NA, "$\\lambda_1$" = NA, "$\\lambda_2$" = NA)
tourism_subset_info <- apply(tourism_subset_info, 1, function(lentry){
  c(Top = sum(lentry[Top]), State = sum(lentry[State]), 
    Zone = sum(lentry[Zone]), Region = sum(lentry[Region]), 
    Total = sum(lentry[Total]), 
    "$\\lambda_0$" = round(tail(lentry, 3), 2)[1],
    "$\\lambda_1$" = round(tail(lentry, 3), 2)[2],
    "$\\lambda_2$" = round(tail(lentry, 3), 2)[3])
}) %>% t() %>% rbind(None, .)
rownames(tourism_subset_info) <- sub("_", "-", rownames(tourism_subset_info))

options(knitr.kable.NA = '-')
tourism_subset_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("hold_position", "repeat_header", "scale_down")) |>
  add_header_above(c("", "Number of time series retained" = 5, "Optimal parameters" = 3), align = "c")
```

```{r}
#| label: fig-tourism-MCB-RMSE
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "MCB test conducted on the methods examined using the tourism data, the ranks are computed considering all the time series of the hierarchy included in the data set, i.e. a total of 111 series in the hierarchy."
data_label <- "tourism"
method_label <- "subset"
h <- 12

test <- readRDS(file = paste0("data/", data_label, "_test.rds"))
reconsf <- readRDS(file = paste0("data_new/", data_label, "_", method_label, "_reconsf.rds"))
lasso_reconsf <- readRDS(file = "data_new/tourism_lasso_reconsf.rds")
reconsf[[1]] <- c(reconsf[[1]], Elasso = list(lasso_reconsf[[1]]$Elasso))

methods <- names(reconsf[[1]])
for(method in methods) {
  out <- unique(test$Index) |> 
    purrr::map(\(index) 
               extract_element(data = reconsf, index = index, 
                               method = method, element = "y_tilde")) %>% 
    do.call(rbind, .)
  assign(tolower(method), out)
}

# Plot 1: h=1-12 average RMSE (111 series)
RMSE <- sapply(methods, function(lmethod){
  assign(lmethod, calc_rmse(fc = get(tolower(lmethod)), test = test, h = h))
}, USE.NAMES = TRUE) |> as.data.frame() |> data.matrix()
colnames(RMSE) <- sub("_", "-", colnames(RMSE))
highlight <- c("OLS", "WLSs", "WLSv", "MinTs")
target <- sapply(highlight, function(len){
  c(paste0(len, c("", "-subset")))
}) |> as.vector()
target <- c("Base", "BU", target, "EMinT", "Elasso") |> rev()
par(mfrow=c(1, 1),
    mar=c(5,0.1,0.1,0.3)
)

nemenyi(RMSE[,target], conf.level = 0.95, plottype = "vmcb",
        sort = FALSE,
        shadow = FALSE,
        group = list(1:2, 3:4, 5:6, 7:8, 9:10, 11:12),
        Title = "",
        Xlab = "Mean ranks",
        Ylab = "")
```

```{r}
#| label: fig-tourism-rmse
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 10
#| fig-height: 4.5
#| fig-cap: "Average reconciliation errors in terms of RMSE (1- to 12-step-ahead) after forecast reconciliation, for a single series, between disaggregate and aggregate views of the tourism data, for the different reconciliation methods. Time series are ordered in the horizontal axis."

RMSE_heatmap <- RMSE
rownames(RMSE_heatmap) <- 1:NROW(RMSE_heatmap)
RMSE_melt <- melt(RMSE_heatmap)  
colnames(RMSE_melt) <- c("Series", "Method", "RMSE")

# # Purples palette
# ggplot(RMSE_melt, aes(x = Series, y = Method, fill = RMSE)) +
#   geom_tile() +
#   scale_fill_gradientn(colors = rev(hcl.colors(50, "Purples"))[c(seq(1, 20, 10), seq(21, 50, 1))]) +
#   labs(x = "Time series", y = "") +
#   scale_x_discrete(expand = c(0, 0),
#                    breaks = seq(20, 100, 20)) +
#   scale_y_discrete(limits=rev(c("Base","BU","OLS","OLS-subset", "WLSs", "WLSs-subset", "WLSv", "WLSv-subset", "MinTs", "MinTs-subset"))) +
#   theme(
#     legend.text = element_text(face = "bold"),
#     plot.background = element_blank(),
#     panel.border = element_blank()
#   )

# 10*4
ggplot(RMSE_melt, aes(x = Series, y = Method, fill = RMSE)) +
  geom_tile() +
  geom_vline(xintercept = c(1.5, 8.5, 35.5), linetype = "dashed", linewidth = 0.5) +
  scale_fill_gradientn(colors = c("#f7d9a6", 
                                  rev(hcl.colors(100, "Purples"))[c(seq(1, 50, 10), seq(51, 100, 1))])) +
  labs(x = "Time series", y = "") +
  scale_x_continuous(expand = c(0, 0), 
                     breaks =  seq(20, 100, 20),
                     sec.axis = dup_axis(name = "",
                                         breaks = c(4.5, 11.5, 39),
                                         labels = c("States", "Zones", "Regions"))) +
  scale_y_discrete(expand = c(0, 0), 
                   limits = rev(c("Base", "BU", "OLS", "OLS-subset", 
                                  "WLSs", "WLSs-subset", "WLSv", "WLSv-subset", 
                                  "MinTs", "MinTs-subset", "EMinT", "Elasso"))) +
  theme(
    plot.background = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, linewidth=1),
    axis.title.x = element_text(face = "bold", size = 12),
    legend.title = element_text(face = "bold", size = 10),
    legend.text = element_text(face = "bold", size = 10),
    legend.position = "bottom",
    axis.text = element_text(face = "bold", size = 10),
    axis.ticks.x.top = element_blank()
  ) +
  guides(fill = guide_colourbar(barwidth = 10,
                                barheight = 1.5))
```

## Labour data

```{r}
#| label: tbl-labour-rmse
#| echo: false
#| results: asis
#| tbl-cap: "Out-of-sample forecast performance (average RMSE) for the labour data."

measure <- "rmse"
data_label <- "labour"
scenario <- NULL
horizons <- c(1, 4, 8, 12)
methods <- c("subset", "intuitive", "lasso")

out_all <- combine_table(data_label, methods, measure, scenario, horizons)
latex_table(out_all)
```

```{r}
#| label: tbl-labour-info
#| echo: false
#| results: asis
#| tbl-cap: "Number of time series retained after subset selection and the optimal hyperparameter values for the labour data."

labour_subset_reconsf <- readRDS(file = "data_new/labour_subset_reconsf.rds")
labour_lasso_reconsf <- readRDS(file = "data_new/labour_lasso_reconsf.rds")
Top <- 1
Duration <- 2:7
STT <- 8:15
Duration_STT <- 16:63
Total <- 1:63

labour_subsetonly_reconsf <- c(labour_subset_reconsf[[1]][grepl("_subset", names(labour_subset_reconsf[[1]]))],
                               Elasso = list(labour_lasso_reconsf[[1]]$Elasso))
labour_subset_z <- sapply(labour_subsetonly_reconsf, function(lentry) lentry$z) |> t()
labour_subset_lambda <- sapply(labour_subsetonly_reconsf, function(lentry) lentry$lambda_opt[c("l0", "l1", "l2")] |> unname()) |> t()
labour_subset_info <- data.frame(labour_subset_z, labour_subset_lambda)

None <- c(Top = length(Top), Duration = length(Duration), STT = length(STT), Duration_STT = length(Duration_STT),
          Total = length(Total), "$\\lambda_0$" = NA, "$\\lambda_1$" = NA, "$\\lambda_2$" = NA)
labour_subset_info <- apply(labour_subset_info, 1, function(lentry){
  c(Top = sum(lentry[Top]), Duration = sum(lentry[Duration]), 
    STT = sum(lentry[STT]), Duration_STT = sum(lentry[Duration_STT]), 
    Total = sum(lentry[Total]), 
    "$\\lambda_0$" = round(tail(lentry, 3), 2)[1],
    "$\\lambda_1$" = round(tail(lentry, 3), 2)[2],
    "$\\lambda_2$" = round(tail(lentry, 3), 2)[3])
}) %>% t() %>% rbind(None, .)
rownames(labour_subset_info) <- sub("_", "-", rownames(labour_subset_info))
colnames(labour_subset_info) <- sub("Duration_STT", "Duration x STT", colnames(labour_subset_info))
options(knitr.kable.NA = '-')
labour_subset_info |>
  kable(format = "latex",
        booktabs = TRUE,
        digits = 2,
        align = rep("r", 8),
        escape = FALSE,
        linesep = "") |>
  kable_styling(latex_options = c("hold_position", "repeat_header", "scale_down")) |>
  add_header_above(c("", "Number of time series retained" = 5, "Optimal parameters" = 3), align = "c")
```

# Further issues

-   More on large hierarchy:
    -   Sub hierarchy + Voting

# References

::: {#refs}
:::
