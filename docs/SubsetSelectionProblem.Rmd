---
title: "Subset Selection for Forecast Reconciliation"
author: "Xiaoqian Wang"
date: '`r Sys.Date()`'
output:   
  pdf_document:
    number_sections: true
linestretch: 1.5
fontsize: 11pt
mainfont: "Times New Roman"
monofont: "Monaco"
geometry: a4paper, margin=20mm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


# Hierarchical time series

Create a hierarchical structure as complete as possible. Only nested structure is considered here. Then the summing matrix $\boldsymbol{S}$ is given and $\hat{\boldsymbol{y}}_h$ can be obtained easily by specifying forecasting models.

![A 2-level hierarchical tree structure](figs/hs.png){width=20%}

For example,
$$
\boldsymbol{S}=\left[\begin{array}{llllll}
1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 \\
\end{array}\right], \text{ and } \hat{\boldsymbol{y}}_h = (10, 6, 5, 1, 4, 0, 2, 5)'.
$$


# Linear forecast reconciliation

$$
\tilde{\boldsymbol{y}}_h=\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h,
$$
where $\boldsymbol{G}_h$ maps the base forecasts $\hat{\boldsymbol{y}}_h$ into the bottom level, it combines all base forecasts to form bottom-level forecasts.

* **Bottom-up approach:** $\boldsymbol{G}_{bu}=\left[\begin{array}{ll} \boldsymbol{O}_{n_b \times n_a} & \boldsymbol{I}_{n_b}\end{array}\right]$

* **Top-down approach:** $\boldsymbol{G}_{td}=\left[\begin{array}{ll} \boldsymbol{p} & \boldsymbol{O}_{n_b \times (n-1)}\end{array}\right]$, where $\boldsymbol{p}$ is an $n_b$-dimensional vector including the set of disaggregation proportions.

* **Middle-out approach:** $\boldsymbol{G}_{mo}=\left[\begin{array}{lll} \boldsymbol{O}_{n_b \times n_t} & \boldsymbol{P}_{n_b \times n_l} & \boldsymbol{O}_{n_b \times n_d}\end{array}\right]$, where $n_t + n_l + n_d = n$, $n_d \geq n_b$, $n_t \geq 1$, $\sum_{i=1}^{n_b}\sum_{j=1}^{n_l}p_{ij} = n_l$.

* **Optimization approaches:** $\boldsymbol{G}_h = \left(\boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1} \boldsymbol{S}\right)^{-1} \boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1}$ is obtained by solving the optimization problem
$$
\min _{\boldsymbol{G}_h}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right), \text{ s.t. } \boldsymbol{G}_h \boldsymbol{S}=\boldsymbol{I}_{n_b}
$$


# Subset selection

a. About $\boldsymbol{G}_h$. Our goal is to **zero out some columns** so that the corresponding base forecasts in $\hat{\boldsymbol{y}}_h$ are not used to form the reconciled bottom-level forecasts and, furthermore, all reconciled forecasts.

b. About $\boldsymbol{S}$. It sums up the reconciled bottom-level forecasts to get the full set of reconciled forecasts. For the purpose of automatic selection and comparison, we don't need to zero out the corresponding rows.

We have to add additional constraints on $\boldsymbol{G}_h$. This leads to a main question:

*How to mathematically describe the constraint?*

## Ideas based on MinT

### Idea 1

$$
\tilde{\boldsymbol{y}}_h=\boldsymbol{S} \boldsymbol{G}_h \boldsymbol{B}_h \hat{\boldsymbol{y}}_h,
$$
where $\boldsymbol{B}_h$ is an $n \times n$ diagonal matrix with elements of the main diagonal being either zero or one.

The optimization problem can be written as
$$
\begin{array}{r}
\min_{\boldsymbol{G}_h}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \boldsymbol{B}_h \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \boldsymbol{B}_h \hat{\boldsymbol{y}}_h\right) + \lambda ||\boldsymbol{B}_h \boldsymbol{1}||_{1}, \\
\text{ s.t. } \boldsymbol{G}_h \boldsymbol{B}_h \boldsymbol{S} =\boldsymbol{I}_{n_b} \\
b_{ii}(1-b_{ii})=0 \text{ for } i =1,2,\ldots,n \\
b_{ij}=0 \text{ for } i \neq j
\end{array}
$$
The difficulty is that we wish to find the value of $\boldsymbol{G}_h$ and $\boldsymbol{B}_h$ simultaneously. 

### Idea 2

$$
\tilde{\boldsymbol{y}}_h=\boldsymbol{S} \check{\boldsymbol{G}}_h \check{\boldsymbol{B}}_h \hat{\boldsymbol{y}}_h,
$$
where $\check{\boldsymbol{G}}_h$ is given using ols, wls, structural scaling, or shrinkage estimator of $\boldsymbol{W}_h$. $\check{\boldsymbol{B}}_h$ is an $n \times n$ diagonal matrix with elements of the main diagonal being either greater than or equal to zero. So $\check{\boldsymbol{B}}_h$ can adaptively changes given $\check{\boldsymbol{G}}_h$ and we can only estimate $\check{\boldsymbol{B}}_h$. But the reconciled forecasts are likely to no longer preserve the unbiasedness.

The optimization problem can be written as
$$
\begin{array}{r}
\min _{\boldsymbol{G}_h}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \check{\boldsymbol{G}}_h \check{\boldsymbol{B}}_h \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \check{\boldsymbol{G}}_h \check{\boldsymbol{B}}_h \hat{\boldsymbol{y}}_h\right) + \lambda ||\check{\boldsymbol{B}}_h \boldsymbol{1}||_{1}, \\
\text{ s.t. } b_{ii} \geq 0 \text{ for } i = 1,\ldots,n\\
b_{ij}=0 \text{ for } i \neq j
\end{array}
$$

**R implementation**

```{r example1, message=FALSE, warning=FALSE}
library(CVXR)

y_hat <- c(10, 6, 5, 1, 4, 0, 2, 5)
S <- rbind(matrix(c(rep(1, 5), c(rep(1, 3), rep(0, 2)), c(rep(0, 3), rep(1, 2))), 
                  nrow = 3, byrow = TRUE),
           diag(1, nrow = 5))

W_inv <- solve(diag(c(5,3,2,1,1,1,1,1))) # structural scaling approximation
G <- solve(t(S) %*% W_inv %*% S) %*% t(S) %*% W_inv

b <- Variable(8)
lambda <- 0.3

e <- y_hat - S %*% G  %*% diag(b) %*% y_hat
obj <- quad_form(e, W_inv) + lambda * norm1(b)
prob <- Problem(Minimize(obj), constraints = list(b >= 0))
res <- solve(prob)
res$getValue(b)

(B_check <- diag(round(as.vector(res$getValue(b)), digits = 3)))

(y_tilde <- as.vector(S %*% G %*% B_check %*% y_hat))
```

### Idea 3

$$
\tilde{\boldsymbol{y}}_h=\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h
$$

According to [Miyashiro & Takano (2015)](https://doi.org/10.1016/j.eswa.2014.07.056), we can use mixed logical programming and rewrite the problem as
$$
\begin{array}{r}
\min _{\boldsymbol{G}_h}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right) + \lambda \sum_{j=1}^{n}b_j, \\
\text{ s.t. } \boldsymbol{G}_h \boldsymbol{S}=\boldsymbol{I}_{n_b} \\
b_j = 0 \Rightarrow ||\boldsymbol{G}_{\cdot j}||_{1}=0 \quad (j=1,2, \ldots, n)\\
b_j \in \{0,1\} \quad (j=1,2, \ldots, n)
\end{array}
$$

## Minimization problem

As already shown in [Ben Taieb & Koo (2019)](https://doi.org/10.1145/3292500.3330976),
$$
\begin{aligned}
& \mathrm{E}\left[\left\|\boldsymbol{y}_{T+h}-\tilde{\boldsymbol{y}}_{h}\right\|_2^2 \mid \boldsymbol{I}_T\right] \\
& =\left\|\boldsymbol{S G}\left(\mathrm{E}\left[\hat{\boldsymbol{y}}_{h} \mid \boldsymbol{I}_T\right]-\mathrm{E}\left[\boldsymbol{y}_{T+h} \mid \boldsymbol{I}_T\right]\right)+(\boldsymbol{S}-\boldsymbol{S G S}) \mathrm{E}\left[\boldsymbol{b}_{T+h} \mid \boldsymbol{I}_T\right]\right\|_2^2 \\
& +\operatorname{Tr}\left(\operatorname{Var}\left[\boldsymbol{y}_{T+h}-\tilde{\boldsymbol{y}}_{h} \mid \boldsymbol{I}_T\right]\right).
\end{aligned}
$$

Under **the unbiasedness conditions (both for base and reconciled forecasts)**, minimizing the above loss reduces to the following problem:
$$
\min _{\boldsymbol{G}_h \in \mathcal{G}} \operatorname{Tr}\left(\operatorname{Var}\left[\boldsymbol{y}_{T+h}-\tilde{\boldsymbol{y}}_{h} \mid \boldsymbol{I}_T\right]\right) \\
\text { s.t. } \boldsymbol{S G_{h} S}=\boldsymbol{S}.
$$

The trace minimization problem can be reformulated in terms of a linear equality constrained least squares problem as follows:
$$
\min _{\boldsymbol{G}_h}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G}_h \hat{\boldsymbol{y}}_h\right), \\
\text{ s.t. } \boldsymbol{G}_h \boldsymbol{S} =\boldsymbol{I}_{n_b}
$$

* The MinT optimization problem must impose **two unbiasedness conditions**.

* Such an assumption does not always hold in practice, especially when we aim to zero out some columns of $\boldsymbol{G}$.

* The out-of-sample forecasting accuracy of base forecasts is an important basis for determining which columns are zeroed out.

## Relaxation of the unbiasedness assumptions

[Ben Taieb & Koo (2019)](https://doi.org/10.1145/3292500.3330976) consider the empirical risk minimization (ERM) problem:
$$
\begin{aligned}
\hat{G}_{\text {ERMreg }}=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\boldsymbol{Y}-\hat{\boldsymbol{Y}} \boldsymbol{G}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2 / N n+\lambda\|\operatorname{vec}(\boldsymbol{G})\|_1\right\},\\
=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda\|\operatorname{vec}(\boldsymbol{G})\|_1\right\}
\end{aligned}
$$
The problem can be reduced to **a standard LASSO problem** with $\operatorname{vec}(\boldsymbol{Y})$ as dependent variable and $(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}})$ as design matrix. it is a LASSO problem with $N \times n$ observations and $m \times N$ variables.

## Ideas based on the assumption relaxation

### Optimization problem

**For our project...**

We aim to deal with the minimization problem

$$
\begin{aligned}
\hat{G}=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\boldsymbol{Y}-\hat{\boldsymbol{Y}} \boldsymbol{G}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2 / N n+\lambda \sum_{j=1}^{n}z_{j} \right\},\\
=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda \sum_{j=1}^{n}z_{j} \right\} \\
s.t. & -M z_{j} \leq \sum_{i=0}^{m-1}|g_{j+in}| \leq M z_{j}  \text{ (Or quadratic form)}\\
& z_{j} \in \{0, 1\} \quad \text{ for } j=1,\ldots,n
\end{aligned}
$$

* Mixed integer programming (Do not have closed-form solution).

* The results are greatly influenced by the values of $M$ and $\lambda$.

* If we assume all elements of $\boldsymbol{G}$ are non-negative, it can be solved by CVXR package even if this is a non-convex problem.

```{r example2, message=FALSE, warning=FALSE}
library(CVXR)

set.seed(123)
S <- rbind(matrix(c(rep(1, 5), c(rep(1, 3), rep(0, 2)), c(rep(0, 3), rep(1, 2))), 
                  nrow = 3, byrow = TRUE),
           diag(1, nrow = 5))
B <- matrix(c(rnorm(10, 1, 1), 
              rnorm(10, 4, 1), 
              rnorm(10, 0, 1),
              rnorm(10, 2, 1),
              rnorm(10, 5, 1)), 
            byrow = FALSE, nrow = 10)
Y <- B %*% t(S)
Y_hat <- matrix(c(rnorm(10, 10, 1), 
                  rnorm(10, 6, 1), 
                  rnorm(10, 5, 1), 
                  rnorm(10, 1, 1), 
                  rnorm(10, 4, 1), 
                  rnorm(10, 0, 1),
                  rnorm(10, 2, 1),
                  rnorm(10, 5, 1)), 
                byrow = FALSE, nrow = 10)

VecY <- as.vector(Y)
D <- kronecker(S, Y_hat)

b <- Variable(8, boolean = TRUE)
#G_trs <- Variable(8, 5)
GB_trs <- Variable(40)

lambda <- 0.1
M <- 100

loss <- sum_squares(VecY - D %*% GB_trs)/(10*8)
penalty <- lambda * norm1(b)
constraints <- list(
  GB_trs >= 0,
  sum_entries(GB_trs[(0:4)*8 + 1]) >= - M*b[1],
  sum_entries(GB_trs[(0:4)*8 + 2]) >= - M*b[2],
  sum_entries(GB_trs[(0:4)*8 + 3]) >= - M*b[3],
  sum_entries(GB_trs[(0:4)*8 + 4]) >= - M*b[4],
  sum_entries(GB_trs[(0:4)*8 + 5]) >= - M*b[5],
  sum_entries(GB_trs[(0:4)*8 + 6]) >= - M*b[6],
  sum_entries(GB_trs[(0:4)*8 + 7]) >= - M*b[7],
  sum_entries(GB_trs[(0:4)*8 + 8]) >= - M*b[8],
  sum_entries(GB_trs[(0:4)*8 + 1]) <=  M*b[1],
  sum_entries(GB_trs[(0:4)*8 + 2]) <=  M*b[2],
  sum_entries(GB_trs[(0:4)*8 + 3]) <=  M*b[3],
  sum_entries(GB_trs[(0:4)*8 + 4]) <=  M*b[4],
  sum_entries(GB_trs[(0:4)*8 + 5]) <=  M*b[5],
  sum_entries(GB_trs[(0:4)*8 + 6]) <=  M*b[6],
  sum_entries(GB_trs[(0:4)*8 + 7]) <=  M*b[7],
  sum_entries(GB_trs[(0:4)*8 + 8]) <=  M*b[8]
)

prob <- Problem(Minimize(loss + penalty), constraints)
res <- solve(prob)
(res$getValue(b))
t(matrix(round(res$getValue(GB_trs), digits = 5), 8, 5, byrow = FALSE))
```

### Possible benchmark based on iterative process

$$
\begin{aligned}
\hat{G}=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\boldsymbol{Y}-\hat{\boldsymbol{Y}} \boldsymbol{B}^{\prime} \boldsymbol{G}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2 / N n+\lambda \| B \|_1 \right\},\\
=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{B}^{\prime} \boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda \| B \|_1 \right\} \\
=&\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}})(\boldsymbol{G} \otimes \boldsymbol{I}_n) \operatorname{vec}\left(\boldsymbol{B}^{\prime}\right)\right\|_2^2 / N n+\lambda \| B \|_1 \right\} \\
= &\underset{\boldsymbol{G} \in \mathcal{G}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}})(\boldsymbol{I}_n \otimes \boldsymbol{B}^{\prime}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda \| B \|_1 \right\}\\
s.t. \quad  & b_{ii} \in \{0, 1\} \quad\text{ for } i=1,\ldots, n \\
& b_{ij} = 0 \quad\text{ for } i\ne j
\end{aligned}
$$

* Initial $\boldsymbol{G}^{0}$ $\Rightarrow$ $\boldsymbol{B}^{1}$ (Lasso problem) $\Rightarrow$ $\boldsymbol{G}^{1}$ (Analytical solution) $\Rightarrow$ $\ldots$

## ROI introduction

The R Optimization Infrastructure (ROI) package is an R **interface** which provides an extensible infrastructure to model linear, quadratic, conic and general nonlinear optimization problems in a consistent way.

ROI provides the modeling capabilities and manages the **plugins**, the plugins add the solvers to ROI.

**Optimization problem**

\begin{equation}
\label{eq:op}
\begin{aligned}
\text{ minimize } & f_0(x) \\
\text { s.t. } & f_i(x) \leq b_i, \quad i=1, \ldots, m
\end{aligned}
\end{equation}

1. Linear programming (LP)

    All $f_i$ $(i= 0, \ldots ,m)$ in Equation \ref{eq:op} are linear.
  
    **All LPs are convex.**

2. Quadratic programming (QP)

    The objective function $f_0$ contains a quadratic part in addition to the linear term. QPs can be expressed in the following manner:
$$
\begin{aligned}
\text { minimize } & \frac{1}{2} x^{\top} Q_0 x+a_0^{\top} x \\
\text { s.t. } & A x \leq b
\end{aligned}
$$
    **A QP is convex if and only if $Q_0$ is positive semidefinite.**
  
    A generalization of the QP is the quadratically constrained quadratic program (QCQP):
$$
\begin{aligned}
\text { minimize } & \frac{1}{2} x^{\top} Q_0 x+a_0^{\top} x \\
\text { s.t. } & \frac{1}{2} x^{\top} Q_i x+a_i^{\top} x \leq b_i, \quad i=1, \ldots, m
\end{aligned}
$$
    **A QCQP is convex if and only if all $Q_i(i=0, \ldots, m)$ are positive semidefinite.**
  
3. Conic programming (CP)

    **CP refers to a class of problems designed to model convex OPs.** A CP is commonly defined as:
$$
\begin{aligned}
\text { minimize } & a_0^{\top} x \\
\text { s.t. } & A x+s=b \\
& s \in \mathcal{K}
\end{aligned}
$$
    where $s$ is a slack variable that is added to an inequality constraint to transform it to an equality, and the set $\mathcal{K}$ is a nonempty closed convex cone.

    Nonlinear objective functions are expressed in epigraph form:
$$
\begin{aligned}
\text { minimize } & t \\
\text { s.t. } & f_0(x) \leq t \\
& f_i(x) \leq b_i
\end{aligned}
$$
    * Zero cone and free cone
    $$
    \mathcal{K}_{\text {zero }}=\{0\}, \mathcal{K}_{\text {free }}=\mathbb{R}=\mathcal{K}_{\text {zero }}^*.
    $$
    E.g., $s_i \in \mathcal{K}_{\text {zero }} \Longleftrightarrow s_i=b_i-a_i^{\top} x=0 \Longleftrightarrow a_i^{\top} x=b_i$.

    * Linear cone (non-negative orthant)
    $$
    \mathcal{K}_{\operatorname{lin}}=\{x \in \mathbb{R} \mid x \geq 0\} .
    $$
    E.g., $s_i \in \mathcal{K}_{\text {lin }} \Longleftrightarrow s_i=b_i-a_i^{\top} x \geq 0 \Longleftrightarrow a_i^{\top} x \leq b_i$.

    * Second-order cone
    $$
    \mathcal{K}_{\mathrm{soc}}^n=\left\{(t, x) \in \mathbb{R}^n \mid x \in \mathbb{R}^{n-1}, t \in \mathbb{R},\|x\|_2 \leq t\right\} .
    $$
    E.g., $s_i \in \mathcal{K}_{\mathrm{soc}}^n \Longleftrightarrow s_i=b_i-a_i^{\top} x \geq 0 \Longleftrightarrow a_i^{\top} x \leq b_i$.
    
    * ...

4. Nonlinear programming (NLP)
    
    At least one $f_i$, $i= 0, \ldots ,m$ in Equation \ref{eq:op} is not linear.
    
    NLPs are not required to be convex, which makes it in general hard to obtain a reliable global solution.

5. Mixed integer programming (MIP)
    
    Additional constraints: some of the objective variables can only take integer values.

![](figs/ROIplug-ins.png){width=90%}

## Optimization problem solving

### Rewrite the optimization problem

Original problem:

\begin{equation}
\label{op_goal}
\begin{aligned}
&\underset{\boldsymbol{G}, \boldsymbol{z}}{\operatorname{argmin}}\left\{\left\|\boldsymbol{Y}-\hat{\boldsymbol{Y}} \boldsymbol{G}^{\prime} \boldsymbol{S}^{\prime}\right\|_F^2 / N n+\lambda \sum_{j=1}^{n}z_{j} \right\},\\
=&\underset{\boldsymbol{G}, \boldsymbol{z}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda \sum_{j=1}^{n}z_{j} \right\} \\
s.t. & -M z_{j} \leq \sum_{i=0}^{m-1}|g_{j+in}| \leq M z_{j}  \text{ (Or quadratic form)}\\
& z_{j} \in \{0, 1\} \quad \text{ for } j=1,\ldots,n
\end{aligned}
\end{equation}

Rewrite the problem:

\begin{equation}
\label{op_rewrite}
\begin{aligned}
&\underset{\boldsymbol{G}, \boldsymbol{z}}{\operatorname{argmin}}\left\{\left\|\operatorname{vec}(\boldsymbol{Y})-(\boldsymbol{S} \otimes \hat{\boldsymbol{Y}}) \operatorname{vec}\left(\boldsymbol{G}^{\prime}\right)\right\|_2^2 / N n+\lambda \sum_{j=1}^{n}z_{j} \right\} \\
=&\underset{\boldsymbol{g}, \boldsymbol{z}}{\operatorname{argmin}}\left\{\left\|\boldsymbol{y}-\boldsymbol{X} \boldsymbol{g}\right\|_2^2 / N n+\lambda \boldsymbol{1}^{\prime}\boldsymbol{z} \right\} \\
=&\underset{\boldsymbol{g}, \boldsymbol{\gamma}, \boldsymbol{z}}{\operatorname{argmin}}\left\{\frac{1}{Nn}\boldsymbol{\gamma}^{\prime}\boldsymbol{\gamma}+\lambda \boldsymbol{1}^{\prime}\boldsymbol{z} \right\} \\
\text{ s.t. } & \boldsymbol{y}-\boldsymbol{X} \boldsymbol{g} = \boldsymbol{\gamma} \\
& \sum_{i=0}^{m-1}\left(g_{j+in}\right)^{2} \leq M z_{j} \quad \text{ for } j=1,\ldots,n \\
& \boldsymbol{z} \in \{0, 1\}^{n}
\end{aligned}
\end{equation}

### ROI code logic

QP with LC and QC:
$$
\begin{aligned}
\underset{\boldsymbol{\beta} = \left(\boldsymbol{g}_{mn}^{\prime}, \boldsymbol{\gamma}_{Nn}^{\prime}, \boldsymbol{z}_{n}^{\prime}\right)^{\prime}}{\operatorname{argmin}} & \frac{1}{2} \boldsymbol{\beta}^{\prime} \boldsymbol{Q}_0 \boldsymbol{\beta}+\boldsymbol{a}_0^{\prime} \boldsymbol{\beta} \\
\text { s.t. } & \boldsymbol{A}\boldsymbol{\beta} = \boldsymbol{y} \\
& \frac{1}{2} \boldsymbol{\beta}^{\prime} \boldsymbol{Q}_j \boldsymbol{\beta}+\boldsymbol{a}_j^{\prime} \boldsymbol{\beta} \leq b_j, \quad j=1, \ldots, n \\
& \boldsymbol{z} \in \{0, 1\}^{n}
\end{aligned}
$$

* $\boldsymbol{Q}_{0} = \begin{pmatrix}
\boldsymbol{O}_{mn} &                                 &                    \\
                    & \frac{2}{Nn}\boldsymbol{1}_{Nn} &                    \\
                    &                                 & \boldsymbol{O}_{n}  \\
\end{pmatrix}$
and
$\boldsymbol{a}_0 = (\boldsymbol{0}_{mn+Nn}^{\prime}, \lambda \boldsymbol{1}_{n}^{\prime})^{\prime}$.

* $\boldsymbol{A} = (\boldsymbol{X}_{Nn \times mn} | \boldsymbol{1}_{Nn} | \boldsymbol{O}_{Nn \times n})$.

* For $j = 1, \ldots, n$, 
    * $\boldsymbol{Q}_{j} = \begin{pmatrix}
2\boldsymbol{D}_{mn} &                      \\
                    & \boldsymbol{O}_{Nn+n} \\
\end{pmatrix}$ with $d_{j+i \times n, j+i \times n} = 1$ $(i = 0, \ldots, m-1)$ and other elements are zeros. 
    * $\boldsymbol{a}_{j} = (\boldsymbol{0}_{mn+Nn}^{\prime}, -M\boldsymbol{d}_{n}^{\prime})^{\prime}$ with $d_{j} = 1$ and other elements are zeros.

### ROI implementation

```{r roi, message=FALSE, warning=FALSE}
Sys.setenv(ROI_LOAD_PLUGINS = "FALSE")
library(ROI)
library(slam)
library(magrittr)
library(ROI.plugin.neos)
library(ROI.plugin.gurobi)

dbind <- function(...) {
  ## sparse matrices construction
  .dbind <- function(x, y) {
    A <- simple_triplet_zero_matrix(NROW(x), NCOL(y))
    B <- simple_triplet_zero_matrix(NROW(y), NCOL(x))
    rbind(cbind(x, A), cbind(B, y))
  }
  Reduce(.dbind, list(...))
}

# Example data
set.seed(123)
S <- rbind(matrix(c(rep(1, 5), c(rep(1, 3), rep(0, 2)), c(rep(0, 3), rep(1, 2))), 
                  nrow = 3, byrow = TRUE),
           diag(1, nrow = 5))
B <- matrix(c(rnorm(10, 1, 1), 
              rnorm(10, 4, 1), 
              rnorm(10, 0, 1),
              rnorm(10, 2, 1),
              rnorm(10, 5, 1)), 
            byrow = FALSE, nrow = 10)
Y <- B %*% t(S)
Y_hat <- matrix(c(rnorm(10, 10, 1), 
                  rnorm(10, 6, 1), 
                  rnorm(10, 5, 1), 
                  rnorm(10, 1, 1), 
                  rnorm(10, 4, 1), 
                  rnorm(10, 0, 1),
                  rnorm(10, 2, 1),
                  rnorm(10, 5, 1)), 
                byrow = FALSE, nrow = 10)
VecY <- as.vector(Y)
D <- kronecker(S, Y_hat)

# Quadratic optimization problem
gp_op <- function(x, y, n, m, lambda, M) {
  ## x: kronecker(S, Y_hat)
  ## y: vec(Y)
  ## lambda: Lagrange multiplier
  ## n: number of all series
  ## m: number of bottom-level series
  ## M: big-M
  stzm <- simple_triplet_zero_matrix
  stdm <- simple_triplet_diag_matrix
  
  Nn <- NROW(x); mn <- NCOL(x)
  Q0 <- dbind(stzm(mn), stdm(2/Nn, Nn), stzm(n))
  a0 <- c(g = double(mn), ga = double(Nn), z = lambda * rep(1, n))
  op <- OP(objective = Q_objective(Q = Q0, L = a0))
  
  ## y - X %*% g = gamma  <=>  X %*% g + gamma = y
  A1 <- cbind(x, stdm(1, Nn), stzm(Nn, n))
  LC1 <- L_constraint(A1, eq(Nn), y)
  ## \sum_{i=0}^{m-1} (g_{j+in})^2 - M z_j <= 0 for j = 1,...,n
  QNULL <- diag(0, mn + Nn + n)
  LNULL <- c(double(mn), double(Nn), double(n))
  Q <- lapply(1:n, function(j){
    i <- 0:(m - 1)
    Q_j <- QNULL
    diag(Q_j)[j + i*n] <- 2
    Q_j
  })
  L <- sapply(1:n, function(j){
    L_j <- LNULL
    L_j[mn + Nn + j] <- -M
    L_j
  }) %>% t()
  QC1 <- Q_constraint(Q = Q,
                      L = L,
                      dir = rep("<=", n),
                      rhs = rep(0, n))
  
  constraints(op) <- rbind(LC1, QC1)
  bounds(op) <- V_bound(li = 1:(mn + Nn), lb = rep.int(-Inf, mn + Nn), nobj = mn + Nn + n)
  types(op) <- c(rep("C", mn + Nn), rep("B", n))
  op
}
op <- gp_op(x = D, y = VecY, n = NROW(S), m = NCOL(S), lambda = 0.1, M = 100)

# Optimal solution - solver = "neos"
# job_neos <- ROI_solve(op, "neos", email = "xiaoqian.wang@monash.edu")
## str(job_neos)
# slt_neos <- solution(job_neos)
# (z <- tail(slt_neos, NROW(S)))
# (G_neos <- matrix(slt_neos[1:(NCOL(S)*NROW(S))], 
#                   nrow = NROW(S), ncol = NCOL(S), byrow = FALSE) %>% 
#   t() %>% 
#   round(digits = 3))

# Optimal solution - solver = "gurobi"
job_gurobi <- ROI_solve(op, "gurobi")
 ## Register a Gurobi account as an academic user, request for a license, and download the current version of Gurobi optimizer
## str(job_gurobi)
slt_gurobi <- solution(job_gurobi)
(z <- tail(slt_gurobi, NROW(S)))
(G_gurobi <- matrix(slt_gurobi[1:(NCOL(S)*NROW(S))], 
                    nrow = NROW(S), ncol = NCOL(S), byrow = FALSE) %>% 
    t() %>% 
    round(digits = 3))
```

## Hyperparameters

### Big-M

### $\lambda$

We can use cross-validation to select the best value of $\lambda$, as implemented in the glmnet package in R.

The glmnet package computes the solutions for **a decreasing sequence of values for $\lambda$**, starting at the smallest value $\lambda_{\max }$ for which the entire vector $\hat{\beta}=0$. And then it selects a minimum value $\lambda_{\min }=\epsilon \lambda_{\max }$, and construct a sequence of $K$ values of $\lambda$ decreasing from $\lambda_{\max }$ to $\lambda_{\min }$ on the log scale. Typical values are $\epsilon=0.001$ and $K=100$.

# Subset selection with shrinkage

## Subset selection + shrinkage

> [Mazumder, R., Radchenko, P., & Dedieu, A. (2022). Subset selection with shrinkage: Sparse linear modeling when the SNR is low. Operations Research.](https://doi.org/10.1287/opre.2022.2276)

**Best subset selection problem**
$$
\underset{\boldsymbol{\beta}}{\operatorname{minimize}} \quad\|\mathbf{y}-\mathbf{X} \boldsymbol{\beta}\|_2^2 \quad \text { s.t. } \quad\|\boldsymbol{\beta}\|_0 \leq k
$$

1. Computationally infeasible

2. $\hat{\boldsymbol{\beta}}$ is instable, especially when SNR (signal to noise ratio) is low, the pairwise (sample) correlations among the features are high, and n is relatively small compared to p. **Overfit**

**Subset selection with shrinkage**
$$
\underset{\boldsymbol{\beta}}{\operatorname{minimize}} \frac{1}{2}\|\mathbf{y}-\mathbf{X} \boldsymbol{\beta}\|_2^2+\underbrace{\lambda\|\boldsymbol{\beta}\|_q}_{\text {Shrinkage }} \quad \text { s.t. } \underbrace{\|\boldsymbol{\beta}\|_0 \leq k}_{\text {Sparsity }} .
$$

- Sparsity & Shrinkage

MIO formulations for this problem:
$$
\begin{array}{c}
\text { minimize } u / 2+\lambda v \\
\text { s.t. }\|\mathbf{y}-\mathbf{X} \boldsymbol{\beta}\|_2^2 \leq u \\
\|\boldsymbol{\beta}\|_q \leq v \\
-\mathcal{M} z_j \leq \beta_j \leq \mathcal{M} z_j, j \in[p] ; \\
\mathbf{z} \in\{0,1\}^p ; \\
\sum_j z_j=k
\end{array}
$$
```{r, out.width="0.4\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Mazumder et al. (2022) framework"), echo=FALSE}
knitr::include_graphics("figs/Mazumder_et_al_2022.pdf")
```

- [Algorithms are written in Python](https://github.com/antoine-dedieu/subset_selection_with_shrinkage). The MIO formulation is solved with Gurobi's mixed integer programming solver.

## $L_0 L_q$-regularized regression problem

> [Hazimeh, H., & Mazumder, R. (2020). Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms. Operations Research, 68(5), 1517-1537.](https://doi.org/10.1287/opre.2019.1919)

$$
\underset{\boldsymbol{\beta}}{\operatorname{minimize}} \frac{1}{2}\|y-X \beta\|_2^2+\lambda_0\|\beta\|_0+\lambda_1\|\beta\|_{1}+\lambda_2\|\beta\|_{2}^{2}
$$

An overview of the different classes of local minima and establish the following hierarchy:

$$
\begin{array}{ccccccccc}
\operatorname{FSI}(k) & \subseteq & \operatorname{PSI}(k) &\subseteq & \operatorname{CW} & \subseteq & \operatorname{IHT} & \subseteq & \operatorname{Stationary} \\
\text { Minima } & & \text { Minima } & & \text { Minima } & & \text { Minima } & & \text{ Solutions }
\end{array}
$$

- For sufficiently large k, FSI(k) and PSI(k) minima coincide with the class of global minimizers.

1. Cyclic coordinate descent
    + full minimization in every coordinate: $\beta^{k+1}$ is obtained by updating the $i$th coordinate (with others held fixed).
    + converges to CW minima
    
2. Cyclic coordinate descent & local combinatorial optimization algorithms (iterative)
    + PSI(k)
    + FSI(k)

- It cannot provide certificates of optimality.

- [L0Learn](https://github.com/hazimehh/L0Learn): an extensible C++ toolkit with an R interface

## Group $l_0$ problem

> [Hazimeh, H., Mazumder, R., & Radchenko, P. (2021). Grouped variable selection with discrete optimization: Computational and statistical perspectives. arXiv preprint](https://arxiv.org/abs/2104.07084)

Suppose that the $p$ predictors are divided into $q$ pre-specified, non-overlapping groups.

**Group $l_0$ with ridge regularization**
$$
\underset{\boldsymbol{\beta}}{\operatorname{minimize}} \|\mathbf{y}-\mathbf{X} \boldsymbol{\beta}\|_2^2+\lambda_0 \sum_{g=1}^q \mathbf{1}\left(\boldsymbol{\beta}_g \neq \mathbf{0}\right)+\lambda_2\|\boldsymbol{\beta}\|_2^2
$$

1. Approximate algorithms (local minimizers)

  - cyclic block coordinate descent (BCD) $\longrightarrow$ (improved by) local combinatorial search
  
  - do not deliver certificates of optimality
  
2. MIP formulation

$$
\begin{aligned}
\underset{\boldsymbol{\theta}, \mathbf{z}, \boldsymbol{s}}{\operatorname{ minimize }} & \tilde{\ell}(\boldsymbol{\theta})+\lambda_0 \sum_{g=1}^q z_g+\lambda_1 \sum_{g=1}^q\left\|\boldsymbol{\theta}_g\right\|_2+\lambda_2 \sum_{g=1}^q s_g, \\
\text { s.t. } & \left\|\boldsymbol{\theta}_g\right\|_2 \leq \mathcal{M}_{\mathrm{U}} z_g, g \in[q] \\
& \left\|\boldsymbol{\theta}_g\right\|_2^2 \leq s_g z_g, \quad g \in[q] \\
& z_g \in\{0,1\}, s_g \geq 0, \quad g \in[q] .
\end{aligned}
$$

- Propose a specialized, nonlinear Branch-and-Bound (BnB) framework.

- Subproblem solver: active-set algorithm, which exploits sparsity by considering a reduced problem restricted to a small subset of groups.

- Upper bounds: obtain a new upper bound by restricting optimization, leading to aggressive pruning in the search tree, which can reduce the overall runtime.

- Branching and search strategies: 
    + for branching, use maximum fractional branching.
    + for search, use breadth-first search and switch to depth-first search if memory issues are encountered.
  
- Solve the associated MIP problem to certified optimality.

- [L0Group](https://github.com/hazimehh/L0Group) written in Python.

## Subset selection with shrinkage under unbiasedness assumptions

The vectorization is frequently used together with the Kronecker product to express matrix multiplication as a linear transformation on matrices.

$$
\operatorname{Vec}(A B C)=\left(C^{\prime} \otimes A\right) \operatorname{vec}(B)
$$
With two unbiasedness conditions, the trace minimization (MinT) problem can be reformulated in terms of a linear equality constrained least squares problem as follow.

$$
\begin{gathered}
\min _{\boldsymbol{G}} \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right) \quad \text { s.t. } \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b} \\
\Downarrow \\
\min _{\boldsymbol{G}} \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \quad \text { s.t. } \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b}
\end{gathered}
$$

If we consider subset selection with shrinkage which also imposes the two unbiasedness conditions of MinT, we have 

$$
\begin{aligned}
\min _{\boldsymbol{G}} & \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G})\right) \\
& \quad + \lambda_0 \sum_{j=1}^n \left\|\boldsymbol{G}_{\cdot j}\right\|_0 +\lambda_1\left\|\operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right)\right\|_1+\lambda_2\left\|\operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right)\right\|_2^2 \\
\text { s.t. } & \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b}
\end{aligned}
$$
where $\boldsymbol{G}^0$ can be a benchmark weight matrix estimated by MinT or other methods, such as bottom-up and top-down.

$$
\begin{aligned}
\min _{\operatorname{vec}\left(\boldsymbol{G}\right), \boldsymbol{z}, \check{\boldsymbol{e}}, \boldsymbol{d}^{+}, \boldsymbol{g}^{+}} & \frac{1}{2}\check{\boldsymbol{e}}^{\prime} \boldsymbol{W}_h^{-1}\check{\boldsymbol{e}} + \lambda_0 \sum_{j=1}^n z_j+\lambda_1\boldsymbol{1}^{\prime}\boldsymbol{d}^{+} + \lambda_2\boldsymbol{d}^{+\prime}\boldsymbol{d}^{+} \\
\text { s.t. } & \hat{\boldsymbol{y}}_h-\left(\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}\right) \operatorname{vec}(\boldsymbol{G}) = \check{\boldsymbol{e}}  \quad \ldots (C1)\\
& \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b} \Leftrightarrow\left(\boldsymbol{S}^{\prime} \otimes \boldsymbol{I}_{n_b}\right) \operatorname{vec}(\boldsymbol{G})=\operatorname{vec}\left(\boldsymbol{I}_{n_b}\right) \quad \ldots (C2) \\
& \sum_{i=1}^{n_b} \boldsymbol{g}_{i + (j-1) n_b}^{+} \leqslant M z_j, \quad j \in[n] \quad \ldots (C3) \\
& \boldsymbol{g}^{+} \geqslant \operatorname{vec}(\boldsymbol{G}) \quad \ldots (C4) \\
& \boldsymbol{g}^{+} \geqslant-\operatorname{vec}(\boldsymbol{G}) \quad \ldots (C5) \\
& \boldsymbol{d}^{+} \geqslant \operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right) \quad \ldots (C6) \\
& \boldsymbol{d}^{+} \geqslant-\operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right) \quad \ldots (C7) \\
& z_j \in\{0,1\}, \quad j \in[n]
\end{aligned}
$$

### R implementation

```{r, echo = FALSE}
rm(list = ls())
source('../R/sourceDir.R')
sourceDir("../R", recursive = TRUE)
MinT_G <- function(S, W){
  solve(t(S) %*% solve(W) %*% S) %*% t(S) %*% solve(W)
}
obj_MP <- function(fc, S, W, G){
  t(fc - S %*% G %*% fc) %*% solve(W) %*% (fc - S %*% G %*% fc) |> as.numeric()
}
obj_MinT <- function(S, W, G){
  S %*% G %*% W %*% t(G) %*% t(S) |> diag() |> sum()
}
```

Hierarchical structure & simple example
```{r}
# Hierarchical structure
S <- rbind(rep(1, 4),
           c(1, 1, 0, 0), c(0, 0, 1, 1),
           diag(1, 4))

# OLS and WLSs estimator
W_I <- diag(7)

W_s <- S %*% matrix(1, nrow = NCOL(S), ncol = 1) |> as.vector() |> diag()
G_s <- MinT_G(S, W_s)

# Example base forecasts
fc <- c(10, 4, 4, 2, 4, 2, 2) # the first base forecast in the middle level performs poorly
```

The two functions (`mip_l0` and `.mip_l0`) give the identical estimated parameters.

- `mip_l0`: use all the objective parameters and constraints, but select different constraints according to the $\lambda$ values.

- `.mip_l0`: use different objective parameters, objective functions, and constraints according to the $\lambda$ values.

```{r, results = "hold"}
print("# mip_l0")
rec <- mip_l0(fc, S, W_I, G0 = NULL, 
              lambda_0 = 0, lambda_1 = 0, lambda_2 = 0, M = NULL, 
              solver = "gurobi")
rec$G

print("# .mip_l0")
.rec <- .mip_l0(fc, S, W_I, G0 = NULL, 
                lambda_0 = 0, lambda_1 = 0, lambda_2 = 0, M = NULL, 
                solver = "gurobi")
.rec$G
```

If we do not put any penalty terms on $\boldsymbol{G}$, that is $\lambda_0 = \lambda_1 = \lambda_2 = 0$, the output should be identical to the result given by MinT using the corresponding estimator for the variance-covariance matrix of the base forecast errors.

Here we consider $W_I$ as an identity matrix and compare various results obtained by our method and MinT.

1. $\boldsymbol{G}$ matrix (**different**)
```{r, results = "hold"}
print("# mip_l0")
(G <- rec$G)

print("# MinT (ols)")
(G_ols <- MinT_G(S, W_I))
```

2. Reconciled forecasts $\tilde{\boldsymbol{y}}$ (**same**)
```{r, results = "hold"}
# mip_l0, MinT (ols)
cbind(S %*% G %*% fc, S %*% G_ols %*% fc)
```

3. Objective values $\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \boldsymbol{G} \hat{\boldsymbol{y}}_h\right)$ (**same and both satisfy $\boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b}$**)
```{r, results = "hold"}
print("# mip_l0")
obj_MP(fc, S, W_I, G)

print("# MinT (ols)")
obj_MP(fc, S, W_I, G_ols)
```

So, when $\lambda_0 = \lambda_1 = \lambda_2 = 0$, our method and MinT give same reconciled forecasts and objective values but **different G matrix**. Here are some explanations:

* For our method, the columns of $\hat{\boldsymbol{y}}_h^{\prime} \otimes \boldsymbol{S}$ are not linearly independent, there are infinitely many least-squares solutions.

* For MinT, under the assumption $\boldsymbol{G}\boldsymbol{S} = \boldsymbol{I}_{n_b}$, Wickramasuriya et al. (2019) first derive the explit solution as $\boldsymbol{G} = \left(\boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1} \boldsymbol{S}\right)^{-1} \boldsymbol{S}^{\prime} \boldsymbol{W}_h^{-1}$, then reformulate the trace minimization problem in terms of a linear equality constrained least squares problem
$$
\min _{\boldsymbol{\tilde{y}}_h}\frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{\tilde{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{\tilde{y}}_h\right), \text{ s.t. } 
\boldsymbol{\tilde{y}}_h = \boldsymbol{S}\boldsymbol{\tilde{b}}_h.
$$
This is a Generalized Least Squares problem, actually the objective parameter is the reconciled forecasts at the bottom level $\boldsymbol{\tilde{b}}_h$ rather than $\boldsymbol{G}$.

* The GLS problem is equivalent to the following problem
$$
\min _{\boldsymbol{\tilde{y}}_h}\frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{\tilde{y}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{\tilde{y}}_h\right), \text{ s.t. } 
\boldsymbol{U}^{\prime}\boldsymbol{\tilde{y}}_h = \boldsymbol{0}\\
\text{ where } \boldsymbol{U}^{\prime} = [\boldsymbol{I}_{n-n_b}| -\boldsymbol{C}].
$$
We can directly solve the quadratic problem as follows.
```{r, results = "hold"}
# lsop
rec_lsop <- lsop(fc, S, W_I, solver = "gurobi")
# lsop, mip_l0, MinT (ols)
cbind(rec_lsop$y_tilde, S %*% G %*% fc, S %*% G_ols %*% fc)
```

* In order to perform subset selection with shrinkage for $\boldsymbol{G}$. We can reformulate our minimization problem as follows.
$$
\begin{aligned}
\min _{\tilde{\boldsymbol{b}}_h, \boldsymbol{G}} & \frac{1}{2}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \tilde{\boldsymbol{b}}_h\right)^{\prime} \boldsymbol{W}_h^{-1}\left(\hat{\boldsymbol{y}}_h-\boldsymbol{S} \tilde{\boldsymbol{b}}_h\right) \\
& + \lambda_0 \sum_{j=1}^n \left\|\boldsymbol{G}_{\cdot j}\right\|_0 +\lambda_1\left\|\operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right)\right\|_1+\lambda_2\left\|\operatorname{vec}\left(\boldsymbol{G}-\boldsymbol{G}^0\right)\right\|_2^2\\
\text { s.t. } & \boldsymbol{G} \boldsymbol{S}=\boldsymbol{I}_{n_b} \\
& \tilde{\boldsymbol{b}}_h = \boldsymbol{G} \hat{\boldsymbol{y}}_h
\end{aligned}
$$
In this case, if $\lambda_0 = \lambda_1 = \lambda_2$, the objective parameters become $\tilde{\boldsymbol{b}}$ and we only output the estimation of $\tilde{\boldsymbol{b}}$. But it is actually the same with the original formulation.

**Some questions**

1. Check the original objective function (the trace of the variance covariance matrix of the **reconciled forecast errors**) given by $var[\boldsymbol{y}_{t+h} - \tilde{\boldsymbol{y}}_{h} | I_t] = \boldsymbol{S}\boldsymbol{G}\boldsymbol{W}\boldsymbol{G}^{\prime}\boldsymbol{S}^{\prime}$ subject to $\boldsymbol{S}\boldsymbol{G}\boldsymbol{S} = \boldsymbol{S}$. The results are different.
```{r, results = "hold"}
print("# mip_l0")
obj_MinT(S, W_I, G)

print("# MinT (ols)")
obj_MinT(S, W_I, G_ols)
```

If we give some penalty to shrink $\boldsymbol{G}$ towards $\boldsymbol{G}_{ols}$, even if the value of $\lambda_1$ is very small, it will return the same $\boldsymbol{G}$ as that obtained from MinT explcit solution.
```{r, results = "hold"}
print("# G_ols")
G_ols

print("# mip_l0 with lambda_0 = lambda_1 = lambda_2 = 0")
G

print("# mip_l0 with lambda_1 = 0.1 and G0 = G_ols")
mip_l0(fc, S, W_I, G0 = G_ols, 
       lambda_0 = 0, lambda_1 = 0.1, lambda_2 = 0, M = NULL, 
       solver = "gurobi")$G

print("# mip_l0 with lambda_1 = 0.001 and G0 = G_ols")
mip_l0(fc, S, W_I, G0 = G_ols, 
       lambda_0 = 0, lambda_1 = 0.001, lambda_2 = 0, M = NULL, 
       solver = "gurobi")$G

print("# mip_l0 with lambda_1 = 0.1 and G0 = G_bu")
G_bu <- cbind(rep(0, 4), rep(0, 4), rep(0, 4), diag(1, 4))
mip_l0(fc, S, W_I, G0 = G_bu, 
       lambda_0 = 0, lambda_1 = 0.1, lambda_2 = 0, M = NULL, 
       solver = "gurobi")$G

print("# mip_l0 with lambda_0 = 0.1, lambda_1 = 0.1 and G0 = G_bu")
mip_l0(fc, S, W_I, G0 = G_bu, 
       lambda_0 = 0.1, lambda_1 = 0.1, lambda_2 = 0, M = NULL, 
       solver = "gurobi")$G
```

2. A.2 Theorem1 in Wickramasuriya et al. (2019).

**Lemma 1 of** [Wang et al.(1986)](https://doi.org/10.1109/TAC.1986.1104370):

Let $K$ and $S$ be $(n \times n)$ matrices. If $K=K^{\prime} \geq 0$ and $S$ is symmetric, then
$$
\lambda_{\min }(S) \text { tr }(K) \leq \operatorname{tr}(K S) \leq \lambda_{\max }(S) \operatorname{tr}(K)
$$

The objective function,
$$
\operatorname{tr}\left[\boldsymbol{S P} \boldsymbol{W}_h \boldsymbol{P}^{\prime} \boldsymbol{S}^{\prime}\right]=\operatorname{tr}\left[\boldsymbol{S}^{\prime} \boldsymbol{S P} \boldsymbol{W}_h \boldsymbol{P}^{\prime}\right].
$$

$\boldsymbol{S}^{\prime} \boldsymbol{S}$ and $\boldsymbol{P} W_h \boldsymbol{P}^{\prime}$ are both symmetric, and positive definite and positive semi-definite respectively. So we have
$$
\lambda_{\min }\left(\boldsymbol{S}^{\prime} \boldsymbol{S}\right) \operatorname{tr}\left[\boldsymbol{P} \boldsymbol{W}_h \boldsymbol{P}^{\prime}\right] \leq \operatorname{tr}\left[\boldsymbol{S}^{\prime} \boldsymbol{S} \boldsymbol{P} \boldsymbol{W}_h \boldsymbol{P}^{\prime}\right] \leq \lambda_{\max }\left(\boldsymbol{S}^{\prime} \boldsymbol{S}\right) \operatorname{tr}\left[\boldsymbol{P} \boldsymbol{W}_h \boldsymbol{P}^{\prime}\right],
$$
where $\boldsymbol{S}^{\prime} \boldsymbol{S}$ is given and its eigenvalues are positive. So the original minimization problem can be restated as
$$
\min _{\boldsymbol{P}} \lambda_{\max }\left(\boldsymbol{S}^{\prime} \boldsymbol{S}\right)\operatorname{tr}\left[\boldsymbol{P} W_h \boldsymbol{P}^{\prime}\right] \quad \text { s.t. } \boldsymbol{P S}=\boldsymbol{I} \\
\Longrightarrow \\
\min _{\boldsymbol{P}} \operatorname{tr}\left[\boldsymbol{P} W_h \boldsymbol{P}^{\prime}\right] \quad \text { s.t. } \boldsymbol{P S}=\boldsymbol{I} \\
$$

* The choice of $\lambda$ is scale dependent.
* One-step head forecast VS. multi-step ahead forecast.
* Different models to be used for each series.


# Other issues

## Temporal reconciliation

* Remove all nodes in a level or several levels.

## Non-negative constraints


